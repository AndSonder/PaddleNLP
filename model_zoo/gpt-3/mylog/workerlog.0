grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
A new Series field (pipeline) detected!
A new field (schedule_mode) detected!
A new Series field (Profiler_auto) detected!
A new field (memory_stats) detected!
[2024-03-01 03:13:57,240] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0301 03:13:57.242063 30601 tcp_utils.cc:181] The server starts to listen on IP_ANY:58855
I0301 03:13:57.242283 30601 tcp_utils.cc:130] Successfully connected to 172.17.0.3:58855
I0301 03:13:57.424289 30601 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 03:13:57,425] [    INFO] topology.py:359 - Total 4 pipe comm group(s) create successfully!
W0301 03:13:57.495677 30601 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.8
W0301 03:13:57.497417 30601 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
/usr/local/lib/python3.9/dist-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 0 is not in group _default_pg10
  warnings.warn(
I0301 03:14:05.432904 30601 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 03:14:05,433] [    INFO] topology.py:359 - Total 1 data comm group(s) create successfully!
[2024-03-01 03:14:05,433] [    INFO] topology.py:359 - Total 4 model comm group(s) create successfully!
[2024-03-01 03:14:05,433] [    INFO] topology.py:359 - Total 4 sharding comm group(s) create successfully!
I0301 03:14:05.433872 30601 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 03:14:05,433] [    INFO] topology.py:289 - HybridParallelInfo: rank_id: 0, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 4, sep_degree: 1, mp_group: [0],  sharding_group: [0], pp_group: [0], dp_group: [0, 1, 2, 3], sep:group: None, check/clip group: [0]
[32m[2024-03-01 03:14:05,435] [INFO][0m - The global seed is set to 2052, local seed is set to 2056 and random seed is set to 1024.[0m
[32m[2024-03-01 03:14:05,437] [INFO][0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:05,437] [INFO][0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:05,437] [INFO][0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json[0m
[32m[2024-03-01 03:14:05,437] [INFO][0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt[0m
[2024-03-01 03:14:05,546] [ WARNING] language_module.py:66 -  > padded vocab (size: 50304) with 0 dummy tokens (new size: 50304)
[32m[2024-03-01 03:14:05,546] [INFO][0m - Model Size: 0.35 B[0m
[32m[2024-03-01 03:14:05,637] [INFO][0m - 
===========================================================
==       PaddleFleetX is powered by PaddlePaddle !       ==
===========================================================
==                                                       ==
==   For more info please go to the following website.   ==
==                                                       ==
==      https://github.com/PaddlePaddle/PaddleFleetX     ==
===========================================================
[0m
[32m[2024-03-01 03:14:05,638] [INFO][0m - Data : [0m
[32m[2024-03-01 03:14:05,638] [INFO][0m -     Eval : [0m
[32m[2024-03-01 03:14:05,638] [INFO][0m -         collate_fn : gpt_collate_fn[0m
[32m[2024-03-01 03:14:05,638] [INFO][0m -         dataset : [0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             input_dir : ./data/[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             max_seq_len : 1024[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             mode : Eval[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             name : GPTDataset[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             num_samples : 40[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             seed : 1024[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -             split : [969, 30, 1][0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -         sample_split : 2[0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -     Train : [0m
[32m[2024-03-01 03:14:05,639] [INFO][0m -         collate_fn : gpt_collate_fn[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -         dataset : [0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             input_dir : ./data/[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             max_seq_len : 1024[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             mode : Train[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             name : GPTDataset[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             num_samples : 120[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             seed : 1024[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -             split : [969, 30, 1][0m
[32m[2024-03-01 03:14:05,640] [INFO][0m -         sample_split : 2[0m
[32m[2024-03-01 03:14:05,640] [INFO][0m - Distributed : [0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -     dp_degree : 2[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -     mp_degree : 1[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -     pipeline : [0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -         schedule_mode : 1F1B[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -     pp_degree : 2[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -     sharding : [0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -         broadcast_overlap : False[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -         reduce_overlap : False[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -         sharding_degree : 1[0m
[32m[2024-03-01 03:14:05,641] [INFO][0m -         sharding_stage : 1[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m - Engine : [0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -     accumulate_steps : 2[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -     eval_freq : 100000[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -     eval_iters : 10[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -     logging_freq : 10[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -     max_steps : 30[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -     mix_precision : [0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -         custom_black_list : ['reduce_sum', 'c_softmax_with_cross_entropy', 'elementwise_div'][0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -         custom_white_list : ['lookup_table', 'lookup_table_v2'][0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -         dtype : float16[0m
[32m[2024-03-01 03:14:05,642] [INFO][0m -         enable : True[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -         level : o2[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -         scale_loss : 32768.0[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -     num_train_epochs : 1[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -     save_load : [0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -         ckpt_dir : None[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -         output_dir : ./output[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -         save_epoch : 1[0m
[32m[2024-03-01 03:14:05,643] [INFO][0m -         save_steps : 9223372036854775807[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     strategy : {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 32768.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': ['lookup_table', 'lookup_table_v2'], 'custom_black_list': ['reduce_sum', 'c_softmax_with_cross_entropy', 'elementwise_div'], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': False}","sharding":"{'enable': False, 'stage': 1, 'degree': 1, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': False, 'k_steps': 1, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': '1F1B', 'vpp_degree': 1, 'vpp_seg_method': '', 'micro_batch_size': 1, 'accumulate_steps': 2, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': True}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': False}","sp_optimization":"{'enable': False}",}[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     test_iters : 100[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     verbose : 2[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m - FusedPasses : [0m
[32m[2024-03-01 03:14:05,644] [INFO][0m - Global : [0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     device : gpu[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     global_batch_size : 4[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     local_batch_size : 2[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     micro_batch_size : 1[0m
[32m[2024-03-01 03:14:05,644] [INFO][0m -     seed : 1024[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m - Model : [0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     attention_probs_dropout_prob : 0[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     ffn_hidden_size : 4096[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     fuse_attn_qkv : True[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     fused_softmax_with_triangular : True[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     hidden_dropout_prob : 0[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     hidden_size : 1024[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     initializer_range : 0.02[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     max_position_embeddings : 1024[0m
[32m[2024-03-01 03:14:05,645] [INFO][0m -     module : GPTModuleAuto[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     name : GPT[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     num_attention_heads : 16[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     num_layers : 24[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     recompute_granularity : full[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     scale_qk_by_layer_num : True[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     sequence_parallel : False[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     type_vocab_size : 16[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     use_flash_attn : False[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     use_recompute : True[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     vocab_size : 50304[0m
[32m[2024-03-01 03:14:05,646] [INFO][0m -     vocab_size_divisible_unit : 128[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m - Optimizer : [0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -     beta1 : 0.9[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -     beta2 : 0.999[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -     epsilon : 1e-08[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -     grad_clip : [0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -         clip_norm : 1.0[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -         name : ClipGradByGlobalNorm[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -     lr : [0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -         decay_steps : 1440000[0m
[32m[2024-03-01 03:14:05,647] [INFO][0m -         max_lr : 5e-05[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -         min_lr : 1e-05[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -         name : CosineAnnealingWithWarmupDecay[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -         use_increments : True[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -         warmup_rate : 0.01[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -     name : AdamW[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -     weight_decay : 0.01[0m
[32m[2024-03-01 03:14:05,648] [INFO][0m - Profiler_auto : [0m
[32m[2024-03-01 03:14:05,648] [INFO][0m -     memory_stats : True[0m
[32m[2024-03-01 03:14:06,190] [INFO][0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:06,191] [INFO][0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:06,191] [INFO][0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json[0m
[32m[2024-03-01 03:14:06,191] [INFO][0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt[0m
[35m[2024-03-01 03:14:08,843] [DEBUG][0m - build dataset(<ppfleetx.data.dataset.gpt_dataset.GPTDataset object at 0x7f9e71820d00>) success...[0m
[32m[2024-03-01 03:14:08,847] [INFO][0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:08,848] [INFO][0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:08,848] [INFO][0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json[0m
[32m[2024-03-01 03:14:08,848] [INFO][0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt[0m
[35m[2024-03-01 03:14:09,021] [DEBUG][0m - build dataset(<ppfleetx.data.dataset.gpt_dataset.GPTDataset object at 0x7f9e7e377490>) success...[0m
[32m[2024-03-01 03:14:09,022] [INFO][0m - run with paddle 0.0.0, commit id ab0f50d8[0m
[35m[2024-03-01 03:14:09,024] [DEBUG][0m - build optimizer (Weight Decay, params: ) success..[0m
[2024-03-01 03:14:09,026] [    INFO] cluster.py:914 - Node Count: 1, Local Device Size: 4, GPU Model: NVIDIA GeForce GTX 1080 Ti, GPU Memory: 11GB, World size: 4, EndPoint: 172.17.0.3:58856.
[2024-03-01 03:14:09,027] [    INFO] engine.py:202 - Distribute training by paddle.distributed.launch
[2024-03-01 03:14:09,027] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
[2024-03-01 03:14:09,027] [ WARNING] fleet.py:272 - The dygraph parallel environment has been initialized.
[2024-03-01 03:14:09,027] [ WARNING] fleet.py:295 - The dygraph hybrid parallel environment has been initialized.
/home/workspace/PaddleNLP/model_zoo/gpt-3/ppfleetx/utils/device.py:50: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-03-01 03:14:09,051] [    INFO] engine.py:655 - Building model with 'to_static' method.
INFO 2024-03-01 03:14:09,051 helper.py:245] start to build program for mode = train.
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
[2024-03-01 03:14:24,031] [    INFO] parallelizer_v2.py:283 - Applying AMP-float16-o2 ...
[2024-03-01 03:14:24,615] [    INFO] auto_parallel_recompute.py:392 - The excluded ops in recompute segments are:
[[], []]
[_remove_and_get_optimizer_op] op type:  check_finite_and_unscale
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  c_allreduce_max
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  check_finite_and_unscale
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  c_allreduce_max
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  concat
[_remove_and_get_optimizer_op] op type:  reduce_any
[_remove_and_get_optimizer_op] op type:  memcpy_d2h
[_remove_and_get_optimizer_op] op type:  update_loss_scaling
[_remove_and_get_optimizer_op] op type:  update_loss_scaling
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  stack
[_remove_and_get_optimizer_op] op type:  reduce_sum
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  stack
[_remove_and_get_optimizer_op] op type:  reduce_sum
[_remove_and_get_optimizer_op] op type:  stack
[_remove_and_get_optimizer_op] op type:  reduce_sum
[_remove_and_get_optimizer_op] op type:  c_allreduce_sum
[_remove_and_get_optimizer_op] op type:  sqrt
[_remove_and_get_optimizer_op] op type:  fill_constant
[_remove_and_get_optimizer_op] op type:  elementwise_max
[_remove_and_get_optimizer_op] op type:  elementwise_div
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
params_grads [(persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False), var embedding_0.w_0@GRAD : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)), (persist trainable param embedding_1.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var embedding_1.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_0.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_0.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_1.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_1.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_1.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_1.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_2.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_2.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_2.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_2.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_3.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_3.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_3.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_0.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_0.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_0.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_0.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_1.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_1.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_1.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_1.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_4.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_4.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_4.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_4.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_5.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_5.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_5.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_5.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_6.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_6.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_6.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_6.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_7.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_7.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_2.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_2.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_2.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_2.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_3.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_3.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_3.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_3.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_8.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_8.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_8.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_8.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_9.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_9.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_9.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_9.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_10.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_10.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_10.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_10.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_11.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_11.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_11.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_4.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_4.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_4.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_4.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_5.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_5.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_5.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_5.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_12.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_12.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_12.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_12.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_13.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_13.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_13.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_13.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_14.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_14.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_14.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_14.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_15.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_15.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_15.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_15.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_6.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_6.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_6.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_6.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_7.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_7.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_7.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_7.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_16.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_16.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_16.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_16.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_17.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_17.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_17.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_17.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_18.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_18.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_18.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_18.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_19.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_19.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_19.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_19.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_8.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_8.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_8.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_8.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_9.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_9.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_9.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_9.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_20.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_20.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_20.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_20.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_21.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_21.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_21.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_21.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_22.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_22.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_22.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_22.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_23.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_23.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_23.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_23.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_10.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_10.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_10.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_10.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_11.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_11.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_11.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_11.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_24.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_24.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_24.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_24.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_25.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_25.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_25.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_25.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_26.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_26.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_26.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_26.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_27.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_27.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_27.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_27.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_12.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_12.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_12.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_12.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_13.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_13.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_13.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_13.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_28.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_28.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_28.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_28.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_29.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_29.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_29.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_29.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_30.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_30.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_30.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_30.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_31.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_31.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_31.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_31.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_14.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_14.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_14.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_14.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_15.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_15.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_15.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_15.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_32.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_32.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_32.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_32.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_33.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_33.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_33.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_33.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_34.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_34.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_34.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_34.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_35.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_35.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_35.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_35.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_16.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_16.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_16.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_16.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_17.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_17.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_17.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_17.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_36.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_36.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_36.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_36.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_37.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_37.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_37.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_37.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_38.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_38.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_38.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_38.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_39.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_39.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_39.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_39.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_18.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_18.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_18.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_18.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_19.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_19.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_19.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_19.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_40.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_40.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_40.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_40.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_41.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_41.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_41.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_41.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_42.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_42.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_42.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_42.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_43.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_43.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_43.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_43.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_20.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_20.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_20.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_20.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_21.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_21.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_21.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_21.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_44.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_44.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_44.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_44.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_45.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_45.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_45.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_45.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_46.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_46.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_46.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_46.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_47.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_47.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_47.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_47.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_22.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_22.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_22.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_22.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_23.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_23.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_23.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_23.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False))]
new_params_to_grads [[persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False), persist var embedding_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)], [persist trainable param embedding_1.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var embedding_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_0.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_0.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_0.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_0.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_0.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_1.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_1.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_1.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_1.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_1.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_1.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_2.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_2.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_2.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_3.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_3.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_2.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_2.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_2.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_4.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_4.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_4.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_4.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_5.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_5.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_5.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_5.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_3.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_3.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_3.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_6.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_6.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_6.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_6.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_7.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_7.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_7.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_4.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_4.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_4.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_4.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_8.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_8.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_8.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_8.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_9.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_9.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_9.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_9.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_5.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_5.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_5.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_5.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_10.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_10.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_10.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_10.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_11.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_11.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_11.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_6.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_6.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_6.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_6.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_12.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_12.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_12.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_12.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_13.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_13.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_13.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_13.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_7.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_7.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_7.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_7.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_14.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_14.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_14.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_14.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_15.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_15.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_15.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_15.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_8.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_8.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_8.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_8.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_16.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_16.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_16.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_16.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_17.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_17.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_17.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_17.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_9.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_9.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_9.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_9.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_18.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_18.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_18.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_18.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_19.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_19.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_19.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_19.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_10.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_10.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_10.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_10.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_20.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_20.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_20.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_20.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_21.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_21.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_21.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_21.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_11.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_11.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_11.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_11.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_22.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_22.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_22.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_22.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_23.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_23.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_23.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_23.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_12.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_12.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_12.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_12.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_24.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_24.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_24.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_24.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_25.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_25.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_25.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_25.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_13.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_13.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_13.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_13.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_26.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_26.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_26.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_26.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_27.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_27.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_27.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_27.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_14.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_14.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_14.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_14.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_28.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_28.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_28.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_28.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_29.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_29.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_29.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_29.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_15.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_15.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_15.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_15.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_30.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_30.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_30.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_30.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_31.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_31.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_31.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_31.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_16.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_16.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_16.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_16.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_32.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_32.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_32.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_32.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_33.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_33.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_33.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_33.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_17.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_17.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_17.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_17.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_34.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_34.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_34.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_34.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_35.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_35.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_35.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_35.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_18.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_18.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_18.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_18.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_36.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_36.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_36.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_36.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_37.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_37.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_37.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_37.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_19.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_19.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_19.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_19.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_38.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_38.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_38.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_38.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_39.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_39.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_39.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_39.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_20.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_20.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_20.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_20.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_40.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_40.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_40.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_40.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_41.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_41.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_41.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_41.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_21.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_21.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_21.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_21.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_42.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_42.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_42.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_42.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_43.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_43.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_43.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_43.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_22.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_22.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_22.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_22.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_44.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_44.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_44.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_44.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_45.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_45.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_45.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_45.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_23.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_23.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_23.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_23.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_46.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_46.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_46.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_46.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_47.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_47.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_47.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_47.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)]]
['embedding_0.w_0@GRAD', 'embedding_1.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_0.b_0@GRAD', 'linear_1.w_0@GRAD', 'linear_1.b_0@GRAD', 'linear_2.w_0@GRAD', 'linear_2.b_0@GRAD', 'linear_3.w_0@GRAD', 'linear_3.b_0@GRAD', 'layer_norm_0.w_0@GRAD', 'layer_norm_0.b_0@GRAD', 'layer_norm_1.w_0@GRAD', 'layer_norm_1.b_0@GRAD', 'linear_4.w_0@GRAD', 'linear_4.b_0@GRAD', 'linear_5.w_0@GRAD', 'linear_5.b_0@GRAD', 'linear_6.w_0@GRAD', 'linear_6.b_0@GRAD', 'linear_7.w_0@GRAD', 'linear_7.b_0@GRAD', 'layer_norm_2.w_0@GRAD', 'layer_norm_2.b_0@GRAD', 'layer_norm_3.w_0@GRAD', 'layer_norm_3.b_0@GRAD', 'linear_8.w_0@GRAD', 'linear_8.b_0@GRAD', 'linear_9.w_0@GRAD', 'linear_9.b_0@GRAD', 'linear_10.w_0@GRAD', 'linear_10.b_0@GRAD', 'linear_11.w_0@GRAD', 'linear_11.b_0@GRAD', 'layer_norm_4.w_0@GRAD', 'layer_norm_4.b_0@GRAD', 'layer_norm_5.w_0@GRAD', 'layer_norm_5.b_0@GRAD', 'linear_12.w_0@GRAD', 'linear_12.b_0@GRAD', 'linear_13.w_0@GRAD', 'linear_13.b_0@GRAD', 'linear_14.w_0@GRAD', 'linear_14.b_0@GRAD', 'linear_15.w_0@GRAD', 'linear_15.b_0@GRAD', 'layer_norm_6.w_0@GRAD', 'layer_norm_6.b_0@GRAD', 'layer_norm_7.w_0@GRAD', 'layer_norm_7.b_0@GRAD', 'linear_16.w_0@GRAD', 'linear_16.b_0@GRAD', 'linear_17.w_0@GRAD', 'linear_17.b_0@GRAD', 'linear_18.w_0@GRAD', 'linear_18.b_0@GRAD', 'linear_19.w_0@GRAD', 'linear_19.b_0@GRAD', 'layer_norm_8.w_0@GRAD', 'layer_norm_8.b_0@GRAD', 'layer_norm_9.w_0@GRAD', 'layer_norm_9.b_0@GRAD', 'linear_20.w_0@GRAD', 'linear_20.b_0@GRAD', 'linear_21.w_0@GRAD', 'linear_21.b_0@GRAD', 'linear_22.w_0@GRAD', 'linear_22.b_0@GRAD', 'linear_23.w_0@GRAD', 'linear_23.b_0@GRAD', 'layer_norm_10.w_0@GRAD', 'layer_norm_10.b_0@GRAD', 'layer_norm_11.w_0@GRAD', 'layer_norm_11.b_0@GRAD', 'linear_24.w_0@GRAD', 'linear_24.b_0@GRAD', 'linear_25.w_0@GRAD', 'linear_25.b_0@GRAD', 'linear_26.w_0@GRAD', 'linear_26.b_0@GRAD', 'linear_27.w_0@GRAD', 'linear_27.b_0@GRAD', 'layer_norm_12.w_0@GRAD', 'layer_norm_12.b_0@GRAD', 'layer_norm_13.w_0@GRAD', 'layer_norm_13.b_0@GRAD', 'linear_28.w_0@GRAD', 'linear_28.b_0@GRAD', 'linear_29.w_0@GRAD', 'linear_29.b_0@GRAD', 'linear_30.w_0@GRAD', 'linear_30.b_0@GRAD', 'linear_31.w_0@GRAD', 'linear_31.b_0@GRAD', 'layer_norm_14.w_0@GRAD', 'layer_norm_14.b_0@GRAD', 'layer_norm_15.w_0@GRAD', 'layer_norm_15.b_0@GRAD', 'linear_32.w_0@GRAD', 'linear_32.b_0@GRAD', 'linear_33.w_0@GRAD', 'linear_33.b_0@GRAD', 'linear_34.w_0@GRAD', 'linear_34.b_0@GRAD', 'linear_35.w_0@GRAD', 'linear_35.b_0@GRAD', 'layer_norm_16.w_0@GRAD', 'layer_norm_16.b_0@GRAD', 'layer_norm_17.w_0@GRAD', 'layer_norm_17.b_0@GRAD', 'linear_36.w_0@GRAD', 'linear_36.b_0@GRAD', 'linear_37.w_0@GRAD', 'linear_37.b_0@GRAD', 'linear_38.w_0@GRAD', 'linear_38.b_0@GRAD', 'linear_39.w_0@GRAD', 'linear_39.b_0@GRAD', 'layer_norm_18.w_0@GRAD', 'layer_norm_18.b_0@GRAD', 'layer_norm_19.w_0@GRAD', 'layer_norm_19.b_0@GRAD', 'linear_40.w_0@GRAD', 'linear_40.b_0@GRAD', 'linear_41.w_0@GRAD', 'linear_41.b_0@GRAD', 'linear_42.w_0@GRAD', 'linear_42.b_0@GRAD', 'linear_43.w_0@GRAD', 'linear_43.b_0@GRAD', 'layer_norm_20.w_0@GRAD', 'layer_norm_20.b_0@GRAD', 'layer_norm_21.w_0@GRAD', 'layer_norm_21.b_0@GRAD', 'linear_44.w_0@GRAD', 'linear_44.b_0@GRAD', 'linear_45.w_0@GRAD', 'linear_45.b_0@GRAD', 'linear_46.w_0@GRAD', 'linear_46.b_0@GRAD', 'linear_47.w_0@GRAD', 'linear_47.b_0@GRAD', 'layer_norm_22.w_0@GRAD', 'layer_norm_22.b_0@GRAD', 'layer_norm_23.w_0@GRAD', 'layer_norm_23.b_0@GRAD'] 2233
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  check_finite_and_unscale
op type:  cast
op type:  c_allreduce_max
op type:  cast
op type:  check_finite_and_unscale
op type:  cast
op type:  c_allreduce_max
op type:  cast
op type:  concat
op type:  reduce_any
op type:  memcpy_d2h
op type:  update_loss_scaling
op type:  update_loss_scaling
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  stack
op type:  reduce_sum
op type:  cast
op type:  stack
op type:  reduce_sum
op type:  stack
op type:  reduce_sum
op type:  c_allreduce_sum
op type:  sqrt
op type:  fill_constant
op type:  elementwise_max
op type:  elementwise_div
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var input0 : LOD_TENSOR.shape(1, 1024).dtype(int64).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var input1 : LOD_TENSOR.shape(1, 1024).dtype(int64).stop_gradient(True)
    persist trainable param embedding_1.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_1.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_0.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_0.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_0.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_0.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_0.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_1.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_0.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_1.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_1.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_1.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_1.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_2.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_3.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_2.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_2.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_2.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_4.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_4.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_1.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_2.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_3.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_1.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_5.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_5.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_3.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_3.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_3.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_3.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_3.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_6.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_6.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_1.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_7.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_7.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_4.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_4.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_4.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_4.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_4.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_8.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_8.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_2.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_4.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_5.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_2.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_9.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_9.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_5.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_5.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_10.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_2.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_11.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_6.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_6.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_6.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_12.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_3.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_6.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_7.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_3.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_13.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_13.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_7.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_7.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_14.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_14.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_14.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_3.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_15.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_15.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_15.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_8.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_8.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_8.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_16.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_16.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_16.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_4.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_8.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_9.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_4.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_9.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_17.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_17.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_17.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_9.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_9.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_9.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_18.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_18.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_18.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_4.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_19.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_19.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_19.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_10.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_10.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_10.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_10.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_10.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_20.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_20.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_20.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_5.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_10.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_10.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_11.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_5.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_11.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_21.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_21.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_21.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_11.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_11.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_11.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_11.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_11.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_22.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_22.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_22.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_5.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_23.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_23.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_23.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_12.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_12.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_12.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_24.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_24.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_24.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_6.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_12.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_12.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_13.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_6.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_13.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_25.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_25.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_25.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_13.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_13.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_13.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_13.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_13.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_26.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_26.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_26.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_6.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_27.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_27.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_27.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_14.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_14.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_14.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_14.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_14.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_28.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_28.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_28.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_28.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_7.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_14.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_14.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_15.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_7.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_15.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_29.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_29.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_29.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_29.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_15.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_15.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_15.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_15.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_15.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_30.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_30.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_30.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_30.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_7.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_31.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_31.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_31.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_31.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_16.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_16.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_16.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_16.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_16.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_32.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_32.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_32.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_32.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_8.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_16.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_16.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_17.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_8.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_17.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_33.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_33.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_33.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_33.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_17.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_17.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_17.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_17.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_17.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_34.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_34.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_34.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_34.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_8.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_35.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_35.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_35.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_35.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_18.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_18.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_18.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_18.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_18.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_36.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_36.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_36.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_36.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_9.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_18.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_18.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_19.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_9.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_19.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_37.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_37.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_37.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_37.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_19.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_19.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_19.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_19.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_19.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_38.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_38.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_38.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_38.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_9.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_39.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_39.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_39.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_39.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_20.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_20.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_20.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_20.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_20.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_40.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_40.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_40.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_40.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_20.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_20.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_10.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_20.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_20.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_21.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_10.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_21.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_21.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_21.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_41.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_41.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_41.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_41.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_21.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_21.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_21.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_21.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_21.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_42.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_42.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_42.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_42.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_10.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_43.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_43.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_43.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_43.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_22.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_22.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_22.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_44.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_44.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_44.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_44.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_22.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_22.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_11.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_22.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_22.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_23.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_11.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_23.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_23.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_23.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_45.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_45.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_45.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_45.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_23.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_23.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_23.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_23.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_23.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_46.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_46.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_46.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_46.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_11.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_47.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_47.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_47.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_47.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_24 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var label0 : LOD_TENSOR.shape(1, 1024).dtype(int64).stop_gradient(True)
    var label1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_2.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_44.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_44.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_22.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_22.tmp_1.subprog_12 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_11.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_2.subprog_12 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_1.subprog_12 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_1.subprog_12 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_1.subprog_12 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_22.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_22.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_23.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_11.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_23.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_1.subprog_12 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_23.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_23.tmp_1.subprog_12 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_45.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_45.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_23.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_23.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_23.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_23.tmp_2.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_46.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_46.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_11.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_47.tmp_0.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_47.tmp_1.subprog_12 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_47.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_47.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_47.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_47.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_46.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_46.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_46.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_23.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_46.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_23.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_23.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_22@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_45.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_45.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_45.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_45.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_44.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_44.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_44.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_22.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_44.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_22.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_22.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_22@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_20.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_20.tmp_2.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_40.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_40.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_20.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_20.tmp_1.subprog_13 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_10.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_2.subprog_13 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_1.subprog_13 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_1.subprog_13 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_1.subprog_13 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_20.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_20.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_21.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_10.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_21.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_1.subprog_13 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_21.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_21.tmp_1.subprog_13 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_41.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_41.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_21.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_21.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_21.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_21.tmp_2.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_42.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_42.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_10.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_43.tmp_0.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_43.tmp_1.subprog_13 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_43.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_43.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_43.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_43.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_42.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_42.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_42.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_21.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_42.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_21.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_21.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_20@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_41.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_41.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_41.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_41.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_40.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_40.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_40.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_40.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_20.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_20.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_20@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_18.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_18.tmp_2.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_36.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_36.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_1.subprog_14 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_9.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_2.subprog_14 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_1.subprog_14 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_1.subprog_14 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_1.subprog_14 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_18.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_18.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_19.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_9.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_19.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_1.subprog_14 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_1.subprog_14 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_37.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_37.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_19.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_19.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_19.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_19.tmp_2.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_38.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_38.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_9.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_39.tmp_0.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_39.tmp_1.subprog_14 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_19@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_39.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_39.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_39.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_39.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_38.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_38.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_38.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_19.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_38.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_19.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_19.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_19@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_18@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_37.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_37.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_37.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_37.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_36.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_36.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_36.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_36.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_18.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_18.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_18@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_16.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_16.tmp_2.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_32.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_32.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_1.subprog_15 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_8.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_2.subprog_15 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_1.subprog_15 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_1.subprog_15 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_1.subprog_15 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_16.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_16.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_17.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_8.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_17.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_1.subprog_15 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_1.subprog_15 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_33.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_33.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_17.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_17.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_17.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_17.tmp_2.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_34.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_34.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_8.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_35.tmp_0.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_35.tmp_1.subprog_15 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_35.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_35.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_35.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_35.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_34.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_34.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_34.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_17.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_34.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_17.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_17.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_16@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_33.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_33.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_33.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_33.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_32.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_32.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_32.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_32.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_16.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_16.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_16@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_14.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_14.tmp_2.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_28.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_28.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_1.subprog_16 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_7.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_2.subprog_16 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_1.subprog_16 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_1.subprog_16 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_1.subprog_16 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_14.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_14.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_15.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_7.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_15.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_1.subprog_16 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_1.subprog_16 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_29.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_29.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_15.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_15.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_15.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_15.tmp_2.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_30.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_30.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_7.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_31.tmp_0.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_31.tmp_1.subprog_16 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_15@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_31.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_31.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_31.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_31.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_30.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_30.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_30.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_15.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_30.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_15.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_15.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_15@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_14@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_29.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_29.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_29.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_29.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_28.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_28.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_28.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_28.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_14.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_14.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_14@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_2.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_24.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_1.subprog_17 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_6.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_2.subprog_17 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_1.subprog_17 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_1.subprog_17 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_1.subprog_17 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_12.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_12.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_13.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_6.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_13.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_1.subprog_17 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_1.subprog_17 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_25.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_13.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_13.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_13.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_13.tmp_2.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_26.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_6.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_27.tmp_1.subprog_17 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_13@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_27.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_27.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_27.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_26.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_26.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_13.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_26.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_13.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_13.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_13@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_12@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_25.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_25.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_25.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_24.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_24.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_24.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_12.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_12.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_12@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_10.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_10.tmp_2.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_20.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_1.subprog_18 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_5.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_2.subprog_18 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_1.subprog_18 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_1.subprog_18 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_1.subprog_18 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_10.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_10.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_11.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_5.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_11.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_1.subprog_18 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_1.subprog_18 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_21.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_11.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_11.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_11.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_11.tmp_2.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_22.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_5.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_23.tmp_1.subprog_18 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_11@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_23.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_23.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_23.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_22.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_22.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_11.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_22.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_11.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_11.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_11@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_10@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_21.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_21.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_21.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_20.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_20.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_20.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_10.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_10.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_10@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_2.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_16.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_1.subprog_19 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_4.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_2.subprog_19 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1.subprog_19 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1.subprog_19 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1.subprog_19 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_8.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_9.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_4.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_9.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1.subprog_19 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1.subprog_19 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_17.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_9.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_9.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_2.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_18.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_4.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_19.tmp_1.subprog_19 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_9@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_19.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_19.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_19.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_18.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_18.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_9.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_18.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_9.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_9.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_9@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_8@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_17.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_17.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_17.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_16.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_16.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_16.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_8.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_8.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_8@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_2.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_12.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1.subprog_20 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_3.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_2.subprog_20 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1.subprog_20 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1.subprog_20 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1.subprog_20 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_6.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_7.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_3.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1.subprog_20 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1.subprog_20 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_13.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_7.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_7.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_2.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_3.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_15.tmp_1.subprog_20 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_7@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_15.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_15.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_15.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_14.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_14.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_14.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_7.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_7.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_7@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_6@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_13.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_13.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_13.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_12.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_12.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_6.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_6.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_6@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_4.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_4.tmp_2.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_8.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1.subprog_21 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_2.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_2.subprog_21 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1.subprog_21 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1.subprog_21 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1.subprog_21 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_4.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_5.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_2.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1.subprog_21 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1.subprog_21 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_9.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_5.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_5.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_2.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_2.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_11.tmp_1.subprog_21 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_11.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_11.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_10.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_10.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_5.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_5.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_4@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_9.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_9.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_8.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_8.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_4.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_4.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_4@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_2.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_4.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1.subprog_22 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_1.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_2.subprog_22 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1.subprog_22 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1.subprog_22 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1.subprog_22 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_2.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_3.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_1.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1.subprog_22 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_1.subprog_22 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_5.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_3.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_3.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_3.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_3.tmp_2.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_1.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_7.tmp_1.subprog_22 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_3@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_7.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_7.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_6.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_6.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_3.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_3.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_3.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_3@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_5.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_5.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_4.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_4.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_2.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_2.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_2.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_0.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1.subprog_23 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_2.subprog_23 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1.subprog_23 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1.subprog_23 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1.subprog_23 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_1.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1.subprog_23 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1.subprog_23 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_1.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_1.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_2.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_0.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_3.tmp_1.subprog_23 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_1@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_3.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_3.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_2.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_2.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_1.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_1.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_1.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_1@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_1.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_1.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_0.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_0.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_1.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp32_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp32_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(2,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var opt_opt_squared_l2_norm_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_19.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_20.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_21.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_22.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_23.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_24.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_25.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_27.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_28.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_29.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_30.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_32.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_34.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_35.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_36.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_39.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_40.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_41.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_42.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_43.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_44.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_45.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_46.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_47.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_48.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_49.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_50.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_51.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_52.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_53.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_54.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_55.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_56.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_57.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_58.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_59.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_60.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_61.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_62.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_63.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_64.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_65.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_66.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_67.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_68.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_69.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_70.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_71.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_72.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_73.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_74.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_75.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_76.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_77.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_78.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_79.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_80.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_81.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_82.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_83.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_84.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_85.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_86.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_87.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_88.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_89.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_90.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_91.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_92.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_93.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_94.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_95.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_96.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_97.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_98.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_99.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_100.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_101.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_102.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_103.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_104.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_105.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_106.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_107.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_108.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_109.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_110.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_111.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_112.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_113.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_114.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_115.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_116.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_117.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_118.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_119.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_120.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_121.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_122.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_123.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_124.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_125.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_126.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_127.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_128.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_129.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_130.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_131.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_132.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_133.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_134.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_135.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_136.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_137.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_138.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_139.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_140.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_141.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_142.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_143.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_144.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_145.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_146.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_147.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_148.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_149.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_150.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_151.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_152.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_153.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_154.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_155.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_156.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_157.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_158.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_159.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_160.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_161.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_162.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_163.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_164.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_165.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_166.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_167.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_168.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_169.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_170.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_171.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_172.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_173.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_174.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_175.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_176.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_177.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_178.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_179.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_180.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_181.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_182.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_183.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_184.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_185.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_186.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_187.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_188.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_189.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_190.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_191.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_192.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_193.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_194.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_195.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_196.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_197.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_198.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_199.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_200.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_201.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_202.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_203.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_204.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_205.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_206.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_207.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_208.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_209.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_210.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_211.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_212.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_213.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_214.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_215.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_216.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_217.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_218.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_219.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_220.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_221.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_222.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_223.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_224.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_225.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_226.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_227.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_228.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_229.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_230.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_231.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_232.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_233.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_234.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_235.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_236.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_237.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_238.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_239.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_240.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_241.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_242.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_243.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_244.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_245.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_246.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_247.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_248.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_249.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_250.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_251.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_252.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_253.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_254.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_255.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_256.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_257.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_258.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_259.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_260.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_261.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_262.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_263.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_264.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_265.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_266.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_267.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_268.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_269.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_270.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_271.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_272.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_273.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_274.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_275.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_276.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_277.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_278.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_279.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_280.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_281.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_282.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_283.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_284.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_285.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_286.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_287.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_288.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_289.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_290.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_291.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_stack_0.tmp_0 : LOD_TENSOR.shape(194, 1).dtype(float16).stop_gradient(False)
    var opt_opt_sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var opt_tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_stack_1.tmp_0 : LOD_TENSOR.shape(98, 1).dtype(float32).stop_gradient(False)
    var opt_opt_sum_1.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_stack_2.tmp_0 : LOD_TENSOR.shape(2,).dtype(float32).stop_gradient(False)
    var opt_opt_sum_2.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_tmp_1 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_2 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_3 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_4 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_5 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_6 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_7 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_8 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_9 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_10 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_11 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_12 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_13 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_14 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_15 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_16 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_17 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_18 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_19 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_20 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_21 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_22 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_23 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_24 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_25 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_26 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_27 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_28 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_29 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_30 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_31 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_32 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_33 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_34 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_35 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_36 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_37 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_38 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_39 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_40 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_41 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_42 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_43 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_44 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_45 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_46 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_47 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_48 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_49 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_50 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_51 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_52 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_53 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_54 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_55 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_56 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_57 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_58 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_59 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_60 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_61 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_62 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_63 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_64 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_65 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_66 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_67 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_68 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_69 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_70 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_71 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_72 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_73 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_74 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_75 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_76 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_77 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_78 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_79 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_80 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_81 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_82 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_83 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_84 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_85 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_86 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_87 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_88 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_89 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_90 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_91 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_92 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_93 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_94 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_95 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_96 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_97 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_98 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var embedding_0.w_0_fp32_master_0 : LOD_TENSOR.shape(50304, 1024).dtype(float32).stop_gradient(True)
    persist var embedding_0.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(50304, 1024).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(50304, 1024).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var embedding_1.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var embedding_1.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var embedding_1.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var embedding_1.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var embedding_1.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_0.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_0.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_1.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_1.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_2.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_2.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_3.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_3.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_4.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_4.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_5.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_5.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_6.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_6.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_7.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_7.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_8.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_8.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_9.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_9.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_10.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_10.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_11.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_11.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_12.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_12.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_12.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_12.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_13.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_13.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_13.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_13.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_14.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_14.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_14.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_14.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_14.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_14.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_15.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_15.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_15.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_15.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_16.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_16.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_16.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_16.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_16.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_16.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_17.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_17.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_17.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_17.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_18.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_18.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_18.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_18.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_19.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_19.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_19.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_19.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_19.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_19.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_20.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_20.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_20.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_20.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_21.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_21.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_21.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_21.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_21.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_21.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_22.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_22.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_22.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_22.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_22.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_22.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_23.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_23.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_23.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_23.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_23.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_23.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_24.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_24.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_24.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_24.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_25.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_25.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_25.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_25.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_25.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_25.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_26.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_26.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_26.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_26.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_27.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_27.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_27.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_27.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_27.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_27.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_28.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_28.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_28.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_28.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_28.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_28.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_28.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_28.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_28.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_28.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_29.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_29.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_29.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_29.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_29.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_29.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_29.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_29.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_29.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_29.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_30.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_30.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_30.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_30.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_30.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_30.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_30.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_30.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_30.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_30.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_31.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_31.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_31.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_31.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_31.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_31.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_31.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_31.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_31.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_31.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_32.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_32.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_32.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_32.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_32.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_32.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_32.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_32.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_32.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_32.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_33.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_33.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_33.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_33.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_33.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_33.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_33.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_33.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_33.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_33.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_34.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_34.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_34.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_34.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_34.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_34.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_34.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_34.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_34.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_34.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_35.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_35.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_35.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_35.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_35.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_35.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_35.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_35.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_35.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_35.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_36.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_36.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_36.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_36.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_36.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_36.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_36.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_36.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_36.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_36.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_37.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_37.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_37.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_37.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_37.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_37.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_37.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_37.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_37.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_37.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_38.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_38.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_38.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_38.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_38.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_38.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_38.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_38.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_38.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_38.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_39.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_39.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_39.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_39.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_39.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_39.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_39.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_39.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_39.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_39.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_40.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_40.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_40.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_40.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_40.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_40.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_40.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_40.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_40.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_40.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_41.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_41.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_41.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_41.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_41.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_41.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_41.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_41.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_41.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_41.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_42.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_42.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_42.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_42.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_42.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_42.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_42.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_42.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_42.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_42.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_43.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_43.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_43.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_43.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_43.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_43.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_43.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_43.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_43.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_43.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_44.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_44.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_44.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_44.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_44.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_44.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_44.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_44.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_44.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_44.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_45.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_45.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_45.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_45.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_45.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_45.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_45.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_45.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_45.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_45.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_46.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_46.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_46.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_46.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_46.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_46.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_46.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_46.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_46.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_46.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_47.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_47.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_47.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_47.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_47.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_47.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_47.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_47.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_47.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_47.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_24@GRAD@recv_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_24@GRAD@RESHARD_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD@RENAME@block0@0@recv_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD@RENAME@block0@0@RESHARD_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    persist var embedding_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    persist var embedding_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var layer_norm_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_0.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_0.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_1.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_1.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_2.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_3.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_2.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_4.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_5.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_5.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_3.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_6.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_7.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_7.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_4.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_4.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_8.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_9.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_9.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_5.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_5.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_10.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_11.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_6.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_6.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_12.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_13.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_13.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_7.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_7.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_14.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_15.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_15.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_8.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_8.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_16.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_17.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_17.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_9.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_9.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_18.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_19.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_19.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_10.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_10.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_20.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_21.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_21.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_11.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_11.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_22.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_23.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_23.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_12.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_12.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_24.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_25.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_25.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_13.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_13.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_26.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_27.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_27.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_14.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_14.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_28.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_28.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_29.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_29.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_15.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_15.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_30.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_30.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_31.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_31.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_16.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_16.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_32.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_32.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_33.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_33.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_17.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_17.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_34.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_34.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_35.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_35.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_18.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_18.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_36.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_36.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_37.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_37.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_19.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_19.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_38.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_38.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_39.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_39.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_20.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_20.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_40.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_40.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_41.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_41.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_21.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_21.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_42.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_42.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_43.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_43.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_22.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_22.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_44.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_44.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_45.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_45.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_23.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_23.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_46.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_46.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_47.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_47.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var gradient_merge_k : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_zero : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_step : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_cond : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(True)
    var _generated_var_0 : STEP_SCOPES)

    {Out=['embedding_0.tmp_0']} = lookup_table_v2(inputs={Ids=['input0'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, with_quant_attr = False)
    {Out=['embedding_1.tmp_0']} = lookup_table_v2(inputs={Ids=['input1'], W=['embedding_1.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, with_quant_attr = False)
    {Out=['tmp_0']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['embedding_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_0.tmp_0'], Variance=['layer_norm_0.tmp_1'], Y=['layer_norm_0.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_0.b_0'], Scale=['layer_norm_0.w_0'], X=['tmp_0']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_0.tmp_0']} = matmul_v2(inputs={X=['layer_norm_0.tmp_2'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_0.tmp_1']} = elementwise_add(inputs={X=['linear_0.tmp_0'], Y=['linear_0.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_0.tmp_0', 'split_0.tmp_1', 'split_0.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_0.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['split_0.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['split_0.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['split_0.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_0.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['scale_0.tmp_0'], Y=['transpose_1.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_1.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_0.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_1.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_0.tmp_0'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.tmp_0']} = matmul_v2(inputs={X=['reshape2_1.tmp_0'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_1.tmp_1']} = elementwise_add(inputs={X=['linear_1.tmp_0'], Y=['linear_1.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1']} = elementwise_add(inputs={X=['tmp_0'], Y=['linear_1.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_1.tmp_0'], Variance=['layer_norm_1.tmp_1'], Y=['layer_norm_1.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_1.b_0'], Scale=['layer_norm_1.w_0'], X=['tmp_1']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_2.tmp_0']} = matmul_v2(inputs={X=['layer_norm_1.tmp_2'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_2.tmp_1']} = elementwise_add(inputs={X=['linear_2.tmp_0'], Y=['linear_2.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_0.tmp_0']} = gelu(inputs={X=['linear_2.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_3.tmp_0']} = matmul_v2(inputs={X=['gelu_0.tmp_0'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_3.tmp_1']} = elementwise_add(inputs={X=['linear_3.tmp_0'], Y=['linear_3.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_2']} = elementwise_add(inputs={X=['tmp_1'], Y=['linear_3.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_2.tmp_0'], Variance=['layer_norm_2.tmp_1'], Y=['layer_norm_2.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_2.b_0'], Scale=['layer_norm_2.w_0'], X=['tmp_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_4.tmp_0']} = matmul_v2(inputs={X=['layer_norm_2.tmp_2'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_4.tmp_1']} = elementwise_add(inputs={X=['linear_4.tmp_0'], Y=['linear_4.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_4.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_1.tmp_0', 'split_1.tmp_1', 'split_1.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_2.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['split_1.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['split_1.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['split_1.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_2.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_4.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['scale_2.tmp_0'], Y=['transpose_5.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_3.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_2.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_1.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_1.tmp_0'], Y=['transpose_6.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['matmul_v2_3.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_5.tmp_0']} = matmul_v2(inputs={X=['reshape2_3.tmp_0'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_5.tmp_1']} = elementwise_add(inputs={X=['linear_5.tmp_0'], Y=['linear_5.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_3']} = elementwise_add(inputs={X=['tmp_2'], Y=['linear_5.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_3.tmp_0'], Variance=['layer_norm_3.tmp_1'], Y=['layer_norm_3.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_3.b_0'], Scale=['layer_norm_3.w_0'], X=['tmp_3']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_6.tmp_0']} = matmul_v2(inputs={X=['layer_norm_3.tmp_2'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_6.tmp_1']} = elementwise_add(inputs={X=['linear_6.tmp_0'], Y=['linear_6.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_1.tmp_0']} = gelu(inputs={X=['linear_6.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_7.tmp_0']} = matmul_v2(inputs={X=['gelu_1.tmp_0'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_7.tmp_1']} = elementwise_add(inputs={X=['linear_7.tmp_0'], Y=['linear_7.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_4']} = elementwise_add(inputs={X=['tmp_3'], Y=['linear_7.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_4.tmp_0'], Variance=['layer_norm_4.tmp_1'], Y=['layer_norm_4.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_4.b_0'], Scale=['layer_norm_4.w_0'], X=['tmp_4']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_8.tmp_0']} = matmul_v2(inputs={X=['layer_norm_4.tmp_2'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_8.tmp_1']} = elementwise_add(inputs={X=['linear_8.tmp_0'], Y=['linear_8.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_2.tmp_0', 'split_2.tmp_1', 'split_2.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_4.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['split_2.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['split_2.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['split_2.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_4.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_8.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_4.tmp_0']} = matmul_v2(inputs={X=['scale_4.tmp_0'], Y=['transpose_9.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_5.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_4.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_2.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_5.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_5.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_2.tmp_0'], Y=['transpose_10.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_11.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.tmp_0']} = matmul_v2(inputs={X=['reshape2_5.tmp_0'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_9.tmp_1']} = elementwise_add(inputs={X=['linear_9.tmp_0'], Y=['linear_9.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_5']} = elementwise_add(inputs={X=['tmp_4'], Y=['linear_9.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_5.tmp_0'], Variance=['layer_norm_5.tmp_1'], Y=['layer_norm_5.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_5.b_0'], Scale=['layer_norm_5.w_0'], X=['tmp_5']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_10.tmp_0']} = matmul_v2(inputs={X=['layer_norm_5.tmp_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_10.tmp_1']} = elementwise_add(inputs={X=['linear_10.tmp_0'], Y=['linear_10.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_2.tmp_0']} = gelu(inputs={X=['linear_10.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_11.tmp_0']} = matmul_v2(inputs={X=['gelu_2.tmp_0'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_11.tmp_1']} = elementwise_add(inputs={X=['linear_11.tmp_0'], Y=['linear_11.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_6']} = elementwise_add(inputs={X=['tmp_5'], Y=['linear_11.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_6.tmp_0'], Variance=['layer_norm_6.tmp_1'], Y=['layer_norm_6.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_6.b_0'], Scale=['layer_norm_6.w_0'], X=['tmp_6']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_12.tmp_0']} = matmul_v2(inputs={X=['layer_norm_6.tmp_2'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_12.tmp_1']} = elementwise_add(inputs={X=['linear_12.tmp_0'], Y=['linear_12.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_12.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_3.tmp_0', 'split_3.tmp_1', 'split_3.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_6.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_12.tmp_0'], XShape=['transpose_12.tmp_1']} = transpose2(inputs={X=['split_3.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_13.tmp_0'], XShape=['transpose_13.tmp_1']} = transpose2(inputs={X=['split_3.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_14.tmp_0'], XShape=['transpose_14.tmp_1']} = transpose2(inputs={X=['split_3.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_6.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_12.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_6.tmp_0']} = matmul_v2(inputs={X=['scale_6.tmp_0'], Y=['transpose_13.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_7.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_6.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_3.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_7.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_7.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_3.tmp_0'], Y=['transpose_14.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_15.tmp_0'], XShape=['transpose_15.tmp_1']} = transpose2(inputs={X=['matmul_v2_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_13.tmp_0']} = matmul_v2(inputs={X=['reshape2_7.tmp_0'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_13.tmp_1']} = elementwise_add(inputs={X=['linear_13.tmp_0'], Y=['linear_13.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_7']} = elementwise_add(inputs={X=['tmp_6'], Y=['linear_13.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_7.tmp_0'], Variance=['layer_norm_7.tmp_1'], Y=['layer_norm_7.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_7.b_0'], Scale=['layer_norm_7.w_0'], X=['tmp_7']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_14.tmp_0']} = matmul_v2(inputs={X=['layer_norm_7.tmp_2'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_14.tmp_1']} = elementwise_add(inputs={X=['linear_14.tmp_0'], Y=['linear_14.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_3.tmp_0']} = gelu(inputs={X=['linear_14.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_15.tmp_0']} = matmul_v2(inputs={X=['gelu_3.tmp_0'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_15.tmp_1']} = elementwise_add(inputs={X=['linear_15.tmp_0'], Y=['linear_15.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_8']} = elementwise_add(inputs={X=['tmp_7'], Y=['linear_15.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_8.tmp_0'], Variance=['layer_norm_8.tmp_1'], Y=['layer_norm_8.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_8.b_0'], Scale=['layer_norm_8.w_0'], X=['tmp_8']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_16.tmp_0']} = matmul_v2(inputs={X=['layer_norm_8.tmp_2'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_16.tmp_1']} = elementwise_add(inputs={X=['linear_16.tmp_0'], Y=['linear_16.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_4.tmp_0', 'split_4.tmp_1', 'split_4.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_8.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_16.tmp_0'], XShape=['transpose_16.tmp_1']} = transpose2(inputs={X=['split_4.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_17.tmp_0'], XShape=['transpose_17.tmp_1']} = transpose2(inputs={X=['split_4.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_18.tmp_0'], XShape=['transpose_18.tmp_1']} = transpose2(inputs={X=['split_4.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_8.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_16.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0']} = matmul_v2(inputs={X=['scale_8.tmp_0'], Y=['transpose_17.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_9.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_8.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_4.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_9.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_9.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_4.tmp_0'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_19.tmp_0'], XShape=['transpose_19.tmp_1']} = transpose2(inputs={X=['matmul_v2_9.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_17.tmp_0']} = matmul_v2(inputs={X=['reshape2_9.tmp_0'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_17.tmp_1']} = elementwise_add(inputs={X=['linear_17.tmp_0'], Y=['linear_17.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_9']} = elementwise_add(inputs={X=['tmp_8'], Y=['linear_17.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_9.tmp_0'], Variance=['layer_norm_9.tmp_1'], Y=['layer_norm_9.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_9.b_0'], Scale=['layer_norm_9.w_0'], X=['tmp_9']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_18.tmp_0']} = matmul_v2(inputs={X=['layer_norm_9.tmp_2'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_18.tmp_1']} = elementwise_add(inputs={X=['linear_18.tmp_0'], Y=['linear_18.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_4.tmp_0']} = gelu(inputs={X=['linear_18.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_19.tmp_0']} = matmul_v2(inputs={X=['gelu_4.tmp_0'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_19.tmp_1']} = elementwise_add(inputs={X=['linear_19.tmp_0'], Y=['linear_19.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_10']} = elementwise_add(inputs={X=['tmp_9'], Y=['linear_19.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_10.tmp_0'], Variance=['layer_norm_10.tmp_1'], Y=['layer_norm_10.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_10.b_0'], Scale=['layer_norm_10.w_0'], X=['tmp_10']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_20.tmp_0']} = matmul_v2(inputs={X=['layer_norm_10.tmp_2'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_20.tmp_1']} = elementwise_add(inputs={X=['linear_20.tmp_0'], Y=['linear_20.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0'], XShape=['reshape2_10.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_20.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_5.tmp_0', 'split_5.tmp_1', 'split_5.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_10.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_20.tmp_0'], XShape=['transpose_20.tmp_1']} = transpose2(inputs={X=['split_5.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_21.tmp_0'], XShape=['transpose_21.tmp_1']} = transpose2(inputs={X=['split_5.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_22.tmp_0'], XShape=['transpose_22.tmp_1']} = transpose2(inputs={X=['split_5.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_10.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_20.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_10.tmp_0']} = matmul_v2(inputs={X=['scale_10.tmp_0'], Y=['transpose_21.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_11.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_10.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_5.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_11.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_11.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_5.tmp_0'], Y=['transpose_22.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_23.tmp_0'], XShape=['transpose_23.tmp_1']} = transpose2(inputs={X=['matmul_v2_11.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0'], XShape=['reshape2_11.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_21.tmp_0']} = matmul_v2(inputs={X=['reshape2_11.tmp_0'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_21.tmp_1']} = elementwise_add(inputs={X=['linear_21.tmp_0'], Y=['linear_21.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_11']} = elementwise_add(inputs={X=['tmp_10'], Y=['linear_21.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_11.tmp_0'], Variance=['layer_norm_11.tmp_1'], Y=['layer_norm_11.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_11.b_0'], Scale=['layer_norm_11.w_0'], X=['tmp_11']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_22.tmp_0']} = matmul_v2(inputs={X=['layer_norm_11.tmp_2'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_22.tmp_1']} = elementwise_add(inputs={X=['linear_22.tmp_0'], Y=['linear_22.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_5.tmp_0']} = gelu(inputs={X=['linear_22.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_23.tmp_0']} = matmul_v2(inputs={X=['gelu_5.tmp_0'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_23.tmp_1']} = elementwise_add(inputs={X=['linear_23.tmp_0'], Y=['linear_23.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_12']} = elementwise_add(inputs={X=['tmp_11'], Y=['linear_23.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_12.tmp_0'], Variance=['layer_norm_12.tmp_1'], Y=['layer_norm_12.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_12.b_0'], Scale=['layer_norm_12.w_0'], X=['tmp_12']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_24.tmp_0']} = matmul_v2(inputs={X=['layer_norm_12.tmp_2'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_24.tmp_1']} = elementwise_add(inputs={X=['linear_24.tmp_0'], Y=['linear_24.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_12.tmp_0'], XShape=['reshape2_12.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_24.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_6.tmp_0', 'split_6.tmp_1', 'split_6.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_12.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_24.tmp_0'], XShape=['transpose_24.tmp_1']} = transpose2(inputs={X=['split_6.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_25.tmp_0'], XShape=['transpose_25.tmp_1']} = transpose2(inputs={X=['split_6.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_26.tmp_0'], XShape=['transpose_26.tmp_1']} = transpose2(inputs={X=['split_6.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_12.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_24.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_12.tmp_0']} = matmul_v2(inputs={X=['scale_12.tmp_0'], Y=['transpose_25.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_13.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_12.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_6.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_13.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_13.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_6.tmp_0'], Y=['transpose_26.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_27.tmp_0'], XShape=['transpose_27.tmp_1']} = transpose2(inputs={X=['matmul_v2_13.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_13.tmp_0'], XShape=['reshape2_13.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_27.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_25.tmp_0']} = matmul_v2(inputs={X=['reshape2_13.tmp_0'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_25.tmp_1']} = elementwise_add(inputs={X=['linear_25.tmp_0'], Y=['linear_25.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_13']} = elementwise_add(inputs={X=['tmp_12'], Y=['linear_25.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_13.tmp_0'], Variance=['layer_norm_13.tmp_1'], Y=['layer_norm_13.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_13.b_0'], Scale=['layer_norm_13.w_0'], X=['tmp_13']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_26.tmp_0']} = matmul_v2(inputs={X=['layer_norm_13.tmp_2'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_26.tmp_1']} = elementwise_add(inputs={X=['linear_26.tmp_0'], Y=['linear_26.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_6.tmp_0']} = gelu(inputs={X=['linear_26.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_27.tmp_0']} = matmul_v2(inputs={X=['gelu_6.tmp_0'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_27.tmp_1']} = elementwise_add(inputs={X=['linear_27.tmp_0'], Y=['linear_27.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_14']} = elementwise_add(inputs={X=['tmp_13'], Y=['linear_27.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_14.tmp_0'], Variance=['layer_norm_14.tmp_1'], Y=['layer_norm_14.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_14.b_0'], Scale=['layer_norm_14.w_0'], X=['tmp_14']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_28.tmp_0']} = matmul_v2(inputs={X=['layer_norm_14.tmp_2'], Y=['linear_28.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_28.tmp_1']} = elementwise_add(inputs={X=['linear_28.tmp_0'], Y=['linear_28.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_14.tmp_0'], XShape=['reshape2_14.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_28.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_7.tmp_0', 'split_7.tmp_1', 'split_7.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_14.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_28.tmp_0'], XShape=['transpose_28.tmp_1']} = transpose2(inputs={X=['split_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_29.tmp_0'], XShape=['transpose_29.tmp_1']} = transpose2(inputs={X=['split_7.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_30.tmp_0'], XShape=['transpose_30.tmp_1']} = transpose2(inputs={X=['split_7.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_14.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_28.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_14.tmp_0']} = matmul_v2(inputs={X=['scale_14.tmp_0'], Y=['transpose_29.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_15.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_14.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_7.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_15.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_15.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_7.tmp_0'], Y=['transpose_30.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_31.tmp_0'], XShape=['transpose_31.tmp_1']} = transpose2(inputs={X=['matmul_v2_15.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0'], XShape=['reshape2_15.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_31.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_29.tmp_0']} = matmul_v2(inputs={X=['reshape2_15.tmp_0'], Y=['linear_29.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_29.tmp_1']} = elementwise_add(inputs={X=['linear_29.tmp_0'], Y=['linear_29.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_15']} = elementwise_add(inputs={X=['tmp_14'], Y=['linear_29.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_15.tmp_0'], Variance=['layer_norm_15.tmp_1'], Y=['layer_norm_15.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_15.b_0'], Scale=['layer_norm_15.w_0'], X=['tmp_15']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_30.tmp_0']} = matmul_v2(inputs={X=['layer_norm_15.tmp_2'], Y=['linear_30.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_30.tmp_1']} = elementwise_add(inputs={X=['linear_30.tmp_0'], Y=['linear_30.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_7.tmp_0']} = gelu(inputs={X=['linear_30.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_31.tmp_0']} = matmul_v2(inputs={X=['gelu_7.tmp_0'], Y=['linear_31.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_31.tmp_1']} = elementwise_add(inputs={X=['linear_31.tmp_0'], Y=['linear_31.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_16']} = elementwise_add(inputs={X=['tmp_15'], Y=['linear_31.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_16.tmp_0'], Variance=['layer_norm_16.tmp_1'], Y=['layer_norm_16.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_16.b_0'], Scale=['layer_norm_16.w_0'], X=['tmp_16']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_32.tmp_0']} = matmul_v2(inputs={X=['layer_norm_16.tmp_2'], Y=['linear_32.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_32.tmp_1']} = elementwise_add(inputs={X=['linear_32.tmp_0'], Y=['linear_32.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0'], XShape=['reshape2_16.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_32.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_8.tmp_0', 'split_8.tmp_1', 'split_8.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_16.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_32.tmp_0'], XShape=['transpose_32.tmp_1']} = transpose2(inputs={X=['split_8.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_33.tmp_0'], XShape=['transpose_33.tmp_1']} = transpose2(inputs={X=['split_8.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_34.tmp_0'], XShape=['transpose_34.tmp_1']} = transpose2(inputs={X=['split_8.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_16.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_32.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_16.tmp_0']} = matmul_v2(inputs={X=['scale_16.tmp_0'], Y=['transpose_33.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_17.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_16.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_8.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_17.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_17.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_8.tmp_0'], Y=['transpose_34.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_35.tmp_0'], XShape=['transpose_35.tmp_1']} = transpose2(inputs={X=['matmul_v2_17.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_17.tmp_0'], XShape=['reshape2_17.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_35.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_33.tmp_0']} = matmul_v2(inputs={X=['reshape2_17.tmp_0'], Y=['linear_33.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_33.tmp_1']} = elementwise_add(inputs={X=['linear_33.tmp_0'], Y=['linear_33.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_17']} = elementwise_add(inputs={X=['tmp_16'], Y=['linear_33.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_17.tmp_0'], Variance=['layer_norm_17.tmp_1'], Y=['layer_norm_17.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_17.b_0'], Scale=['layer_norm_17.w_0'], X=['tmp_17']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_34.tmp_0']} = matmul_v2(inputs={X=['layer_norm_17.tmp_2'], Y=['linear_34.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_34.tmp_1']} = elementwise_add(inputs={X=['linear_34.tmp_0'], Y=['linear_34.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_8.tmp_0']} = gelu(inputs={X=['linear_34.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_35.tmp_0']} = matmul_v2(inputs={X=['gelu_8.tmp_0'], Y=['linear_35.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_35.tmp_1']} = elementwise_add(inputs={X=['linear_35.tmp_0'], Y=['linear_35.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_18']} = elementwise_add(inputs={X=['tmp_17'], Y=['linear_35.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_18.tmp_0'], Variance=['layer_norm_18.tmp_1'], Y=['layer_norm_18.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_18.b_0'], Scale=['layer_norm_18.w_0'], X=['tmp_18']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_36.tmp_0']} = matmul_v2(inputs={X=['layer_norm_18.tmp_2'], Y=['linear_36.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_36.tmp_1']} = elementwise_add(inputs={X=['linear_36.tmp_0'], Y=['linear_36.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_18.tmp_0'], XShape=['reshape2_18.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_36.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_9.tmp_0', 'split_9.tmp_1', 'split_9.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_18.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_36.tmp_0'], XShape=['transpose_36.tmp_1']} = transpose2(inputs={X=['split_9.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_37.tmp_0'], XShape=['transpose_37.tmp_1']} = transpose2(inputs={X=['split_9.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_38.tmp_0'], XShape=['transpose_38.tmp_1']} = transpose2(inputs={X=['split_9.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_18.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_36.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_18.tmp_0']} = matmul_v2(inputs={X=['scale_18.tmp_0'], Y=['transpose_37.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_19.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_18.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_9.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_19.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_19.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_9.tmp_0'], Y=['transpose_38.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_39.tmp_0'], XShape=['transpose_39.tmp_1']} = transpose2(inputs={X=['matmul_v2_19.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_19.tmp_0'], XShape=['reshape2_19.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_39.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_37.tmp_0']} = matmul_v2(inputs={X=['reshape2_19.tmp_0'], Y=['linear_37.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_37.tmp_1']} = elementwise_add(inputs={X=['linear_37.tmp_0'], Y=['linear_37.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_19']} = elementwise_add(inputs={X=['tmp_18'], Y=['linear_37.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_19.tmp_0'], Variance=['layer_norm_19.tmp_1'], Y=['layer_norm_19.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_19.b_0'], Scale=['layer_norm_19.w_0'], X=['tmp_19']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_38.tmp_0']} = matmul_v2(inputs={X=['layer_norm_19.tmp_2'], Y=['linear_38.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_38.tmp_1']} = elementwise_add(inputs={X=['linear_38.tmp_0'], Y=['linear_38.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_9.tmp_0']} = gelu(inputs={X=['linear_38.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_39.tmp_0']} = matmul_v2(inputs={X=['gelu_9.tmp_0'], Y=['linear_39.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_39.tmp_1']} = elementwise_add(inputs={X=['linear_39.tmp_0'], Y=['linear_39.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_20']} = elementwise_add(inputs={X=['tmp_19'], Y=['linear_39.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_20.tmp_0'], Variance=['layer_norm_20.tmp_1'], Y=['layer_norm_20.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_20.b_0'], Scale=['layer_norm_20.w_0'], X=['tmp_20']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_40.tmp_0']} = matmul_v2(inputs={X=['layer_norm_20.tmp_2'], Y=['linear_40.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_40.tmp_1']} = elementwise_add(inputs={X=['linear_40.tmp_0'], Y=['linear_40.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_20.tmp_0'], XShape=['reshape2_20.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_40.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_10.tmp_0', 'split_10.tmp_1', 'split_10.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_20.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_40.tmp_0'], XShape=['transpose_40.tmp_1']} = transpose2(inputs={X=['split_10.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_41.tmp_0'], XShape=['transpose_41.tmp_1']} = transpose2(inputs={X=['split_10.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_42.tmp_0'], XShape=['transpose_42.tmp_1']} = transpose2(inputs={X=['split_10.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_20.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_40.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_20.tmp_0']} = matmul_v2(inputs={X=['scale_20.tmp_0'], Y=['transpose_41.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_21.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_20.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_10.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_21.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_21.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_10.tmp_0'], Y=['transpose_42.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_43.tmp_0'], XShape=['transpose_43.tmp_1']} = transpose2(inputs={X=['matmul_v2_21.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_21.tmp_0'], XShape=['reshape2_21.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_43.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_41.tmp_0']} = matmul_v2(inputs={X=['reshape2_21.tmp_0'], Y=['linear_41.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_41.tmp_1']} = elementwise_add(inputs={X=['linear_41.tmp_0'], Y=['linear_41.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21']} = elementwise_add(inputs={X=['tmp_20'], Y=['linear_41.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_21.tmp_0'], Variance=['layer_norm_21.tmp_1'], Y=['layer_norm_21.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_21.b_0'], Scale=['layer_norm_21.w_0'], X=['tmp_21']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_42.tmp_0']} = matmul_v2(inputs={X=['layer_norm_21.tmp_2'], Y=['linear_42.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_42.tmp_1']} = elementwise_add(inputs={X=['linear_42.tmp_0'], Y=['linear_42.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_10.tmp_0']} = gelu(inputs={X=['linear_42.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_43.tmp_0']} = matmul_v2(inputs={X=['gelu_10.tmp_0'], Y=['linear_43.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_43.tmp_1']} = elementwise_add(inputs={X=['linear_43.tmp_0'], Y=['linear_43.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_22']} = elementwise_add(inputs={X=['tmp_21'], Y=['linear_43.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_22.tmp_0'], Variance=['layer_norm_22.tmp_1'], Y=['layer_norm_22.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_22.b_0'], Scale=['layer_norm_22.w_0'], X=['tmp_22']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_44.tmp_0']} = matmul_v2(inputs={X=['layer_norm_22.tmp_2'], Y=['linear_44.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_44.tmp_1']} = elementwise_add(inputs={X=['linear_44.tmp_0'], Y=['linear_44.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_22.tmp_0'], XShape=['reshape2_22.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_44.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_11.tmp_0', 'split_11.tmp_1', 'split_11.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_22.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_44.tmp_0'], XShape=['transpose_44.tmp_1']} = transpose2(inputs={X=['split_11.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_45.tmp_0'], XShape=['transpose_45.tmp_1']} = transpose2(inputs={X=['split_11.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_46.tmp_0'], XShape=['transpose_46.tmp_1']} = transpose2(inputs={X=['split_11.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_22.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_44.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_22.tmp_0']} = matmul_v2(inputs={X=['scale_22.tmp_0'], Y=['transpose_45.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_23.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_22.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_11.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_23.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_23.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_11.tmp_0'], Y=['transpose_46.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_47.tmp_0'], XShape=['transpose_47.tmp_1']} = transpose2(inputs={X=['matmul_v2_23.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_23.tmp_0'], XShape=['reshape2_23.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_47.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_45.tmp_0']} = matmul_v2(inputs={X=['reshape2_23.tmp_0'], Y=['linear_45.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_45.tmp_1']} = elementwise_add(inputs={X=['linear_45.tmp_0'], Y=['linear_45.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_23']} = elementwise_add(inputs={X=['tmp_22'], Y=['linear_45.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_23.tmp_0'], Variance=['layer_norm_23.tmp_1'], Y=['layer_norm_23.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_23.b_0'], Scale=['layer_norm_23.w_0'], X=['tmp_23']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_46.tmp_0']} = matmul_v2(inputs={X=['layer_norm_23.tmp_2'], Y=['linear_46.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_46.tmp_1']} = elementwise_add(inputs={X=['linear_46.tmp_0'], Y=['linear_46.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_11.tmp_0']} = gelu(inputs={X=['linear_46.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_47.tmp_0']} = matmul_v2(inputs={X=['gelu_11.tmp_0'], Y=['linear_47.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_47.tmp_1']} = elementwise_add(inputs={X=['linear_47.tmp_0'], Y=['linear_47.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_24']} = elementwise_add(inputs={X=['tmp_23'], Y=['linear_47.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 0, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_24']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 33, use_calc_stream = True, with_quant_attr = False)
    send_v2(inputs={X=['embedding_0.w_0']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 33, use_calc_stream = True, with_quant_attr = False)
    {Mean=['layer_norm_22.tmp_0.subprog_12'], Variance=['layer_norm_22.tmp_1.subprog_12'], Y=['layer_norm_22.tmp_2.subprog_12']} = layer_norm(inputs={Bias=['layer_norm_22.b_0'], Scale=['layer_norm_22.w_0'], X=['tmp_22']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_44.tmp_0.subprog_12']} = matmul_v2(inputs={X=['layer_norm_22.tmp_2.subprog_12'], Y=['linear_44.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_44.tmp_1.subprog_12']} = elementwise_add(inputs={X=['linear_44.tmp_0.subprog_12'], Y=['linear_44.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_22.tmp_0.subprog_12'], XShape=['reshape2_22.tmp_1.subprog_12']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_44.tmp_1.subprog_12']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_11.tmp_0.subprog_12', 'split_11.tmp_1.subprog_12', 'split_11.tmp_2.subprog_12']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_22.tmp_0.subprog_12']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_44.tmp_0.subprog_12'], XShape=['transpose_44.tmp_1.subprog_12']} = transpose2(inputs={X=['split_11.tmp_0.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_45.tmp_0.subprog_12'], XShape=['transpose_45.tmp_1.subprog_12']} = transpose2(inputs={X=['split_11.tmp_1.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_46.tmp_0.subprog_12'], XShape=['transpose_46.tmp_1.subprog_12']} = transpose2(inputs={X=['split_11.tmp_2.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_22.tmp_0.subprog_12']} = scale(inputs={ScaleTensor=[], X=['transpose_44.tmp_0.subprog_12']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_22.tmp_0.subprog_12']} = matmul_v2(inputs={X=['scale_22.tmp_0.subprog_12'], Y=['transpose_45.tmp_0.subprog_12']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_23.tmp_0.subprog_12']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_22.tmp_0.subprog_12']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_11.tmp_0.subprog_12']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_23.tmp_0.subprog_12']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_23.tmp_0.subprog_12']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_11.tmp_0.subprog_12'], Y=['transpose_46.tmp_0.subprog_12']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_47.tmp_0.subprog_12'], XShape=['transpose_47.tmp_1.subprog_12']} = transpose2(inputs={X=['matmul_v2_23.tmp_0.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_23.tmp_0.subprog_12'], XShape=['reshape2_23.tmp_1.subprog_12']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_47.tmp_0.subprog_12']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_45.tmp_0.subprog_12']} = matmul_v2(inputs={X=['reshape2_23.tmp_0.subprog_12'], Y=['linear_45.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_45.tmp_1.subprog_12']} = elementwise_add(inputs={X=['linear_45.tmp_0.subprog_12'], Y=['linear_45.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_23.subprog_12']} = elementwise_add(inputs={X=['tmp_22'], Y=['linear_45.tmp_1.subprog_12']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_23.tmp_0.subprog_12'], Variance=['layer_norm_23.tmp_1.subprog_12'], Y=['layer_norm_23.tmp_2.subprog_12']} = layer_norm(inputs={Bias=['layer_norm_23.b_0'], Scale=['layer_norm_23.w_0'], X=['tmp_23.subprog_12']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_46.tmp_0.subprog_12']} = matmul_v2(inputs={X=['layer_norm_23.tmp_2.subprog_12'], Y=['linear_46.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_46.tmp_1.subprog_12']} = elementwise_add(inputs={X=['linear_46.tmp_0.subprog_12'], Y=['linear_46.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_11.tmp_0.subprog_12']} = gelu(inputs={X=['linear_46.tmp_1.subprog_12']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_47.tmp_0.subprog_12']} = matmul_v2(inputs={X=['gelu_11.tmp_0.subprog_12'], Y=['linear_47.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_47.tmp_1.subprog_12']} = elementwise_add(inputs={X=['linear_47.tmp_0.subprog_12'], Y=['linear_47.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_24@GRAD@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [1, 1024, 1024], peer = 0, ring_id = 35, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_24@GRAD@RESHARD_0']} = assign(inputs={X=['tmp_24@GRAD@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_23@GRAD@RENAME@block0@0'], Y@GRAD=['linear_47.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_24@GRAD@RESHARD_0'], X=['tmp_23.subprog_12'], Y=['linear_47.tmp_1.subprog_12']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_47.tmp_0@GRAD'], Y@GRAD=['linear_47.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_47.tmp_1@GRAD'], X=['linear_47.tmp_0.subprog_12'], Y=['linear_47.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_47.b_0', 'linear_47.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_11.tmp_0@GRAD'], Y@GRAD=['linear_47.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_47.tmp_0@GRAD'], X=['gelu_11.tmp_0.subprog_12'], Y=['linear_47.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_47.w_0', 'linear_47.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_46.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_11.tmp_0@GRAD'], X=['linear_46.tmp_1.subprog_12']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_46.tmp_0@GRAD'], Y@GRAD=['linear_46.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_46.tmp_1@GRAD'], X=['linear_46.tmp_0.subprog_12'], Y=['linear_46.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_46.b_0', 'linear_46.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_23.tmp_2@GRAD'], Y@GRAD=['linear_46.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_46.tmp_0@GRAD'], X=['layer_norm_23.tmp_2.subprog_12'], Y=['linear_46.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_46.w_0', 'linear_46.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_23.b_0@GRAD'], Scale@GRAD=['layer_norm_23.w_0@GRAD'], X@GRAD=['tmp_23@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_23.b_0'], Mean=['layer_norm_23.tmp_0.subprog_12'], Scale=['layer_norm_23.w_0'], Variance=['layer_norm_23.tmp_1.subprog_12'], X=['tmp_23.subprog_12'], Y@GRAD=['layer_norm_23.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['layer_norm_23.b_0', 'layer_norm_23.b_0@GRAD', 'layer_norm_23.w_0', 'layer_norm_23.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_23@GRAD']} = sum(inputs={X=['tmp_23@GRAD@RENAME@block0@0', 'tmp_23@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_22@GRAD@RENAME@block0@0'], Y@GRAD=['linear_45.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['tmp_22'], Y=['linear_45.tmp_1.subprog_12']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_45.tmp_0@GRAD'], Y@GRAD=['linear_45.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_45.tmp_1@GRAD'], X=['linear_45.tmp_0.subprog_12'], Y=['linear_45.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_45.b_0', 'linear_45.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_23.tmp_0@GRAD'], Y@GRAD=['linear_45.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_45.tmp_0@GRAD'], X=['reshape2_23.tmp_0.subprog_12'], Y=['linear_45.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_45.w_0', 'linear_45.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_47.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_23.tmp_0@GRAD'], XShape=['reshape2_23.tmp_1.subprog_12']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_23.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_47.tmp_0@GRAD'], XShape=['transpose_47.tmp_1.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_11.tmp_0@GRAD'], Y@GRAD=['transpose_46.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_23.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_11.tmp_0.subprog_12'], Y=['transpose_46.tmp_0.subprog_12']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_23.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_11.tmp_0.subprog_12'], Out@GRAD=['fused_softmax_mask_upper_triangle_11.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_22.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_23.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_22.tmp_0@GRAD'], Y@GRAD=['transpose_45.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_22.tmp_0@GRAD'], X=['scale_22.tmp_0.subprog_12'], Y=['transpose_45.tmp_0.subprog_12']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_44.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_22.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_11.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_46.tmp_0@GRAD'], XShape=['transpose_46.tmp_1.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_11.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_45.tmp_0@GRAD'], XShape=['transpose_45.tmp_1.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_11.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_44.tmp_0@GRAD'], XShape=['transpose_44.tmp_1.subprog_12']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_22.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_11.tmp_0@GRAD', 'split_11.tmp_1@GRAD', 'split_11.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_44.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_22.tmp_0@GRAD'], XShape=['reshape2_22.tmp_1.subprog_12']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_44.tmp_0@GRAD'], Y@GRAD=['linear_44.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_44.tmp_1@GRAD'], X=['linear_44.tmp_0.subprog_12'], Y=['linear_44.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_44.b_0', 'linear_44.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_22.tmp_2@GRAD'], Y@GRAD=['linear_44.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_44.tmp_0@GRAD'], X=['layer_norm_22.tmp_2.subprog_12'], Y=['linear_44.w_0']}, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['linear_44.w_0', 'linear_44.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_22.b_0@GRAD'], Scale@GRAD=['layer_norm_22.w_0@GRAD'], X@GRAD=['tmp_22@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_22.b_0'], Mean=['layer_norm_22.tmp_0.subprog_12'], Scale=['layer_norm_22.w_0'], Variance=['layer_norm_22.tmp_1.subprog_12'], X=['tmp_22'], Y@GRAD=['layer_norm_22.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_11/, op_role = 1, op_role_var = ['layer_norm_22.b_0', 'layer_norm_22.b_0@GRAD', 'layer_norm_22.w_0', 'layer_norm_22.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_20.tmp_0.subprog_13'], Variance=['layer_norm_20.tmp_1.subprog_13'], Y=['layer_norm_20.tmp_2.subprog_13']} = layer_norm(inputs={Bias=['layer_norm_20.b_0'], Scale=['layer_norm_20.w_0'], X=['tmp_20']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_40.tmp_0.subprog_13']} = matmul_v2(inputs={X=['layer_norm_20.tmp_2.subprog_13'], Y=['linear_40.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_40.tmp_1.subprog_13']} = elementwise_add(inputs={X=['linear_40.tmp_0.subprog_13'], Y=['linear_40.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_20.tmp_0.subprog_13'], XShape=['reshape2_20.tmp_1.subprog_13']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_40.tmp_1.subprog_13']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_10.tmp_0.subprog_13', 'split_10.tmp_1.subprog_13', 'split_10.tmp_2.subprog_13']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_20.tmp_0.subprog_13']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_40.tmp_0.subprog_13'], XShape=['transpose_40.tmp_1.subprog_13']} = transpose2(inputs={X=['split_10.tmp_0.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_41.tmp_0.subprog_13'], XShape=['transpose_41.tmp_1.subprog_13']} = transpose2(inputs={X=['split_10.tmp_1.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_42.tmp_0.subprog_13'], XShape=['transpose_42.tmp_1.subprog_13']} = transpose2(inputs={X=['split_10.tmp_2.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_20.tmp_0.subprog_13']} = scale(inputs={ScaleTensor=[], X=['transpose_40.tmp_0.subprog_13']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_20.tmp_0.subprog_13']} = matmul_v2(inputs={X=['scale_20.tmp_0.subprog_13'], Y=['transpose_41.tmp_0.subprog_13']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_21.tmp_0.subprog_13']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_20.tmp_0.subprog_13']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_10.tmp_0.subprog_13']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_21.tmp_0.subprog_13']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_21.tmp_0.subprog_13']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_10.tmp_0.subprog_13'], Y=['transpose_42.tmp_0.subprog_13']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_43.tmp_0.subprog_13'], XShape=['transpose_43.tmp_1.subprog_13']} = transpose2(inputs={X=['matmul_v2_21.tmp_0.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_21.tmp_0.subprog_13'], XShape=['reshape2_21.tmp_1.subprog_13']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_43.tmp_0.subprog_13']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_41.tmp_0.subprog_13']} = matmul_v2(inputs={X=['reshape2_21.tmp_0.subprog_13'], Y=['linear_41.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_41.tmp_1.subprog_13']} = elementwise_add(inputs={X=['linear_41.tmp_0.subprog_13'], Y=['linear_41.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21.subprog_13']} = elementwise_add(inputs={X=['tmp_20'], Y=['linear_41.tmp_1.subprog_13']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_21.tmp_0.subprog_13'], Variance=['layer_norm_21.tmp_1.subprog_13'], Y=['layer_norm_21.tmp_2.subprog_13']} = layer_norm(inputs={Bias=['layer_norm_21.b_0'], Scale=['layer_norm_21.w_0'], X=['tmp_21.subprog_13']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_42.tmp_0.subprog_13']} = matmul_v2(inputs={X=['layer_norm_21.tmp_2.subprog_13'], Y=['linear_42.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_42.tmp_1.subprog_13']} = elementwise_add(inputs={X=['linear_42.tmp_0.subprog_13'], Y=['linear_42.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_10.tmp_0.subprog_13']} = gelu(inputs={X=['linear_42.tmp_1.subprog_13']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_43.tmp_0.subprog_13']} = matmul_v2(inputs={X=['gelu_10.tmp_0.subprog_13'], Y=['linear_43.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_43.tmp_1.subprog_13']} = elementwise_add(inputs={X=['linear_43.tmp_0.subprog_13'], Y=['linear_43.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_22@GRAD']} = sum(inputs={X=['tmp_22@GRAD@RENAME@block0@0', 'tmp_22@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_21@GRAD@RENAME@block0@0'], Y@GRAD=['linear_43.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_22@GRAD'], X=['tmp_21.subprog_13'], Y=['linear_43.tmp_1.subprog_13']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_43.tmp_0@GRAD'], Y@GRAD=['linear_43.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_43.tmp_1@GRAD'], X=['linear_43.tmp_0.subprog_13'], Y=['linear_43.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_43.b_0', 'linear_43.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_10.tmp_0@GRAD'], Y@GRAD=['linear_43.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_43.tmp_0@GRAD'], X=['gelu_10.tmp_0.subprog_13'], Y=['linear_43.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_43.w_0', 'linear_43.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_42.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_10.tmp_0@GRAD'], X=['linear_42.tmp_1.subprog_13']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_42.tmp_0@GRAD'], Y@GRAD=['linear_42.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_42.tmp_1@GRAD'], X=['linear_42.tmp_0.subprog_13'], Y=['linear_42.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_42.b_0', 'linear_42.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_21.tmp_2@GRAD'], Y@GRAD=['linear_42.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_42.tmp_0@GRAD'], X=['layer_norm_21.tmp_2.subprog_13'], Y=['linear_42.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_42.w_0', 'linear_42.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_21.b_0@GRAD'], Scale@GRAD=['layer_norm_21.w_0@GRAD'], X@GRAD=['tmp_21@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_21.b_0'], Mean=['layer_norm_21.tmp_0.subprog_13'], Scale=['layer_norm_21.w_0'], Variance=['layer_norm_21.tmp_1.subprog_13'], X=['tmp_21.subprog_13'], Y@GRAD=['layer_norm_21.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['layer_norm_21.b_0', 'layer_norm_21.b_0@GRAD', 'layer_norm_21.w_0', 'layer_norm_21.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_21@GRAD']} = sum(inputs={X=['tmp_21@GRAD@RENAME@block0@0', 'tmp_21@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_20@GRAD@RENAME@block0@0'], Y@GRAD=['linear_41.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['tmp_20'], Y=['linear_41.tmp_1.subprog_13']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_41.tmp_0@GRAD'], Y@GRAD=['linear_41.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_41.tmp_1@GRAD'], X=['linear_41.tmp_0.subprog_13'], Y=['linear_41.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_41.b_0', 'linear_41.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_21.tmp_0@GRAD'], Y@GRAD=['linear_41.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_41.tmp_0@GRAD'], X=['reshape2_21.tmp_0.subprog_13'], Y=['linear_41.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_41.w_0', 'linear_41.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_43.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_21.tmp_0@GRAD'], XShape=['reshape2_21.tmp_1.subprog_13']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_21.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_43.tmp_0@GRAD'], XShape=['transpose_43.tmp_1.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_10.tmp_0@GRAD'], Y@GRAD=['transpose_42.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_21.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_10.tmp_0.subprog_13'], Y=['transpose_42.tmp_0.subprog_13']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_21.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_10.tmp_0.subprog_13'], Out@GRAD=['fused_softmax_mask_upper_triangle_10.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_20.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_21.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_20.tmp_0@GRAD'], Y@GRAD=['transpose_41.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_20.tmp_0@GRAD'], X=['scale_20.tmp_0.subprog_13'], Y=['transpose_41.tmp_0.subprog_13']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_40.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_20.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_10.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_42.tmp_0@GRAD'], XShape=['transpose_42.tmp_1.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_10.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_41.tmp_0@GRAD'], XShape=['transpose_41.tmp_1.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_40.tmp_0@GRAD'], XShape=['transpose_40.tmp_1.subprog_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_20.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_10.tmp_0@GRAD', 'split_10.tmp_1@GRAD', 'split_10.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_40.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_20.tmp_0@GRAD'], XShape=['reshape2_20.tmp_1.subprog_13']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_40.tmp_0@GRAD'], Y@GRAD=['linear_40.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_40.tmp_1@GRAD'], X=['linear_40.tmp_0.subprog_13'], Y=['linear_40.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_40.b_0', 'linear_40.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_20.tmp_2@GRAD'], Y@GRAD=['linear_40.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_40.tmp_0@GRAD'], X=['layer_norm_20.tmp_2.subprog_13'], Y=['linear_40.w_0']}, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['linear_40.w_0', 'linear_40.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_20.b_0@GRAD'], Scale@GRAD=['layer_norm_20.w_0@GRAD'], X@GRAD=['tmp_20@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_20.b_0'], Mean=['layer_norm_20.tmp_0.subprog_13'], Scale=['layer_norm_20.w_0'], Variance=['layer_norm_20.tmp_1.subprog_13'], X=['tmp_20'], Y@GRAD=['layer_norm_20.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_10/, op_role = 1, op_role_var = ['layer_norm_20.b_0', 'layer_norm_20.b_0@GRAD', 'layer_norm_20.w_0', 'layer_norm_20.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_18.tmp_0.subprog_14'], Variance=['layer_norm_18.tmp_1.subprog_14'], Y=['layer_norm_18.tmp_2.subprog_14']} = layer_norm(inputs={Bias=['layer_norm_18.b_0'], Scale=['layer_norm_18.w_0'], X=['tmp_18']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_36.tmp_0.subprog_14']} = matmul_v2(inputs={X=['layer_norm_18.tmp_2.subprog_14'], Y=['linear_36.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_36.tmp_1.subprog_14']} = elementwise_add(inputs={X=['linear_36.tmp_0.subprog_14'], Y=['linear_36.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_18.tmp_0.subprog_14'], XShape=['reshape2_18.tmp_1.subprog_14']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_36.tmp_1.subprog_14']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_9.tmp_0.subprog_14', 'split_9.tmp_1.subprog_14', 'split_9.tmp_2.subprog_14']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_18.tmp_0.subprog_14']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_36.tmp_0.subprog_14'], XShape=['transpose_36.tmp_1.subprog_14']} = transpose2(inputs={X=['split_9.tmp_0.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_37.tmp_0.subprog_14'], XShape=['transpose_37.tmp_1.subprog_14']} = transpose2(inputs={X=['split_9.tmp_1.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_38.tmp_0.subprog_14'], XShape=['transpose_38.tmp_1.subprog_14']} = transpose2(inputs={X=['split_9.tmp_2.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_18.tmp_0.subprog_14']} = scale(inputs={ScaleTensor=[], X=['transpose_36.tmp_0.subprog_14']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_18.tmp_0.subprog_14']} = matmul_v2(inputs={X=['scale_18.tmp_0.subprog_14'], Y=['transpose_37.tmp_0.subprog_14']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_19.tmp_0.subprog_14']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_18.tmp_0.subprog_14']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_9.tmp_0.subprog_14']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_19.tmp_0.subprog_14']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_19.tmp_0.subprog_14']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_9.tmp_0.subprog_14'], Y=['transpose_38.tmp_0.subprog_14']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_39.tmp_0.subprog_14'], XShape=['transpose_39.tmp_1.subprog_14']} = transpose2(inputs={X=['matmul_v2_19.tmp_0.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_19.tmp_0.subprog_14'], XShape=['reshape2_19.tmp_1.subprog_14']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_39.tmp_0.subprog_14']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_37.tmp_0.subprog_14']} = matmul_v2(inputs={X=['reshape2_19.tmp_0.subprog_14'], Y=['linear_37.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_37.tmp_1.subprog_14']} = elementwise_add(inputs={X=['linear_37.tmp_0.subprog_14'], Y=['linear_37.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_19.subprog_14']} = elementwise_add(inputs={X=['tmp_18'], Y=['linear_37.tmp_1.subprog_14']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_19.tmp_0.subprog_14'], Variance=['layer_norm_19.tmp_1.subprog_14'], Y=['layer_norm_19.tmp_2.subprog_14']} = layer_norm(inputs={Bias=['layer_norm_19.b_0'], Scale=['layer_norm_19.w_0'], X=['tmp_19.subprog_14']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_38.tmp_0.subprog_14']} = matmul_v2(inputs={X=['layer_norm_19.tmp_2.subprog_14'], Y=['linear_38.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_38.tmp_1.subprog_14']} = elementwise_add(inputs={X=['linear_38.tmp_0.subprog_14'], Y=['linear_38.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_9.tmp_0.subprog_14']} = gelu(inputs={X=['linear_38.tmp_1.subprog_14']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_39.tmp_0.subprog_14']} = matmul_v2(inputs={X=['gelu_9.tmp_0.subprog_14'], Y=['linear_39.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_39.tmp_1.subprog_14']} = elementwise_add(inputs={X=['linear_39.tmp_0.subprog_14'], Y=['linear_39.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_20@GRAD']} = sum(inputs={X=['tmp_20@GRAD@RENAME@block0@0', 'tmp_20@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_19@GRAD@RENAME@block0@0'], Y@GRAD=['linear_39.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['tmp_19.subprog_14'], Y=['linear_39.tmp_1.subprog_14']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_39.tmp_0@GRAD'], Y@GRAD=['linear_39.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_39.tmp_1@GRAD'], X=['linear_39.tmp_0.subprog_14'], Y=['linear_39.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_39.b_0', 'linear_39.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_9.tmp_0@GRAD'], Y@GRAD=['linear_39.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_39.tmp_0@GRAD'], X=['gelu_9.tmp_0.subprog_14'], Y=['linear_39.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_39.w_0', 'linear_39.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_38.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_9.tmp_0@GRAD'], X=['linear_38.tmp_1.subprog_14']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_38.tmp_0@GRAD'], Y@GRAD=['linear_38.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_38.tmp_1@GRAD'], X=['linear_38.tmp_0.subprog_14'], Y=['linear_38.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_38.b_0', 'linear_38.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_19.tmp_2@GRAD'], Y@GRAD=['linear_38.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_38.tmp_0@GRAD'], X=['layer_norm_19.tmp_2.subprog_14'], Y=['linear_38.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_38.w_0', 'linear_38.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_19.b_0@GRAD'], Scale@GRAD=['layer_norm_19.w_0@GRAD'], X@GRAD=['tmp_19@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_19.b_0'], Mean=['layer_norm_19.tmp_0.subprog_14'], Scale=['layer_norm_19.w_0'], Variance=['layer_norm_19.tmp_1.subprog_14'], X=['tmp_19.subprog_14'], Y@GRAD=['layer_norm_19.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['layer_norm_19.b_0', 'layer_norm_19.b_0@GRAD', 'layer_norm_19.w_0', 'layer_norm_19.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_19@GRAD']} = sum(inputs={X=['tmp_19@GRAD@RENAME@block0@0', 'tmp_19@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_18@GRAD@RENAME@block0@0'], Y@GRAD=['linear_37.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_19@GRAD'], X=['tmp_18'], Y=['linear_37.tmp_1.subprog_14']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_37.tmp_0@GRAD'], Y@GRAD=['linear_37.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_37.tmp_1@GRAD'], X=['linear_37.tmp_0.subprog_14'], Y=['linear_37.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_37.b_0', 'linear_37.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_19.tmp_0@GRAD'], Y@GRAD=['linear_37.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_37.tmp_0@GRAD'], X=['reshape2_19.tmp_0.subprog_14'], Y=['linear_37.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_37.w_0', 'linear_37.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_39.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_19.tmp_0@GRAD'], XShape=['reshape2_19.tmp_1.subprog_14']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_19.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_39.tmp_0@GRAD'], XShape=['transpose_39.tmp_1.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_9.tmp_0@GRAD'], Y@GRAD=['transpose_38.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_19.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_9.tmp_0.subprog_14'], Y=['transpose_38.tmp_0.subprog_14']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_19.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_9.tmp_0.subprog_14'], Out@GRAD=['fused_softmax_mask_upper_triangle_9.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_18.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_19.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_18.tmp_0@GRAD'], Y@GRAD=['transpose_37.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_18.tmp_0@GRAD'], X=['scale_18.tmp_0.subprog_14'], Y=['transpose_37.tmp_0.subprog_14']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_36.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_18.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_9.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_38.tmp_0@GRAD'], XShape=['transpose_38.tmp_1.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_9.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_37.tmp_0@GRAD'], XShape=['transpose_37.tmp_1.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_36.tmp_0@GRAD'], XShape=['transpose_36.tmp_1.subprog_14']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_18.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_9.tmp_0@GRAD', 'split_9.tmp_1@GRAD', 'split_9.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_36.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_18.tmp_0@GRAD'], XShape=['reshape2_18.tmp_1.subprog_14']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_36.tmp_0@GRAD'], Y@GRAD=['linear_36.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_36.tmp_1@GRAD'], X=['linear_36.tmp_0.subprog_14'], Y=['linear_36.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_36.b_0', 'linear_36.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_18.tmp_2@GRAD'], Y@GRAD=['linear_36.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_36.tmp_0@GRAD'], X=['layer_norm_18.tmp_2.subprog_14'], Y=['linear_36.w_0']}, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['linear_36.w_0', 'linear_36.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_18.b_0@GRAD'], Scale@GRAD=['layer_norm_18.w_0@GRAD'], X@GRAD=['tmp_18@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_18.b_0'], Mean=['layer_norm_18.tmp_0.subprog_14'], Scale=['layer_norm_18.w_0'], Variance=['layer_norm_18.tmp_1.subprog_14'], X=['tmp_18'], Y@GRAD=['layer_norm_18.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_9/, op_role = 1, op_role_var = ['layer_norm_18.b_0', 'layer_norm_18.b_0@GRAD', 'layer_norm_18.w_0', 'layer_norm_18.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_16.tmp_0.subprog_15'], Variance=['layer_norm_16.tmp_1.subprog_15'], Y=['layer_norm_16.tmp_2.subprog_15']} = layer_norm(inputs={Bias=['layer_norm_16.b_0'], Scale=['layer_norm_16.w_0'], X=['tmp_16']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_32.tmp_0.subprog_15']} = matmul_v2(inputs={X=['layer_norm_16.tmp_2.subprog_15'], Y=['linear_32.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_32.tmp_1.subprog_15']} = elementwise_add(inputs={X=['linear_32.tmp_0.subprog_15'], Y=['linear_32.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0.subprog_15'], XShape=['reshape2_16.tmp_1.subprog_15']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_32.tmp_1.subprog_15']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_8.tmp_0.subprog_15', 'split_8.tmp_1.subprog_15', 'split_8.tmp_2.subprog_15']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_16.tmp_0.subprog_15']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_32.tmp_0.subprog_15'], XShape=['transpose_32.tmp_1.subprog_15']} = transpose2(inputs={X=['split_8.tmp_0.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_33.tmp_0.subprog_15'], XShape=['transpose_33.tmp_1.subprog_15']} = transpose2(inputs={X=['split_8.tmp_1.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_34.tmp_0.subprog_15'], XShape=['transpose_34.tmp_1.subprog_15']} = transpose2(inputs={X=['split_8.tmp_2.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_16.tmp_0.subprog_15']} = scale(inputs={ScaleTensor=[], X=['transpose_32.tmp_0.subprog_15']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_16.tmp_0.subprog_15']} = matmul_v2(inputs={X=['scale_16.tmp_0.subprog_15'], Y=['transpose_33.tmp_0.subprog_15']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_17.tmp_0.subprog_15']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_16.tmp_0.subprog_15']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_8.tmp_0.subprog_15']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_17.tmp_0.subprog_15']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_17.tmp_0.subprog_15']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_8.tmp_0.subprog_15'], Y=['transpose_34.tmp_0.subprog_15']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_35.tmp_0.subprog_15'], XShape=['transpose_35.tmp_1.subprog_15']} = transpose2(inputs={X=['matmul_v2_17.tmp_0.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_17.tmp_0.subprog_15'], XShape=['reshape2_17.tmp_1.subprog_15']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_35.tmp_0.subprog_15']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_33.tmp_0.subprog_15']} = matmul_v2(inputs={X=['reshape2_17.tmp_0.subprog_15'], Y=['linear_33.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_33.tmp_1.subprog_15']} = elementwise_add(inputs={X=['linear_33.tmp_0.subprog_15'], Y=['linear_33.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_17.subprog_15']} = elementwise_add(inputs={X=['tmp_16'], Y=['linear_33.tmp_1.subprog_15']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_17.tmp_0.subprog_15'], Variance=['layer_norm_17.tmp_1.subprog_15'], Y=['layer_norm_17.tmp_2.subprog_15']} = layer_norm(inputs={Bias=['layer_norm_17.b_0'], Scale=['layer_norm_17.w_0'], X=['tmp_17.subprog_15']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_34.tmp_0.subprog_15']} = matmul_v2(inputs={X=['layer_norm_17.tmp_2.subprog_15'], Y=['linear_34.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_34.tmp_1.subprog_15']} = elementwise_add(inputs={X=['linear_34.tmp_0.subprog_15'], Y=['linear_34.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_8.tmp_0.subprog_15']} = gelu(inputs={X=['linear_34.tmp_1.subprog_15']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_35.tmp_0.subprog_15']} = matmul_v2(inputs={X=['gelu_8.tmp_0.subprog_15'], Y=['linear_35.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_35.tmp_1.subprog_15']} = elementwise_add(inputs={X=['linear_35.tmp_0.subprog_15'], Y=['linear_35.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_18@GRAD']} = sum(inputs={X=['tmp_18@GRAD@RENAME@block0@0', 'tmp_18@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_17@GRAD@RENAME@block0@0'], Y@GRAD=['linear_35.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_18@GRAD'], X=['tmp_17.subprog_15'], Y=['linear_35.tmp_1.subprog_15']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_35.tmp_0@GRAD'], Y@GRAD=['linear_35.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_35.tmp_1@GRAD'], X=['linear_35.tmp_0.subprog_15'], Y=['linear_35.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_35.b_0', 'linear_35.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_8.tmp_0@GRAD'], Y@GRAD=['linear_35.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_35.tmp_0@GRAD'], X=['gelu_8.tmp_0.subprog_15'], Y=['linear_35.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_35.w_0', 'linear_35.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_34.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_8.tmp_0@GRAD'], X=['linear_34.tmp_1.subprog_15']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_34.tmp_0@GRAD'], Y@GRAD=['linear_34.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_34.tmp_1@GRAD'], X=['linear_34.tmp_0.subprog_15'], Y=['linear_34.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_34.b_0', 'linear_34.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_17.tmp_2@GRAD'], Y@GRAD=['linear_34.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_34.tmp_0@GRAD'], X=['layer_norm_17.tmp_2.subprog_15'], Y=['linear_34.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_34.w_0', 'linear_34.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_17.b_0@GRAD'], Scale@GRAD=['layer_norm_17.w_0@GRAD'], X@GRAD=['tmp_17@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_17.b_0'], Mean=['layer_norm_17.tmp_0.subprog_15'], Scale=['layer_norm_17.w_0'], Variance=['layer_norm_17.tmp_1.subprog_15'], X=['tmp_17.subprog_15'], Y@GRAD=['layer_norm_17.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['layer_norm_17.b_0', 'layer_norm_17.b_0@GRAD', 'layer_norm_17.w_0', 'layer_norm_17.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_17@GRAD']} = sum(inputs={X=['tmp_17@GRAD@RENAME@block0@0', 'tmp_17@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_16@GRAD@RENAME@block0@0'], Y@GRAD=['linear_33.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_17@GRAD'], X=['tmp_16'], Y=['linear_33.tmp_1.subprog_15']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_33.tmp_0@GRAD'], Y@GRAD=['linear_33.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_33.tmp_1@GRAD'], X=['linear_33.tmp_0.subprog_15'], Y=['linear_33.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_33.b_0', 'linear_33.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_17.tmp_0@GRAD'], Y@GRAD=['linear_33.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_33.tmp_0@GRAD'], X=['reshape2_17.tmp_0.subprog_15'], Y=['linear_33.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_33.w_0', 'linear_33.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_35.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_17.tmp_0@GRAD'], XShape=['reshape2_17.tmp_1.subprog_15']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_17.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_35.tmp_0@GRAD'], XShape=['transpose_35.tmp_1.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_8.tmp_0@GRAD'], Y@GRAD=['transpose_34.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_17.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_8.tmp_0.subprog_15'], Y=['transpose_34.tmp_0.subprog_15']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_17.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_8.tmp_0.subprog_15'], Out@GRAD=['fused_softmax_mask_upper_triangle_8.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_16.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_17.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_16.tmp_0@GRAD'], Y@GRAD=['transpose_33.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_16.tmp_0@GRAD'], X=['scale_16.tmp_0.subprog_15'], Y=['transpose_33.tmp_0.subprog_15']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_32.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_16.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_8.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_34.tmp_0@GRAD'], XShape=['transpose_34.tmp_1.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_8.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_33.tmp_0@GRAD'], XShape=['transpose_33.tmp_1.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_32.tmp_0@GRAD'], XShape=['transpose_32.tmp_1.subprog_15']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_8.tmp_0@GRAD', 'split_8.tmp_1@GRAD', 'split_8.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_32.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_16.tmp_0@GRAD'], XShape=['reshape2_16.tmp_1.subprog_15']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_32.tmp_0@GRAD'], Y@GRAD=['linear_32.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_32.tmp_1@GRAD'], X=['linear_32.tmp_0.subprog_15'], Y=['linear_32.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_32.b_0', 'linear_32.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_16.tmp_2@GRAD'], Y@GRAD=['linear_32.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_32.tmp_0@GRAD'], X=['layer_norm_16.tmp_2.subprog_15'], Y=['linear_32.w_0']}, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['linear_32.w_0', 'linear_32.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_16.b_0@GRAD'], Scale@GRAD=['layer_norm_16.w_0@GRAD'], X@GRAD=['tmp_16@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_16.b_0'], Mean=['layer_norm_16.tmp_0.subprog_15'], Scale=['layer_norm_16.w_0'], Variance=['layer_norm_16.tmp_1.subprog_15'], X=['tmp_16'], Y@GRAD=['layer_norm_16.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_8/, op_role = 1, op_role_var = ['layer_norm_16.b_0', 'layer_norm_16.b_0@GRAD', 'layer_norm_16.w_0', 'layer_norm_16.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_14.tmp_0.subprog_16'], Variance=['layer_norm_14.tmp_1.subprog_16'], Y=['layer_norm_14.tmp_2.subprog_16']} = layer_norm(inputs={Bias=['layer_norm_14.b_0'], Scale=['layer_norm_14.w_0'], X=['tmp_14']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_28.tmp_0.subprog_16']} = matmul_v2(inputs={X=['layer_norm_14.tmp_2.subprog_16'], Y=['linear_28.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_28.tmp_1.subprog_16']} = elementwise_add(inputs={X=['linear_28.tmp_0.subprog_16'], Y=['linear_28.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_14.tmp_0.subprog_16'], XShape=['reshape2_14.tmp_1.subprog_16']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_28.tmp_1.subprog_16']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_7.tmp_0.subprog_16', 'split_7.tmp_1.subprog_16', 'split_7.tmp_2.subprog_16']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_14.tmp_0.subprog_16']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_28.tmp_0.subprog_16'], XShape=['transpose_28.tmp_1.subprog_16']} = transpose2(inputs={X=['split_7.tmp_0.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_29.tmp_0.subprog_16'], XShape=['transpose_29.tmp_1.subprog_16']} = transpose2(inputs={X=['split_7.tmp_1.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_30.tmp_0.subprog_16'], XShape=['transpose_30.tmp_1.subprog_16']} = transpose2(inputs={X=['split_7.tmp_2.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_14.tmp_0.subprog_16']} = scale(inputs={ScaleTensor=[], X=['transpose_28.tmp_0.subprog_16']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_14.tmp_0.subprog_16']} = matmul_v2(inputs={X=['scale_14.tmp_0.subprog_16'], Y=['transpose_29.tmp_0.subprog_16']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_15.tmp_0.subprog_16']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_14.tmp_0.subprog_16']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_7.tmp_0.subprog_16']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_15.tmp_0.subprog_16']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_15.tmp_0.subprog_16']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_7.tmp_0.subprog_16'], Y=['transpose_30.tmp_0.subprog_16']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_31.tmp_0.subprog_16'], XShape=['transpose_31.tmp_1.subprog_16']} = transpose2(inputs={X=['matmul_v2_15.tmp_0.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0.subprog_16'], XShape=['reshape2_15.tmp_1.subprog_16']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_31.tmp_0.subprog_16']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_29.tmp_0.subprog_16']} = matmul_v2(inputs={X=['reshape2_15.tmp_0.subprog_16'], Y=['linear_29.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_29.tmp_1.subprog_16']} = elementwise_add(inputs={X=['linear_29.tmp_0.subprog_16'], Y=['linear_29.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_15.subprog_16']} = elementwise_add(inputs={X=['tmp_14'], Y=['linear_29.tmp_1.subprog_16']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_15.tmp_0.subprog_16'], Variance=['layer_norm_15.tmp_1.subprog_16'], Y=['layer_norm_15.tmp_2.subprog_16']} = layer_norm(inputs={Bias=['layer_norm_15.b_0'], Scale=['layer_norm_15.w_0'], X=['tmp_15.subprog_16']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_30.tmp_0.subprog_16']} = matmul_v2(inputs={X=['layer_norm_15.tmp_2.subprog_16'], Y=['linear_30.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_30.tmp_1.subprog_16']} = elementwise_add(inputs={X=['linear_30.tmp_0.subprog_16'], Y=['linear_30.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_7.tmp_0.subprog_16']} = gelu(inputs={X=['linear_30.tmp_1.subprog_16']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_31.tmp_0.subprog_16']} = matmul_v2(inputs={X=['gelu_7.tmp_0.subprog_16'], Y=['linear_31.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_31.tmp_1.subprog_16']} = elementwise_add(inputs={X=['linear_31.tmp_0.subprog_16'], Y=['linear_31.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_16@GRAD']} = sum(inputs={X=['tmp_16@GRAD@RENAME@block0@0', 'tmp_16@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_15@GRAD@RENAME@block0@0'], Y@GRAD=['linear_31.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_16@GRAD'], X=['tmp_15.subprog_16'], Y=['linear_31.tmp_1.subprog_16']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_31.tmp_0@GRAD'], Y@GRAD=['linear_31.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_31.tmp_1@GRAD'], X=['linear_31.tmp_0.subprog_16'], Y=['linear_31.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_31.b_0', 'linear_31.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_7.tmp_0@GRAD'], Y@GRAD=['linear_31.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_31.tmp_0@GRAD'], X=['gelu_7.tmp_0.subprog_16'], Y=['linear_31.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_31.w_0', 'linear_31.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_30.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_7.tmp_0@GRAD'], X=['linear_30.tmp_1.subprog_16']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_30.tmp_0@GRAD'], Y@GRAD=['linear_30.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_30.tmp_1@GRAD'], X=['linear_30.tmp_0.subprog_16'], Y=['linear_30.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_30.b_0', 'linear_30.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_15.tmp_2@GRAD'], Y@GRAD=['linear_30.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_30.tmp_0@GRAD'], X=['layer_norm_15.tmp_2.subprog_16'], Y=['linear_30.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_30.w_0', 'linear_30.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_15.b_0@GRAD'], Scale@GRAD=['layer_norm_15.w_0@GRAD'], X@GRAD=['tmp_15@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_15.b_0'], Mean=['layer_norm_15.tmp_0.subprog_16'], Scale=['layer_norm_15.w_0'], Variance=['layer_norm_15.tmp_1.subprog_16'], X=['tmp_15.subprog_16'], Y@GRAD=['layer_norm_15.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['layer_norm_15.b_0', 'layer_norm_15.b_0@GRAD', 'layer_norm_15.w_0', 'layer_norm_15.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_15@GRAD']} = sum(inputs={X=['tmp_15@GRAD@RENAME@block0@0', 'tmp_15@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_14@GRAD@RENAME@block0@0'], Y@GRAD=['linear_29.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_15@GRAD'], X=['tmp_14'], Y=['linear_29.tmp_1.subprog_16']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_29.tmp_0@GRAD'], Y@GRAD=['linear_29.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_29.tmp_1@GRAD'], X=['linear_29.tmp_0.subprog_16'], Y=['linear_29.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_29.b_0', 'linear_29.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_15.tmp_0@GRAD'], Y@GRAD=['linear_29.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_29.tmp_0@GRAD'], X=['reshape2_15.tmp_0.subprog_16'], Y=['linear_29.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_29.w_0', 'linear_29.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_31.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_15.tmp_0@GRAD'], XShape=['reshape2_15.tmp_1.subprog_16']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_15.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_31.tmp_0@GRAD'], XShape=['transpose_31.tmp_1.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_7.tmp_0@GRAD'], Y@GRAD=['transpose_30.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_15.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_7.tmp_0.subprog_16'], Y=['transpose_30.tmp_0.subprog_16']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_15.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_7.tmp_0.subprog_16'], Out@GRAD=['fused_softmax_mask_upper_triangle_7.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_14.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_15.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_14.tmp_0@GRAD'], Y@GRAD=['transpose_29.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_14.tmp_0@GRAD'], X=['scale_14.tmp_0.subprog_16'], Y=['transpose_29.tmp_0.subprog_16']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_28.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_14.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_7.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_30.tmp_0@GRAD'], XShape=['transpose_30.tmp_1.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_7.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_29.tmp_0@GRAD'], XShape=['transpose_29.tmp_1.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_28.tmp_0@GRAD'], XShape=['transpose_28.tmp_1.subprog_16']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_14.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_7.tmp_0@GRAD', 'split_7.tmp_1@GRAD', 'split_7.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_28.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_14.tmp_0@GRAD'], XShape=['reshape2_14.tmp_1.subprog_16']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_28.tmp_0@GRAD'], Y@GRAD=['linear_28.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_28.tmp_1@GRAD'], X=['linear_28.tmp_0.subprog_16'], Y=['linear_28.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_28.b_0', 'linear_28.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_14.tmp_2@GRAD'], Y@GRAD=['linear_28.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_28.tmp_0@GRAD'], X=['layer_norm_14.tmp_2.subprog_16'], Y=['linear_28.w_0']}, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['linear_28.w_0', 'linear_28.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_14.b_0@GRAD'], Scale@GRAD=['layer_norm_14.w_0@GRAD'], X@GRAD=['tmp_14@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_14.b_0'], Mean=['layer_norm_14.tmp_0.subprog_16'], Scale=['layer_norm_14.w_0'], Variance=['layer_norm_14.tmp_1.subprog_16'], X=['tmp_14'], Y@GRAD=['layer_norm_14.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_7/, op_role = 1, op_role_var = ['layer_norm_14.b_0', 'layer_norm_14.b_0@GRAD', 'layer_norm_14.w_0', 'layer_norm_14.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_12.tmp_0.subprog_17'], Variance=['layer_norm_12.tmp_1.subprog_17'], Y=['layer_norm_12.tmp_2.subprog_17']} = layer_norm(inputs={Bias=['layer_norm_12.b_0'], Scale=['layer_norm_12.w_0'], X=['tmp_12']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_24.tmp_0.subprog_17']} = matmul_v2(inputs={X=['layer_norm_12.tmp_2.subprog_17'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_24.tmp_1.subprog_17']} = elementwise_add(inputs={X=['linear_24.tmp_0.subprog_17'], Y=['linear_24.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_12.tmp_0.subprog_17'], XShape=['reshape2_12.tmp_1.subprog_17']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_24.tmp_1.subprog_17']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_6.tmp_0.subprog_17', 'split_6.tmp_1.subprog_17', 'split_6.tmp_2.subprog_17']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_12.tmp_0.subprog_17']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_24.tmp_0.subprog_17'], XShape=['transpose_24.tmp_1.subprog_17']} = transpose2(inputs={X=['split_6.tmp_0.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_25.tmp_0.subprog_17'], XShape=['transpose_25.tmp_1.subprog_17']} = transpose2(inputs={X=['split_6.tmp_1.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_26.tmp_0.subprog_17'], XShape=['transpose_26.tmp_1.subprog_17']} = transpose2(inputs={X=['split_6.tmp_2.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_12.tmp_0.subprog_17']} = scale(inputs={ScaleTensor=[], X=['transpose_24.tmp_0.subprog_17']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_12.tmp_0.subprog_17']} = matmul_v2(inputs={X=['scale_12.tmp_0.subprog_17'], Y=['transpose_25.tmp_0.subprog_17']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_13.tmp_0.subprog_17']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_12.tmp_0.subprog_17']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_6.tmp_0.subprog_17']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_13.tmp_0.subprog_17']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_13.tmp_0.subprog_17']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_6.tmp_0.subprog_17'], Y=['transpose_26.tmp_0.subprog_17']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_27.tmp_0.subprog_17'], XShape=['transpose_27.tmp_1.subprog_17']} = transpose2(inputs={X=['matmul_v2_13.tmp_0.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_13.tmp_0.subprog_17'], XShape=['reshape2_13.tmp_1.subprog_17']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_27.tmp_0.subprog_17']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_25.tmp_0.subprog_17']} = matmul_v2(inputs={X=['reshape2_13.tmp_0.subprog_17'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_25.tmp_1.subprog_17']} = elementwise_add(inputs={X=['linear_25.tmp_0.subprog_17'], Y=['linear_25.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_13.subprog_17']} = elementwise_add(inputs={X=['tmp_12'], Y=['linear_25.tmp_1.subprog_17']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_13.tmp_0.subprog_17'], Variance=['layer_norm_13.tmp_1.subprog_17'], Y=['layer_norm_13.tmp_2.subprog_17']} = layer_norm(inputs={Bias=['layer_norm_13.b_0'], Scale=['layer_norm_13.w_0'], X=['tmp_13.subprog_17']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_26.tmp_0.subprog_17']} = matmul_v2(inputs={X=['layer_norm_13.tmp_2.subprog_17'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_26.tmp_1.subprog_17']} = elementwise_add(inputs={X=['linear_26.tmp_0.subprog_17'], Y=['linear_26.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_6.tmp_0.subprog_17']} = gelu(inputs={X=['linear_26.tmp_1.subprog_17']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_27.tmp_0.subprog_17']} = matmul_v2(inputs={X=['gelu_6.tmp_0.subprog_17'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_27.tmp_1.subprog_17']} = elementwise_add(inputs={X=['linear_27.tmp_0.subprog_17'], Y=['linear_27.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_14@GRAD']} = sum(inputs={X=['tmp_14@GRAD@RENAME@block0@0', 'tmp_14@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_13@GRAD@RENAME@block0@0'], Y@GRAD=['linear_27.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_14@GRAD'], X=['tmp_13.subprog_17'], Y=['linear_27.tmp_1.subprog_17']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_27.tmp_0@GRAD'], Y@GRAD=['linear_27.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_27.tmp_1@GRAD'], X=['linear_27.tmp_0.subprog_17'], Y=['linear_27.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_27.b_0', 'linear_27.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_6.tmp_0@GRAD'], Y@GRAD=['linear_27.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_27.tmp_0@GRAD'], X=['gelu_6.tmp_0.subprog_17'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_26.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_6.tmp_0@GRAD'], X=['linear_26.tmp_1.subprog_17']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_26.tmp_0@GRAD'], Y@GRAD=['linear_26.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_26.tmp_1@GRAD'], X=['linear_26.tmp_0.subprog_17'], Y=['linear_26.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_26.b_0', 'linear_26.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_13.tmp_2@GRAD'], Y@GRAD=['linear_26.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_26.tmp_0@GRAD'], X=['layer_norm_13.tmp_2.subprog_17'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_13.b_0@GRAD'], Scale@GRAD=['layer_norm_13.w_0@GRAD'], X@GRAD=['tmp_13@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_13.b_0'], Mean=['layer_norm_13.tmp_0.subprog_17'], Scale=['layer_norm_13.w_0'], Variance=['layer_norm_13.tmp_1.subprog_17'], X=['tmp_13.subprog_17'], Y@GRAD=['layer_norm_13.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['layer_norm_13.b_0', 'layer_norm_13.b_0@GRAD', 'layer_norm_13.w_0', 'layer_norm_13.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_13@GRAD']} = sum(inputs={X=['tmp_13@GRAD@RENAME@block0@0', 'tmp_13@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_12@GRAD@RENAME@block0@0'], Y@GRAD=['linear_25.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['tmp_12'], Y=['linear_25.tmp_1.subprog_17']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_25.tmp_0@GRAD'], Y@GRAD=['linear_25.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_25.tmp_1@GRAD'], X=['linear_25.tmp_0.subprog_17'], Y=['linear_25.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_25.b_0', 'linear_25.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_13.tmp_0@GRAD'], Y@GRAD=['linear_25.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_25.tmp_0@GRAD'], X=['reshape2_13.tmp_0.subprog_17'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_27.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_13.tmp_0@GRAD'], XShape=['reshape2_13.tmp_1.subprog_17']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_13.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_27.tmp_0@GRAD'], XShape=['transpose_27.tmp_1.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_6.tmp_0@GRAD'], Y@GRAD=['transpose_26.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_13.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_6.tmp_0.subprog_17'], Y=['transpose_26.tmp_0.subprog_17']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_13.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_6.tmp_0.subprog_17'], Out@GRAD=['fused_softmax_mask_upper_triangle_6.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_12.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_13.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_12.tmp_0@GRAD'], Y@GRAD=['transpose_25.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_12.tmp_0@GRAD'], X=['scale_12.tmp_0.subprog_17'], Y=['transpose_25.tmp_0.subprog_17']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_24.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_12.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_6.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_26.tmp_0@GRAD'], XShape=['transpose_26.tmp_1.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_6.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_25.tmp_0@GRAD'], XShape=['transpose_25.tmp_1.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_24.tmp_0@GRAD'], XShape=['transpose_24.tmp_1.subprog_17']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_12.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_6.tmp_0@GRAD', 'split_6.tmp_1@GRAD', 'split_6.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_24.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_12.tmp_0@GRAD'], XShape=['reshape2_12.tmp_1.subprog_17']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_24.tmp_0@GRAD'], Y@GRAD=['linear_24.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_24.tmp_1@GRAD'], X=['linear_24.tmp_0.subprog_17'], Y=['linear_24.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_24.b_0', 'linear_24.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_12.tmp_2@GRAD'], Y@GRAD=['linear_24.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_24.tmp_0@GRAD'], X=['layer_norm_12.tmp_2.subprog_17'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_12.b_0@GRAD'], Scale@GRAD=['layer_norm_12.w_0@GRAD'], X@GRAD=['tmp_12@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_12.b_0'], Mean=['layer_norm_12.tmp_0.subprog_17'], Scale=['layer_norm_12.w_0'], Variance=['layer_norm_12.tmp_1.subprog_17'], X=['tmp_12'], Y@GRAD=['layer_norm_12.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_6/, op_role = 1, op_role_var = ['layer_norm_12.b_0', 'layer_norm_12.b_0@GRAD', 'layer_norm_12.w_0', 'layer_norm_12.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_10.tmp_0.subprog_18'], Variance=['layer_norm_10.tmp_1.subprog_18'], Y=['layer_norm_10.tmp_2.subprog_18']} = layer_norm(inputs={Bias=['layer_norm_10.b_0'], Scale=['layer_norm_10.w_0'], X=['tmp_10']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_20.tmp_0.subprog_18']} = matmul_v2(inputs={X=['layer_norm_10.tmp_2.subprog_18'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_20.tmp_1.subprog_18']} = elementwise_add(inputs={X=['linear_20.tmp_0.subprog_18'], Y=['linear_20.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0.subprog_18'], XShape=['reshape2_10.tmp_1.subprog_18']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_20.tmp_1.subprog_18']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_5.tmp_0.subprog_18', 'split_5.tmp_1.subprog_18', 'split_5.tmp_2.subprog_18']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_10.tmp_0.subprog_18']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_20.tmp_0.subprog_18'], XShape=['transpose_20.tmp_1.subprog_18']} = transpose2(inputs={X=['split_5.tmp_0.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_21.tmp_0.subprog_18'], XShape=['transpose_21.tmp_1.subprog_18']} = transpose2(inputs={X=['split_5.tmp_1.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_22.tmp_0.subprog_18'], XShape=['transpose_22.tmp_1.subprog_18']} = transpose2(inputs={X=['split_5.tmp_2.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_10.tmp_0.subprog_18']} = scale(inputs={ScaleTensor=[], X=['transpose_20.tmp_0.subprog_18']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_10.tmp_0.subprog_18']} = matmul_v2(inputs={X=['scale_10.tmp_0.subprog_18'], Y=['transpose_21.tmp_0.subprog_18']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_11.tmp_0.subprog_18']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_10.tmp_0.subprog_18']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_5.tmp_0.subprog_18']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_11.tmp_0.subprog_18']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_11.tmp_0.subprog_18']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_5.tmp_0.subprog_18'], Y=['transpose_22.tmp_0.subprog_18']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_23.tmp_0.subprog_18'], XShape=['transpose_23.tmp_1.subprog_18']} = transpose2(inputs={X=['matmul_v2_11.tmp_0.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0.subprog_18'], XShape=['reshape2_11.tmp_1.subprog_18']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_23.tmp_0.subprog_18']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_21.tmp_0.subprog_18']} = matmul_v2(inputs={X=['reshape2_11.tmp_0.subprog_18'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_21.tmp_1.subprog_18']} = elementwise_add(inputs={X=['linear_21.tmp_0.subprog_18'], Y=['linear_21.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_11.subprog_18']} = elementwise_add(inputs={X=['tmp_10'], Y=['linear_21.tmp_1.subprog_18']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_11.tmp_0.subprog_18'], Variance=['layer_norm_11.tmp_1.subprog_18'], Y=['layer_norm_11.tmp_2.subprog_18']} = layer_norm(inputs={Bias=['layer_norm_11.b_0'], Scale=['layer_norm_11.w_0'], X=['tmp_11.subprog_18']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_22.tmp_0.subprog_18']} = matmul_v2(inputs={X=['layer_norm_11.tmp_2.subprog_18'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_22.tmp_1.subprog_18']} = elementwise_add(inputs={X=['linear_22.tmp_0.subprog_18'], Y=['linear_22.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_5.tmp_0.subprog_18']} = gelu(inputs={X=['linear_22.tmp_1.subprog_18']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_23.tmp_0.subprog_18']} = matmul_v2(inputs={X=['gelu_5.tmp_0.subprog_18'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_23.tmp_1.subprog_18']} = elementwise_add(inputs={X=['linear_23.tmp_0.subprog_18'], Y=['linear_23.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_12@GRAD']} = sum(inputs={X=['tmp_12@GRAD@RENAME@block0@0', 'tmp_12@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_11@GRAD@RENAME@block0@0'], Y@GRAD=['linear_23.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['tmp_11.subprog_18'], Y=['linear_23.tmp_1.subprog_18']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_23.tmp_0@GRAD'], Y@GRAD=['linear_23.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_23.tmp_1@GRAD'], X=['linear_23.tmp_0.subprog_18'], Y=['linear_23.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_23.b_0', 'linear_23.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_5.tmp_0@GRAD'], Y@GRAD=['linear_23.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_23.tmp_0@GRAD'], X=['gelu_5.tmp_0.subprog_18'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_22.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_5.tmp_0@GRAD'], X=['linear_22.tmp_1.subprog_18']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_22.tmp_0@GRAD'], Y@GRAD=['linear_22.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_22.tmp_1@GRAD'], X=['linear_22.tmp_0.subprog_18'], Y=['linear_22.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_22.b_0', 'linear_22.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_11.tmp_2@GRAD'], Y@GRAD=['linear_22.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_22.tmp_0@GRAD'], X=['layer_norm_11.tmp_2.subprog_18'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_11.b_0@GRAD'], Scale@GRAD=['layer_norm_11.w_0@GRAD'], X@GRAD=['tmp_11@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_11.b_0'], Mean=['layer_norm_11.tmp_0.subprog_18'], Scale=['layer_norm_11.w_0'], Variance=['layer_norm_11.tmp_1.subprog_18'], X=['tmp_11.subprog_18'], Y@GRAD=['layer_norm_11.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['layer_norm_11.b_0', 'layer_norm_11.b_0@GRAD', 'layer_norm_11.w_0', 'layer_norm_11.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_11@GRAD']} = sum(inputs={X=['tmp_11@GRAD@RENAME@block0@0', 'tmp_11@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_10@GRAD@RENAME@block0@0'], Y@GRAD=['linear_21.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_11@GRAD'], X=['tmp_10'], Y=['linear_21.tmp_1.subprog_18']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_21.tmp_0@GRAD'], Y@GRAD=['linear_21.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_21.tmp_1@GRAD'], X=['linear_21.tmp_0.subprog_18'], Y=['linear_21.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_21.b_0', 'linear_21.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_11.tmp_0@GRAD'], Y@GRAD=['linear_21.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_21.tmp_0@GRAD'], X=['reshape2_11.tmp_0.subprog_18'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_23.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_1.subprog_18']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_11.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_23.tmp_0@GRAD'], XShape=['transpose_23.tmp_1.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_5.tmp_0@GRAD'], Y@GRAD=['transpose_22.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_11.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_5.tmp_0.subprog_18'], Y=['transpose_22.tmp_0.subprog_18']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_11.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_5.tmp_0.subprog_18'], Out@GRAD=['fused_softmax_mask_upper_triangle_5.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_10.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_11.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_10.tmp_0@GRAD'], Y@GRAD=['transpose_21.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_10.tmp_0@GRAD'], X=['scale_10.tmp_0.subprog_18'], Y=['transpose_21.tmp_0.subprog_18']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_20.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_10.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_5.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_22.tmp_0@GRAD'], XShape=['transpose_22.tmp_1.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_5.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_21.tmp_0@GRAD'], XShape=['transpose_21.tmp_1.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_20.tmp_0@GRAD'], XShape=['transpose_20.tmp_1.subprog_18']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_5.tmp_0@GRAD', 'split_5.tmp_1@GRAD', 'split_5.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_20.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_10.tmp_0@GRAD'], XShape=['reshape2_10.tmp_1.subprog_18']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_20.tmp_0@GRAD'], Y@GRAD=['linear_20.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_20.tmp_1@GRAD'], X=['linear_20.tmp_0.subprog_18'], Y=['linear_20.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_20.b_0', 'linear_20.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_10.tmp_2@GRAD'], Y@GRAD=['linear_20.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_20.tmp_0@GRAD'], X=['layer_norm_10.tmp_2.subprog_18'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_10.b_0@GRAD'], Scale@GRAD=['layer_norm_10.w_0@GRAD'], X@GRAD=['tmp_10@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_10.b_0'], Mean=['layer_norm_10.tmp_0.subprog_18'], Scale=['layer_norm_10.w_0'], Variance=['layer_norm_10.tmp_1.subprog_18'], X=['tmp_10'], Y@GRAD=['layer_norm_10.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_5/, op_role = 1, op_role_var = ['layer_norm_10.b_0', 'layer_norm_10.b_0@GRAD', 'layer_norm_10.w_0', 'layer_norm_10.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_8.tmp_0.subprog_19'], Variance=['layer_norm_8.tmp_1.subprog_19'], Y=['layer_norm_8.tmp_2.subprog_19']} = layer_norm(inputs={Bias=['layer_norm_8.b_0'], Scale=['layer_norm_8.w_0'], X=['tmp_8']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_16.tmp_0.subprog_19']} = matmul_v2(inputs={X=['layer_norm_8.tmp_2.subprog_19'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_16.tmp_1.subprog_19']} = elementwise_add(inputs={X=['linear_16.tmp_0.subprog_19'], Y=['linear_16.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_8.tmp_0.subprog_19'], XShape=['reshape2_8.tmp_1.subprog_19']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_1.subprog_19']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_4.tmp_0.subprog_19', 'split_4.tmp_1.subprog_19', 'split_4.tmp_2.subprog_19']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_8.tmp_0.subprog_19']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_16.tmp_0.subprog_19'], XShape=['transpose_16.tmp_1.subprog_19']} = transpose2(inputs={X=['split_4.tmp_0.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_17.tmp_0.subprog_19'], XShape=['transpose_17.tmp_1.subprog_19']} = transpose2(inputs={X=['split_4.tmp_1.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_18.tmp_0.subprog_19'], XShape=['transpose_18.tmp_1.subprog_19']} = transpose2(inputs={X=['split_4.tmp_2.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_8.tmp_0.subprog_19']} = scale(inputs={ScaleTensor=[], X=['transpose_16.tmp_0.subprog_19']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0.subprog_19']} = matmul_v2(inputs={X=['scale_8.tmp_0.subprog_19'], Y=['transpose_17.tmp_0.subprog_19']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_9.tmp_0.subprog_19']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_8.tmp_0.subprog_19']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_4.tmp_0.subprog_19']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_9.tmp_0.subprog_19']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_9.tmp_0.subprog_19']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_4.tmp_0.subprog_19'], Y=['transpose_18.tmp_0.subprog_19']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_19.tmp_0.subprog_19'], XShape=['transpose_19.tmp_1.subprog_19']} = transpose2(inputs={X=['matmul_v2_9.tmp_0.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_9.tmp_0.subprog_19'], XShape=['reshape2_9.tmp_1.subprog_19']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0.subprog_19']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_17.tmp_0.subprog_19']} = matmul_v2(inputs={X=['reshape2_9.tmp_0.subprog_19'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_17.tmp_1.subprog_19']} = elementwise_add(inputs={X=['linear_17.tmp_0.subprog_19'], Y=['linear_17.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_9.subprog_19']} = elementwise_add(inputs={X=['tmp_8'], Y=['linear_17.tmp_1.subprog_19']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_9.tmp_0.subprog_19'], Variance=['layer_norm_9.tmp_1.subprog_19'], Y=['layer_norm_9.tmp_2.subprog_19']} = layer_norm(inputs={Bias=['layer_norm_9.b_0'], Scale=['layer_norm_9.w_0'], X=['tmp_9.subprog_19']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_18.tmp_0.subprog_19']} = matmul_v2(inputs={X=['layer_norm_9.tmp_2.subprog_19'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_18.tmp_1.subprog_19']} = elementwise_add(inputs={X=['linear_18.tmp_0.subprog_19'], Y=['linear_18.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_4.tmp_0.subprog_19']} = gelu(inputs={X=['linear_18.tmp_1.subprog_19']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_19.tmp_0.subprog_19']} = matmul_v2(inputs={X=['gelu_4.tmp_0.subprog_19'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_19.tmp_1.subprog_19']} = elementwise_add(inputs={X=['linear_19.tmp_0.subprog_19'], Y=['linear_19.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_10@GRAD']} = sum(inputs={X=['tmp_10@GRAD@RENAME@block0@0', 'tmp_10@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_9@GRAD@RENAME@block0@0'], Y@GRAD=['linear_19.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['tmp_9.subprog_19'], Y=['linear_19.tmp_1.subprog_19']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_19.tmp_0@GRAD'], Y@GRAD=['linear_19.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_19.tmp_1@GRAD'], X=['linear_19.tmp_0.subprog_19'], Y=['linear_19.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_19.b_0', 'linear_19.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_4.tmp_0@GRAD'], Y@GRAD=['linear_19.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_19.tmp_0@GRAD'], X=['gelu_4.tmp_0.subprog_19'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_18.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_4.tmp_0@GRAD'], X=['linear_18.tmp_1.subprog_19']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_18.tmp_0@GRAD'], Y@GRAD=['linear_18.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_18.tmp_1@GRAD'], X=['linear_18.tmp_0.subprog_19'], Y=['linear_18.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_18.b_0', 'linear_18.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_9.tmp_2@GRAD'], Y@GRAD=['linear_18.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_18.tmp_0@GRAD'], X=['layer_norm_9.tmp_2.subprog_19'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_9.b_0@GRAD'], Scale@GRAD=['layer_norm_9.w_0@GRAD'], X@GRAD=['tmp_9@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_9.b_0'], Mean=['layer_norm_9.tmp_0.subprog_19'], Scale=['layer_norm_9.w_0'], Variance=['layer_norm_9.tmp_1.subprog_19'], X=['tmp_9.subprog_19'], Y@GRAD=['layer_norm_9.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['layer_norm_9.b_0', 'layer_norm_9.b_0@GRAD', 'layer_norm_9.w_0', 'layer_norm_9.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_9@GRAD']} = sum(inputs={X=['tmp_9@GRAD@RENAME@block0@0', 'tmp_9@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_8@GRAD@RENAME@block0@0'], Y@GRAD=['linear_17.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['tmp_8'], Y=['linear_17.tmp_1.subprog_19']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_17.tmp_0@GRAD'], Y@GRAD=['linear_17.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_17.tmp_1@GRAD'], X=['linear_17.tmp_0.subprog_19'], Y=['linear_17.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_17.b_0', 'linear_17.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD'], Y@GRAD=['linear_17.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_17.tmp_0@GRAD'], X=['reshape2_9.tmp_0.subprog_19'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_19.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1.subprog_19']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['transpose_19.tmp_1.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_4.tmp_0@GRAD'], Y@GRAD=['transpose_18.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_9.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_4.tmp_0.subprog_19'], Y=['transpose_18.tmp_0.subprog_19']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_9.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_4.tmp_0.subprog_19'], Out@GRAD=['fused_softmax_mask_upper_triangle_4.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_9.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_8.tmp_0@GRAD'], Y@GRAD=['transpose_17.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_8.tmp_0@GRAD'], X=['scale_8.tmp_0.subprog_19'], Y=['transpose_17.tmp_0.subprog_19']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_16.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_8.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_4.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_18.tmp_0@GRAD'], XShape=['transpose_18.tmp_1.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_4.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_17.tmp_0@GRAD'], XShape=['transpose_17.tmp_1.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_16.tmp_0@GRAD'], XShape=['transpose_16.tmp_1.subprog_19']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_8.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_4.tmp_0@GRAD', 'split_4.tmp_1@GRAD', 'split_4.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_16.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_8.tmp_0@GRAD'], XShape=['reshape2_8.tmp_1.subprog_19']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_16.tmp_0@GRAD'], Y@GRAD=['linear_16.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_16.tmp_1@GRAD'], X=['linear_16.tmp_0.subprog_19'], Y=['linear_16.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_16.b_0', 'linear_16.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_8.tmp_2@GRAD'], Y@GRAD=['linear_16.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_16.tmp_0@GRAD'], X=['layer_norm_8.tmp_2.subprog_19'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_8.b_0@GRAD'], Scale@GRAD=['layer_norm_8.w_0@GRAD'], X@GRAD=['tmp_8@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_8.b_0'], Mean=['layer_norm_8.tmp_0.subprog_19'], Scale=['layer_norm_8.w_0'], Variance=['layer_norm_8.tmp_1.subprog_19'], X=['tmp_8'], Y@GRAD=['layer_norm_8.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_4/, op_role = 1, op_role_var = ['layer_norm_8.b_0', 'layer_norm_8.b_0@GRAD', 'layer_norm_8.w_0', 'layer_norm_8.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_6.tmp_0.subprog_20'], Variance=['layer_norm_6.tmp_1.subprog_20'], Y=['layer_norm_6.tmp_2.subprog_20']} = layer_norm(inputs={Bias=['layer_norm_6.b_0'], Scale=['layer_norm_6.w_0'], X=['tmp_6']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_12.tmp_0.subprog_20']} = matmul_v2(inputs={X=['layer_norm_6.tmp_2.subprog_20'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_12.tmp_1.subprog_20']} = elementwise_add(inputs={X=['linear_12.tmp_0.subprog_20'], Y=['linear_12.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0.subprog_20'], XShape=['reshape2_6.tmp_1.subprog_20']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_12.tmp_1.subprog_20']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_3.tmp_0.subprog_20', 'split_3.tmp_1.subprog_20', 'split_3.tmp_2.subprog_20']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_6.tmp_0.subprog_20']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_12.tmp_0.subprog_20'], XShape=['transpose_12.tmp_1.subprog_20']} = transpose2(inputs={X=['split_3.tmp_0.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_13.tmp_0.subprog_20'], XShape=['transpose_13.tmp_1.subprog_20']} = transpose2(inputs={X=['split_3.tmp_1.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_14.tmp_0.subprog_20'], XShape=['transpose_14.tmp_1.subprog_20']} = transpose2(inputs={X=['split_3.tmp_2.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_6.tmp_0.subprog_20']} = scale(inputs={ScaleTensor=[], X=['transpose_12.tmp_0.subprog_20']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_6.tmp_0.subprog_20']} = matmul_v2(inputs={X=['scale_6.tmp_0.subprog_20'], Y=['transpose_13.tmp_0.subprog_20']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_7.tmp_0.subprog_20']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_6.tmp_0.subprog_20']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_3.tmp_0.subprog_20']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_7.tmp_0.subprog_20']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_7.tmp_0.subprog_20']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_3.tmp_0.subprog_20'], Y=['transpose_14.tmp_0.subprog_20']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_15.tmp_0.subprog_20'], XShape=['transpose_15.tmp_1.subprog_20']} = transpose2(inputs={X=['matmul_v2_7.tmp_0.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_7.tmp_0.subprog_20'], XShape=['reshape2_7.tmp_1.subprog_20']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_15.tmp_0.subprog_20']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_13.tmp_0.subprog_20']} = matmul_v2(inputs={X=['reshape2_7.tmp_0.subprog_20'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_13.tmp_1.subprog_20']} = elementwise_add(inputs={X=['linear_13.tmp_0.subprog_20'], Y=['linear_13.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_7.subprog_20']} = elementwise_add(inputs={X=['tmp_6'], Y=['linear_13.tmp_1.subprog_20']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_7.tmp_0.subprog_20'], Variance=['layer_norm_7.tmp_1.subprog_20'], Y=['layer_norm_7.tmp_2.subprog_20']} = layer_norm(inputs={Bias=['layer_norm_7.b_0'], Scale=['layer_norm_7.w_0'], X=['tmp_7.subprog_20']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_14.tmp_0.subprog_20']} = matmul_v2(inputs={X=['layer_norm_7.tmp_2.subprog_20'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_14.tmp_1.subprog_20']} = elementwise_add(inputs={X=['linear_14.tmp_0.subprog_20'], Y=['linear_14.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_3.tmp_0.subprog_20']} = gelu(inputs={X=['linear_14.tmp_1.subprog_20']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_15.tmp_0.subprog_20']} = matmul_v2(inputs={X=['gelu_3.tmp_0.subprog_20'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_15.tmp_1.subprog_20']} = elementwise_add(inputs={X=['linear_15.tmp_0.subprog_20'], Y=['linear_15.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_8@GRAD']} = sum(inputs={X=['tmp_8@GRAD@RENAME@block0@0', 'tmp_8@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_7@GRAD@RENAME@block0@0'], Y@GRAD=['linear_15.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['tmp_7.subprog_20'], Y=['linear_15.tmp_1.subprog_20']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_15.tmp_0@GRAD'], Y@GRAD=['linear_15.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_15.tmp_1@GRAD'], X=['linear_15.tmp_0.subprog_20'], Y=['linear_15.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_15.b_0', 'linear_15.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_3.tmp_0@GRAD'], Y@GRAD=['linear_15.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_15.tmp_0@GRAD'], X=['gelu_3.tmp_0.subprog_20'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_14.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_3.tmp_0@GRAD'], X=['linear_14.tmp_1.subprog_20']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_14.tmp_0@GRAD'], Y@GRAD=['linear_14.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_14.tmp_1@GRAD'], X=['linear_14.tmp_0.subprog_20'], Y=['linear_14.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_14.b_0', 'linear_14.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_7.tmp_2@GRAD'], Y@GRAD=['linear_14.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_14.tmp_0@GRAD'], X=['layer_norm_7.tmp_2.subprog_20'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_7.b_0@GRAD'], Scale@GRAD=['layer_norm_7.w_0@GRAD'], X@GRAD=['tmp_7@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_7.b_0'], Mean=['layer_norm_7.tmp_0.subprog_20'], Scale=['layer_norm_7.w_0'], Variance=['layer_norm_7.tmp_1.subprog_20'], X=['tmp_7.subprog_20'], Y@GRAD=['layer_norm_7.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['layer_norm_7.b_0', 'layer_norm_7.b_0@GRAD', 'layer_norm_7.w_0', 'layer_norm_7.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_7@GRAD']} = sum(inputs={X=['tmp_7@GRAD@RENAME@block0@0', 'tmp_7@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_6@GRAD@RENAME@block0@0'], Y@GRAD=['linear_13.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_7@GRAD'], X=['tmp_6'], Y=['linear_13.tmp_1.subprog_20']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_13.tmp_0@GRAD'], Y@GRAD=['linear_13.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_13.tmp_1@GRAD'], X=['linear_13.tmp_0.subprog_20'], Y=['linear_13.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_13.b_0', 'linear_13.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_7.tmp_0@GRAD'], Y@GRAD=['linear_13.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_13.tmp_0@GRAD'], X=['reshape2_7.tmp_0.subprog_20'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_15.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1.subprog_20']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['transpose_15.tmp_1.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_3.tmp_0@GRAD'], Y@GRAD=['transpose_14.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_7.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_3.tmp_0.subprog_20'], Y=['transpose_14.tmp_0.subprog_20']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_7.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_3.tmp_0.subprog_20'], Out@GRAD=['fused_softmax_mask_upper_triangle_3.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_6.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_7.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_6.tmp_0@GRAD'], Y@GRAD=['transpose_13.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_6.tmp_0@GRAD'], X=['scale_6.tmp_0.subprog_20'], Y=['transpose_13.tmp_0.subprog_20']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_12.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_6.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_3.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_14.tmp_0@GRAD'], XShape=['transpose_14.tmp_1.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_3.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_13.tmp_0@GRAD'], XShape=['transpose_13.tmp_1.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_12.tmp_0@GRAD'], XShape=['transpose_12.tmp_1.subprog_20']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_3.tmp_0@GRAD', 'split_3.tmp_1@GRAD', 'split_3.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_12.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1.subprog_20']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_12.tmp_0@GRAD'], Y@GRAD=['linear_12.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_12.tmp_1@GRAD'], X=['linear_12.tmp_0.subprog_20'], Y=['linear_12.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_12.b_0', 'linear_12.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_6.tmp_2@GRAD'], Y@GRAD=['linear_12.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_12.tmp_0@GRAD'], X=['layer_norm_6.tmp_2.subprog_20'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_6.b_0@GRAD'], Scale@GRAD=['layer_norm_6.w_0@GRAD'], X@GRAD=['tmp_6@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_6.b_0'], Mean=['layer_norm_6.tmp_0.subprog_20'], Scale=['layer_norm_6.w_0'], Variance=['layer_norm_6.tmp_1.subprog_20'], X=['tmp_6'], Y@GRAD=['layer_norm_6.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['layer_norm_6.b_0', 'layer_norm_6.b_0@GRAD', 'layer_norm_6.w_0', 'layer_norm_6.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_4.tmp_0.subprog_21'], Variance=['layer_norm_4.tmp_1.subprog_21'], Y=['layer_norm_4.tmp_2.subprog_21']} = layer_norm(inputs={Bias=['layer_norm_4.b_0'], Scale=['layer_norm_4.w_0'], X=['tmp_4']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_8.tmp_0.subprog_21']} = matmul_v2(inputs={X=['layer_norm_4.tmp_2.subprog_21'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_8.tmp_1.subprog_21']} = elementwise_add(inputs={X=['linear_8.tmp_0.subprog_21'], Y=['linear_8.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_4.tmp_0.subprog_21'], XShape=['reshape2_4.tmp_1.subprog_21']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_1.subprog_21']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_2.tmp_0.subprog_21', 'split_2.tmp_1.subprog_21', 'split_2.tmp_2.subprog_21']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_4.tmp_0.subprog_21']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_8.tmp_0.subprog_21'], XShape=['transpose_8.tmp_1.subprog_21']} = transpose2(inputs={X=['split_2.tmp_0.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_9.tmp_0.subprog_21'], XShape=['transpose_9.tmp_1.subprog_21']} = transpose2(inputs={X=['split_2.tmp_1.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_10.tmp_0.subprog_21'], XShape=['transpose_10.tmp_1.subprog_21']} = transpose2(inputs={X=['split_2.tmp_2.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_4.tmp_0.subprog_21']} = scale(inputs={ScaleTensor=[], X=['transpose_8.tmp_0.subprog_21']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_4.tmp_0.subprog_21']} = matmul_v2(inputs={X=['scale_4.tmp_0.subprog_21'], Y=['transpose_9.tmp_0.subprog_21']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_5.tmp_0.subprog_21']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_4.tmp_0.subprog_21']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_2.tmp_0.subprog_21']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_5.tmp_0.subprog_21']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_5.tmp_0.subprog_21']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_2.tmp_0.subprog_21'], Y=['transpose_10.tmp_0.subprog_21']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_11.tmp_0.subprog_21'], XShape=['transpose_11.tmp_1.subprog_21']} = transpose2(inputs={X=['matmul_v2_5.tmp_0.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0.subprog_21'], XShape=['reshape2_5.tmp_1.subprog_21']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_11.tmp_0.subprog_21']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.tmp_0.subprog_21']} = matmul_v2(inputs={X=['reshape2_5.tmp_0.subprog_21'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_9.tmp_1.subprog_21']} = elementwise_add(inputs={X=['linear_9.tmp_0.subprog_21'], Y=['linear_9.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_5.subprog_21']} = elementwise_add(inputs={X=['tmp_4'], Y=['linear_9.tmp_1.subprog_21']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_5.tmp_0.subprog_21'], Variance=['layer_norm_5.tmp_1.subprog_21'], Y=['layer_norm_5.tmp_2.subprog_21']} = layer_norm(inputs={Bias=['layer_norm_5.b_0'], Scale=['layer_norm_5.w_0'], X=['tmp_5.subprog_21']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_10.tmp_0.subprog_21']} = matmul_v2(inputs={X=['layer_norm_5.tmp_2.subprog_21'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_10.tmp_1.subprog_21']} = elementwise_add(inputs={X=['linear_10.tmp_0.subprog_21'], Y=['linear_10.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_2.tmp_0.subprog_21']} = gelu(inputs={X=['linear_10.tmp_1.subprog_21']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_11.tmp_0.subprog_21']} = matmul_v2(inputs={X=['gelu_2.tmp_0.subprog_21'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_11.tmp_1.subprog_21']} = elementwise_add(inputs={X=['linear_11.tmp_0.subprog_21'], Y=['linear_11.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_6@GRAD']} = sum(inputs={X=['tmp_6@GRAD@RENAME@block0@0', 'tmp_6@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@0'], Y@GRAD=['linear_11.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['tmp_5.subprog_21'], Y=['linear_11.tmp_1.subprog_21']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_11.tmp_0@GRAD'], Y@GRAD=['linear_11.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_11.tmp_1@GRAD'], X=['linear_11.tmp_0.subprog_21'], Y=['linear_11.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_11.b_0', 'linear_11.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_2.tmp_0@GRAD'], Y@GRAD=['linear_11.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_11.tmp_0@GRAD'], X=['gelu_2.tmp_0.subprog_21'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_10.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_2.tmp_0@GRAD'], X=['linear_10.tmp_1.subprog_21']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_10.tmp_0@GRAD'], Y@GRAD=['linear_10.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_10.tmp_1@GRAD'], X=['linear_10.tmp_0.subprog_21'], Y=['linear_10.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_10.b_0', 'linear_10.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_5.tmp_2@GRAD'], Y@GRAD=['linear_10.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_10.tmp_0@GRAD'], X=['layer_norm_5.tmp_2.subprog_21'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_5.b_0@GRAD'], Scale@GRAD=['layer_norm_5.w_0@GRAD'], X@GRAD=['tmp_5@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_5.b_0'], Mean=['layer_norm_5.tmp_0.subprog_21'], Scale=['layer_norm_5.w_0'], Variance=['layer_norm_5.tmp_1.subprog_21'], X=['tmp_5.subprog_21'], Y@GRAD=['layer_norm_5.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['layer_norm_5.b_0', 'layer_norm_5.b_0@GRAD', 'layer_norm_5.w_0', 'layer_norm_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_5@GRAD']} = sum(inputs={X=['tmp_5@GRAD@RENAME@block0@0', 'tmp_5@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_4@GRAD@RENAME@block0@0'], Y@GRAD=['linear_9.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['tmp_4'], Y=['linear_9.tmp_1.subprog_21']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_9.tmp_0@GRAD'], Y@GRAD=['linear_9.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_9.tmp_1@GRAD'], X=['linear_9.tmp_0.subprog_21'], Y=['linear_9.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_9.b_0', 'linear_9.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD'], Y@GRAD=['linear_9.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_9.tmp_0@GRAD'], X=['reshape2_5.tmp_0.subprog_21'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1.subprog_21']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_2.tmp_0@GRAD'], Y@GRAD=['transpose_10.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_5.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_2.tmp_0.subprog_21'], Y=['transpose_10.tmp_0.subprog_21']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_5.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_2.tmp_0.subprog_21'], Out@GRAD=['fused_softmax_mask_upper_triangle_2.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_5.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_4.tmp_0@GRAD'], Y@GRAD=['transpose_9.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_4.tmp_0@GRAD'], X=['scale_4.tmp_0.subprog_21'], Y=['transpose_9.tmp_0.subprog_21']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_4.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_2.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_2.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1.subprog_21']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_4.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_2.tmp_0@GRAD', 'split_2.tmp_1@GRAD', 'split_2.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_8.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1.subprog_21']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_8.tmp_0@GRAD'], Y@GRAD=['linear_8.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_8.tmp_1@GRAD'], X=['linear_8.tmp_0.subprog_21'], Y=['linear_8.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_8.b_0', 'linear_8.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_4.tmp_2@GRAD'], Y@GRAD=['linear_8.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_8.tmp_0@GRAD'], X=['layer_norm_4.tmp_2.subprog_21'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_4.b_0@GRAD'], Scale@GRAD=['layer_norm_4.w_0@GRAD'], X@GRAD=['tmp_4@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_4.b_0'], Mean=['layer_norm_4.tmp_0.subprog_21'], Scale=['layer_norm_4.w_0'], Variance=['layer_norm_4.tmp_1.subprog_21'], X=['tmp_4'], Y@GRAD=['layer_norm_4.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['layer_norm_4.b_0', 'layer_norm_4.b_0@GRAD', 'layer_norm_4.w_0', 'layer_norm_4.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_2.tmp_0.subprog_22'], Variance=['layer_norm_2.tmp_1.subprog_22'], Y=['layer_norm_2.tmp_2.subprog_22']} = layer_norm(inputs={Bias=['layer_norm_2.b_0'], Scale=['layer_norm_2.w_0'], X=['tmp_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_4.tmp_0.subprog_22']} = matmul_v2(inputs={X=['layer_norm_2.tmp_2.subprog_22'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_4.tmp_1.subprog_22']} = elementwise_add(inputs={X=['linear_4.tmp_0.subprog_22'], Y=['linear_4.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_2.tmp_0.subprog_22'], XShape=['reshape2_2.tmp_1.subprog_22']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_4.tmp_1.subprog_22']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_1.tmp_0.subprog_22', 'split_1.tmp_1.subprog_22', 'split_1.tmp_2.subprog_22']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_2.tmp_0.subprog_22']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_4.tmp_0.subprog_22'], XShape=['transpose_4.tmp_1.subprog_22']} = transpose2(inputs={X=['split_1.tmp_0.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0.subprog_22'], XShape=['transpose_5.tmp_1.subprog_22']} = transpose2(inputs={X=['split_1.tmp_1.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_6.tmp_0.subprog_22'], XShape=['transpose_6.tmp_1.subprog_22']} = transpose2(inputs={X=['split_1.tmp_2.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_2.tmp_0.subprog_22']} = scale(inputs={ScaleTensor=[], X=['transpose_4.tmp_0.subprog_22']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0.subprog_22']} = matmul_v2(inputs={X=['scale_2.tmp_0.subprog_22'], Y=['transpose_5.tmp_0.subprog_22']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_3.tmp_0.subprog_22']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_2.tmp_0.subprog_22']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_1.tmp_0.subprog_22']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_3.tmp_0.subprog_22']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_3.tmp_0.subprog_22']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_1.tmp_0.subprog_22'], Y=['transpose_6.tmp_0.subprog_22']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_7.tmp_0.subprog_22'], XShape=['transpose_7.tmp_1.subprog_22']} = transpose2(inputs={X=['matmul_v2_3.tmp_0.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_3.tmp_0.subprog_22'], XShape=['reshape2_3.tmp_1.subprog_22']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0.subprog_22']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_5.tmp_0.subprog_22']} = matmul_v2(inputs={X=['reshape2_3.tmp_0.subprog_22'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_5.tmp_1.subprog_22']} = elementwise_add(inputs={X=['linear_5.tmp_0.subprog_22'], Y=['linear_5.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_3.subprog_22']} = elementwise_add(inputs={X=['tmp_2'], Y=['linear_5.tmp_1.subprog_22']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_3.tmp_0.subprog_22'], Variance=['layer_norm_3.tmp_1.subprog_22'], Y=['layer_norm_3.tmp_2.subprog_22']} = layer_norm(inputs={Bias=['layer_norm_3.b_0'], Scale=['layer_norm_3.w_0'], X=['tmp_3.subprog_22']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_6.tmp_0.subprog_22']} = matmul_v2(inputs={X=['layer_norm_3.tmp_2.subprog_22'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_6.tmp_1.subprog_22']} = elementwise_add(inputs={X=['linear_6.tmp_0.subprog_22'], Y=['linear_6.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_1.tmp_0.subprog_22']} = gelu(inputs={X=['linear_6.tmp_1.subprog_22']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_7.tmp_0.subprog_22']} = matmul_v2(inputs={X=['gelu_1.tmp_0.subprog_22'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_7.tmp_1.subprog_22']} = elementwise_add(inputs={X=['linear_7.tmp_0.subprog_22'], Y=['linear_7.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_4@GRAD']} = sum(inputs={X=['tmp_4@GRAD@RENAME@block0@0', 'tmp_4@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_3@GRAD@RENAME@block0@0'], Y@GRAD=['linear_7.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['tmp_3.subprog_22'], Y=['linear_7.tmp_1.subprog_22']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_7.tmp_0@GRAD'], Y@GRAD=['linear_7.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_7.tmp_1@GRAD'], X=['linear_7.tmp_0.subprog_22'], Y=['linear_7.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_7.b_0', 'linear_7.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_1.tmp_0@GRAD'], Y@GRAD=['linear_7.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_7.tmp_0@GRAD'], X=['gelu_1.tmp_0.subprog_22'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_6.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_1.tmp_0@GRAD'], X=['linear_6.tmp_1.subprog_22']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_6.tmp_0@GRAD'], Y@GRAD=['linear_6.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_6.tmp_1@GRAD'], X=['linear_6.tmp_0.subprog_22'], Y=['linear_6.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_6.b_0', 'linear_6.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_3.tmp_2@GRAD'], Y@GRAD=['linear_6.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_6.tmp_0@GRAD'], X=['layer_norm_3.tmp_2.subprog_22'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_3.b_0@GRAD'], Scale@GRAD=['layer_norm_3.w_0@GRAD'], X@GRAD=['tmp_3@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_3.b_0'], Mean=['layer_norm_3.tmp_0.subprog_22'], Scale=['layer_norm_3.w_0'], Variance=['layer_norm_3.tmp_1.subprog_22'], X=['tmp_3.subprog_22'], Y@GRAD=['layer_norm_3.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['layer_norm_3.b_0', 'layer_norm_3.b_0@GRAD', 'layer_norm_3.w_0', 'layer_norm_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_3@GRAD']} = sum(inputs={X=['tmp_3@GRAD@RENAME@block0@0', 'tmp_3@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_2@GRAD@RENAME@block0@0'], Y@GRAD=['linear_5.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_3@GRAD'], X=['tmp_2'], Y=['linear_5.tmp_1.subprog_22']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_5.tmp_0@GRAD'], Y@GRAD=['linear_5.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_5.tmp_1@GRAD'], X=['linear_5.tmp_0.subprog_22'], Y=['linear_5.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_5.b_0', 'linear_5.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_3.tmp_0@GRAD'], Y@GRAD=['linear_5.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_5.tmp_0@GRAD'], X=['reshape2_3.tmp_0.subprog_22'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_3.tmp_0@GRAD'], XShape=['reshape2_3.tmp_1.subprog_22']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_1.tmp_0@GRAD'], Y@GRAD=['transpose_6.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_1.tmp_0.subprog_22'], Y=['transpose_6.tmp_0.subprog_22']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_3.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_1.tmp_0.subprog_22'], Out@GRAD=['fused_softmax_mask_upper_triangle_1.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_2.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_3.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_2.tmp_0@GRAD'], Y@GRAD=['transpose_5.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['scale_2.tmp_0.subprog_22'], Y=['transpose_5.tmp_0.subprog_22']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_2.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_1.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_1.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1.subprog_22']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_2.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_1.tmp_0@GRAD', 'split_1.tmp_1@GRAD', 'split_1.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_4.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1.subprog_22']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_4.tmp_0@GRAD'], Y@GRAD=['linear_4.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_4.tmp_1@GRAD'], X=['linear_4.tmp_0.subprog_22'], Y=['linear_4.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_4.b_0', 'linear_4.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_2.tmp_2@GRAD'], Y@GRAD=['linear_4.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_4.tmp_0@GRAD'], X=['layer_norm_2.tmp_2.subprog_22'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_2.b_0@GRAD'], Scale@GRAD=['layer_norm_2.w_0@GRAD'], X@GRAD=['tmp_2@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_2.b_0'], Mean=['layer_norm_2.tmp_0.subprog_22'], Scale=['layer_norm_2.w_0'], Variance=['layer_norm_2.tmp_1.subprog_22'], X=['tmp_2'], Y@GRAD=['layer_norm_2.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['layer_norm_2.b_0', 'layer_norm_2.b_0@GRAD', 'layer_norm_2.w_0', 'layer_norm_2.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_0.tmp_0.subprog_23'], Variance=['layer_norm_0.tmp_1.subprog_23'], Y=['layer_norm_0.tmp_2.subprog_23']} = layer_norm(inputs={Bias=['layer_norm_0.b_0'], Scale=['layer_norm_0.w_0'], X=['tmp_0']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_0.tmp_0.subprog_23']} = matmul_v2(inputs={X=['layer_norm_0.tmp_2.subprog_23'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_0.tmp_1.subprog_23']} = elementwise_add(inputs={X=['linear_0.tmp_0.subprog_23'], Y=['linear_0.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0.subprog_23'], XShape=['reshape2_0.tmp_1.subprog_23']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_1.subprog_23']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_0.tmp_0.subprog_23', 'split_0.tmp_1.subprog_23', 'split_0.tmp_2.subprog_23']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_0.tmp_0.subprog_23']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0.subprog_23'], XShape=['transpose_0.tmp_1.subprog_23']} = transpose2(inputs={X=['split_0.tmp_0.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_1.tmp_0.subprog_23'], XShape=['transpose_1.tmp_1.subprog_23']} = transpose2(inputs={X=['split_0.tmp_1.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_2.tmp_0.subprog_23'], XShape=['transpose_2.tmp_1.subprog_23']} = transpose2(inputs={X=['split_0.tmp_2.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_0.tmp_0.subprog_23']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0.subprog_23']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0.subprog_23']} = matmul_v2(inputs={X=['scale_0.tmp_0.subprog_23'], Y=['transpose_1.tmp_0.subprog_23']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_1.tmp_0.subprog_23']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_0.tmp_0.subprog_23']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_0.tmp_0.subprog_23']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_1.tmp_0.subprog_23']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_1.tmp_0.subprog_23']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_0.tmp_0.subprog_23'], Y=['transpose_2.tmp_0.subprog_23']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_3.tmp_0.subprog_23'], XShape=['transpose_3.tmp_1.subprog_23']} = transpose2(inputs={X=['matmul_v2_1.tmp_0.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0.subprog_23'], XShape=['reshape2_1.tmp_1.subprog_23']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0.subprog_23']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.tmp_0.subprog_23']} = matmul_v2(inputs={X=['reshape2_1.tmp_0.subprog_23'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_1.tmp_1.subprog_23']} = elementwise_add(inputs={X=['linear_1.tmp_0.subprog_23'], Y=['linear_1.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1.subprog_23']} = elementwise_add(inputs={X=['tmp_0'], Y=['linear_1.tmp_1.subprog_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_1.tmp_0.subprog_23'], Variance=['layer_norm_1.tmp_1.subprog_23'], Y=['layer_norm_1.tmp_2.subprog_23']} = layer_norm(inputs={Bias=['layer_norm_1.b_0'], Scale=['layer_norm_1.w_0'], X=['tmp_1.subprog_23']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_2.tmp_0.subprog_23']} = matmul_v2(inputs={X=['layer_norm_1.tmp_2.subprog_23'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_2.tmp_1.subprog_23']} = elementwise_add(inputs={X=['linear_2.tmp_0.subprog_23'], Y=['linear_2.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_0.tmp_0.subprog_23']} = gelu(inputs={X=['linear_2.tmp_1.subprog_23']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_3.tmp_0.subprog_23']} = matmul_v2(inputs={X=['gelu_0.tmp_0.subprog_23'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_3.tmp_1.subprog_23']} = elementwise_add(inputs={X=['linear_3.tmp_0.subprog_23'], Y=['linear_3.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_2@GRAD']} = sum(inputs={X=['tmp_2@GRAD@RENAME@block0@0', 'tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_1@GRAD@RENAME@block0@0'], Y@GRAD=['linear_3.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_2@GRAD'], X=['tmp_1.subprog_23'], Y=['linear_3.tmp_1.subprog_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_3.tmp_0@GRAD'], Y@GRAD=['linear_3.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_3.tmp_1@GRAD'], X=['linear_3.tmp_0.subprog_23'], Y=['linear_3.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_3.b_0', 'linear_3.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_0.tmp_0@GRAD'], Y@GRAD=['linear_3.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_3.tmp_0@GRAD'], X=['gelu_0.tmp_0.subprog_23'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_2.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_0.tmp_0@GRAD'], X=['linear_2.tmp_1.subprog_23']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_2.tmp_0@GRAD'], Y@GRAD=['linear_2.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_2.tmp_1@GRAD'], X=['linear_2.tmp_0.subprog_23'], Y=['linear_2.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_2.b_0', 'linear_2.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_1.tmp_2@GRAD'], Y@GRAD=['linear_2.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_2.tmp_0@GRAD'], X=['layer_norm_1.tmp_2.subprog_23'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_1.b_0@GRAD'], Scale@GRAD=['layer_norm_1.w_0@GRAD'], X@GRAD=['tmp_1@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_1.b_0'], Mean=['layer_norm_1.tmp_0.subprog_23'], Scale=['layer_norm_1.w_0'], Variance=['layer_norm_1.tmp_1.subprog_23'], X=['tmp_1.subprog_23'], Y@GRAD=['layer_norm_1.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['layer_norm_1.b_0', 'layer_norm_1.b_0@GRAD', 'layer_norm_1.w_0', 'layer_norm_1.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_1@GRAD']} = sum(inputs={X=['tmp_1@GRAD@RENAME@block0@0', 'tmp_1@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_0@GRAD@RENAME@block0@0'], Y@GRAD=['linear_1.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_1@GRAD'], X=['tmp_0'], Y=['linear_1.tmp_1.subprog_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_1.tmp_0@GRAD'], Y@GRAD=['linear_1.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_1.tmp_1@GRAD'], X=['linear_1.tmp_0.subprog_23'], Y=['linear_1.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_1.b_0', 'linear_1.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD'], Y@GRAD=['linear_1.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_1.tmp_0@GRAD'], X=['reshape2_1.tmp_0.subprog_23'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_3.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1.subprog_23']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_0.tmp_0@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_0.tmp_0.subprog_23'], Y=['transpose_2.tmp_0.subprog_23']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_1.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_0.tmp_0.subprog_23'], Out@GRAD=['fused_softmax_mask_upper_triangle_0.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_1.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_0.tmp_0@GRAD'], Y@GRAD=['transpose_1.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['scale_0.tmp_0.subprog_23'], Y=['transpose_1.tmp_0.subprog_23']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_0.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_0.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_0.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1.subprog_23']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_0.tmp_0@GRAD', 'split_0.tmp_1@GRAD', 'split_0.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_0.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1.subprog_23']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_0.tmp_0@GRAD'], Y@GRAD=['linear_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_0.tmp_1@GRAD'], X=['linear_0.tmp_0.subprog_23'], Y=['linear_0.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_0.b_0', 'linear_0.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_0.tmp_2@GRAD'], Y@GRAD=['linear_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_0.tmp_0@GRAD'], X=['layer_norm_0.tmp_2.subprog_23'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_0.b_0@GRAD'], Scale@GRAD=['layer_norm_0.w_0@GRAD'], X@GRAD=['tmp_0@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_0.b_0'], Mean=['layer_norm_0.tmp_0.subprog_23'], Scale=['layer_norm_0.w_0'], Variance=['layer_norm_0.tmp_1.subprog_23'], X=['tmp_0'], Y@GRAD=['layer_norm_0.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['layer_norm_0.b_0', 'layer_norm_0.b_0@GRAD', 'layer_norm_0.w_0', 'layer_norm_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_0@GRAD']} = sum(inputs={X=['tmp_0@GRAD@RENAME@block0@0', 'tmp_0@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['embedding_0.tmp_0@GRAD'], Y@GRAD=['embedding_1.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_0@GRAD'], X=['embedding_0.tmp_0'], Y=['embedding_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {W@GRAD=['embedding_1.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['input1'], Out@GRAD=['embedding_1.tmp_0@GRAD'], W=['embedding_1.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_1.w_0', 'embedding_1.w_0@GRAD'], padding_idx = -1, with_quant_attr = False)
    {W@GRAD=['embedding_0.w_0@GRAD@RENAME@block0@1']} = lookup_table_v2_grad(inputs={Ids=['input0'], Out@GRAD=['embedding_0.tmp_0@GRAD'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], padding_idx = -1, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['embedding_0.w_0@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@RENAME@block0@1']} = scale(inputs={ScaleTensor=[], X=['embedding_0.w_0@GRAD@RENAME@block0@1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@RENAME@block0@0@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [50304, 1024], peer = 0, ring_id = 35, use_calc_stream = True, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@RENAME@block0@0@RESHARD_0']} = assign(inputs={X=['embedding_0.w_0@GRAD@RENAME@block0@0@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD']} = sum(inputs={X=['embedding_0.w_0@GRAD@RENAME@block0@0@RESHARD_0', 'embedding_0.w_0@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['embedding_0.w_0@GRAD@MERGE'], Y=['embedding_0.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gradient_merge_step']} = increment(inputs={X=['gradient_merge_step']}, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], step = 1.0, with_quant_attr = False)
    {Out=['gradient_merge_step']} = elementwise_mod(inputs={X=['gradient_merge_step'], Y=['gradient_merge_k']}, axis = -1, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gradient_merge_cond']} = equal(inputs={X=['gradient_merge_step'], Y=['gradient_merge_zero']}, axis = -1, force_cpu = False, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_13.w_0_fp32_master_0_moment1_0', 'linear_14.b_0_fp32_master_0_moment2_0', 'linear_1.b_0_fp32_master_0_moment1_0', 'layer_norm_7.b_0', 'linear_22.w_0_fp32_master_0_moment2_0', 'linear_39.w_0_fp32_master_0', 'embedding_1.w_0_fp32_master_0_moment1_0', 'linear_9.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_19.w_0_moment1_0', 'linear_31.w_0_fp32_master_0_moment1_0', 'linear_33.w_0_fp32_master_0', 'linear_14.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_10.b_0_beta2_pow_acc_0', 'layer_norm_3.w_0_beta2_pow_acc_0', 'linear_23.b_0_fp32_master_0_moment1_0', 'linear_10.b_0_fp32_master_0_moment2_0', 'linear_8.w_0_fp32_master_0_moment1_0', 'embedding_1.w_0@GRAD@MERGE', 'linear_38.w_0', 'linear_36.b_0_fp32_master_0_moment2_0', 'layer_norm_21.b_0@GRAD@MERGE', 'linear_14.b_0_fp32_master_0', 'layer_norm_9.b_0_moment2_0', 'linear_14.w_0_fp32_master_0', 'layer_norm_17.w_0_moment1_0', 'opt_opt_squared_l2_norm_93.tmp_0', 'linear_28.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_13', 'linear_38.w_0_fp32_master_0_moment1_0', 'linear_22.b_0_fp32_master_0_moment2_0', 'layer_norm_22.w_0@GRAD@MERGE', 'linear_6.w_0_fp32_master_0_moment2_0', 'layer_norm_12.w_0_moment2_0', 'linear_7.w_0_fp32_master_0', 'linear_43.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_44.w_0@GRAD@MERGE', 'linear_6.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_11.b_0', 'linear_13.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_5.b_0', 'linear_2.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_32.b_0_fp32_master_0_moment2_0', 'linear_37.b_0@GRAD@MERGE', 'layer_norm_18.w_0', 'linear_1.w_0@GRAD@MERGE', 'linear_20.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_30.b_0@GRAD@MERGE', 'opt_tmp_41', 'linear_42.w_0_fp32_master_0_moment1_0', 'linear_17.b_0_fp32_master_0', 'linear_36.b_0_fp32_master_0_moment1_0', 'opt_tmp_81', 'linear_20.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_3.w_0_moment2_0', 'opt_tmp_90', 'linear_47.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_21.w_0_beta1_pow_acc_0', 'layer_norm_4.b_0_beta2_pow_acc_0', 'linear_30.b_0_fp32_master_0', 'linear_28.b_0_fp32_master_0_moment2_0', 'linear_35.w_0', 'linear_43.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_17.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_14.w_0@GRAD@MERGE', 'linear_38.b_0_fp32_master_0_moment1_0', 'layer_norm_22.w_0_moment1_0', 'opt_opt_squared_l2_norm_70.tmp_0', 'opt_tmp_70', 'linear_33.b_0@GRAD@MERGE', 'linear_9.b_0_fp32_master_0', 'linear_46.b_0_fp32_master_0', 'opt_opt_squared_l2_norm_64.tmp_0', 'linear_38.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_5.b_0@GRAD@MERGE', 'linear_21.b_0_fp32_master_0', 'layer_norm_6.b_0_beta1_pow_acc_0', 'layer_norm_15.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_11.tmp_0', 'linear_26.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_18.b_0_moment2_0', 'linear_47.b_0_fp32_master_0_moment2_0', 'linear_45.b_0', 'linear_5.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_19.w_0_beta1_pow_acc_0', 'embedding_0.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_130.tmp_0', 'opt_opt_squared_l2_norm_131.tmp_0', 'opt_tmp_30', 'linear_32.b_0_fp32_master_0', 'linear_5.w_0@GRAD@MERGE', 'layer_norm_2.w_0_beta2_pow_acc_0', 'linear_44.b_0_fp32_master_0_moment2_0', 'layer_norm_16.b_0', 'layer_norm_13.w_0_beta2_pow_acc_0', 'linear_42.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_23.b_0_fp32_master_0_moment2_0', 'opt_tmp_73', 'opt_opt_squared_l2_norm_49.tmp_0', 'linear_5.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_4.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_43.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_29.w_0_fp32_master_0_moment1_0', 'linear_4.w_0_fp32_master_0', 'linear_15.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_111.tmp_0', 'layer_norm_5.w_0_moment1_0', 'layer_norm_3.w_0_moment1_0', 'opt_opt_squared_l2_norm_34.tmp_0', 'linear_36.b_0', 'linear_38.w_0_fp32_master_0', 'linear_11.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_34.b_0_fp32_master_0_moment1_0', 'opt_tmp_68', 'layer_norm_19.w_0', 'linear_17.w_0_fp32_master_0_moment2_0', 'layer_norm_12.w_0_moment1_0', 'layer_norm_20.b_0_moment2_0', 'linear_10.b_0_fp32_master_0', 'linear_12.w_0', 'layer_norm_4.w_0', 'opt_tmp_19', 'linear_7.w_0', 'opt_opt_squared_l2_norm_125.tmp_0', 'layer_norm_5.b_0_beta1_pow_acc_0', 'linear_35.b_0_fp32_master_0_moment1_0', 'linear_0.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_13.w_0_beta1_pow_acc_0', 'linear_1.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_26.b_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_9.tmp_0', 'linear_24.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_27.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_21.b_0_moment1_0', 'linear_37.w_0_fp32_master_0', 'layer_norm_17.w_0_moment2_0', 'opt_opt_squared_l2_norm_114.tmp_0', 'layer_norm_4.b_0', 'linear_36.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_9.w_0@GRAD@MERGE', 'linear_2.b_0_fp32_master_0_moment1_0', 'linear_19.b_0', 'linear_33.b_0_fp32_master_0', 'linear_10.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_38.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_10.w_0_fp32_master_0_moment1_0', 'layer_norm_18.b_0@GRAD@MERGE', 'layer_norm_21.b_0_beta2_pow_acc_0', 'layer_norm_7.w_0_beta1_pow_acc_0', 'linear_18.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_105.tmp_0', 'layer_norm_1.b_0', 'linear_46.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_23.b_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_92.tmp_0', 'linear_17.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_69', 'opt_tmp_46', 'linear_12.w_0_fp32_master_0_moment1_0', 'linear_45.w_0_fp32_master_0_moment2_0', 'linear_36.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_0.b_0_moment2_0', 'linear_16.b_0_fp32_master_0', 'linear_24.w_0_fp32_master_0', 'linear_25.b_0', 'layer_norm_9.w_0', 'opt_tmp_95', 'linear_30.w_0_fp32_master_0_moment1_0', 'linear_15.b_0_fp32_master_0', 'layer_norm_6.b_0_moment1_0', 'opt_opt_squared_l2_norm_132.tmp_0', 'linear_17.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_3.w_0_fp32_master_0_moment1_0', 'opt_tmp_66', 'layer_norm_2.b_0', 'opt_opt_squared_l2_norm_67.tmp_0', 'linear_36.w_0_fp32_master_0_moment1_0', 'linear_44.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_22.w_0@GRAD@MERGE', 'opt_elementwise_div_0', 'linear_1.w_0', 'layer_norm_13.w_0_moment2_0', 'linear_28.b_0', 'linear_38.w_0@GRAD@MERGE', 'layer_norm_19.w_0_beta2_pow_acc_0', 'opt_opt_stack_1.tmp_0', 'linear_35.b_0_fp32_master_0', 'opt_tmp_61', 'linear_5.b_0_fp32_master_0_moment1_0', 'layer_norm_12.b_0_moment1_0', 'opt_opt_squared_l2_norm_44.tmp_0', 'layer_norm_18.w_0_beta2_pow_acc_0', 'linear_30.w_0', 'linear_22.b_0_fp32_master_0', 'linear_18.w_0_fp32_master_0', 'linear_18.b_0@GRAD@MERGE', 'linear_20.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_16.w_0_beta2_pow_acc_0', 'layer_norm_18.b_0_beta1_pow_acc_0', 'layer_norm_3.b_0_beta2_pow_acc_0', 'layer_norm_2.b_0_beta2_pow_acc_0', 'layer_norm_3.b_0_moment2_0', 'linear_21.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_15.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_53.tmp_0', 'linear_31.b_0@GRAD@MERGE', 'linear_41.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_elementwise_max_0', 'linear_34.b_0_fp32_master_0_moment2_0', 'opt_tmp_82', 'linear_47.w_0_fp32_master_0_moment1_0', 'linear_47.w_0_fp32_master_0', 'linear_4.w_0', 'linear_12.w_0_fp32_master_0_moment2_0', 'linear_25.w_0', 'opt_tmp_20', 'opt_tmp_79', 'layer_norm_8.b_0@GRAD@MERGE', 'linear_2.b_0_fp32_master_0', 'linear_40.b_0', 'linear_37.w_0_fp32_master_0_moment1_0', 'linear_3.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_44.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_8.b_0_moment2_0', 'linear_38.b_0@GRAD@MERGE', 'find_infinite_scale.@fp16_0@cast_int32', 'linear_0.w_0@GRAD@MERGE', 'layer_norm_7.b_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_133.tmp_0', 'linear_31.w_0@GRAD@MERGE', 'linear_16.b_0_fp32_master_0_moment2_0', 'linear_31.b_0_fp32_master_0_moment2_0', 'linear_14.b_0', 'opt_opt_squared_l2_norm_69.tmp_0', 'opt_opt_squared_l2_norm_139.tmp_0', 'opt_opt_squared_l2_norm_2.tmp_0', 'linear_29.w_0@GRAD@MERGE', 'layer_norm_4.b_0_beta1_pow_acc_0', 'layer_norm_9.w_0_beta1_pow_acc_0', 'linear_14.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_10.tmp_0', 'opt_opt_squared_l2_norm_82.tmp_0', 'opt_opt_squared_l2_norm_57.tmp_0', 'linear_32.w_0_fp32_master_0_moment1_0', 'linear_41.b_0@GRAD@MERGE', 'linear_0.b_0_fp32_master_0_moment2_0', 'linear_47.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_15.b_0_moment2_0', 'layer_norm_4.w_0_moment1_0', 'linear_17.b_0_fp32_master_0_moment1_0', 'linear_20.b_0', 'opt_opt_squared_l2_norm_16.tmp_0', 'linear_10.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_48.tmp_0', 'linear_38.b_0_fp32_master_0', 'opt_tmp_91', 'linear_40.w_0_fp32_master_0', 'layer_norm_13.b_0@GRAD@MERGE', 'linear_26.b_0', 'layer_norm_23.b_0', 'linear_40.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_8', 'linear_47.b_0_fp32_master_0_moment1_0', 'linear_46.w_0', 'linear_45.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_22.b_0', 'linear_0.b_0_fp32_master_0_moment1_0', 'layer_norm_20.w_0_moment2_0', 'opt_tmp_97', 'linear_26.b_0_fp32_master_0_moment1_0', 'linear_28.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_65.tmp_0', 'layer_norm_6.b_0_beta2_pow_acc_0', 'loss_scaling_0', 'opt_opt_squared_l2_norm_54.tmp_0', 'linear_40.w_0_fp32_master_0_moment1_0', 'linear_4.w_0_fp32_master_0_moment2_0', 'linear_14.b_0@GRAD@MERGE', 'linear_20.w_0', 'linear_32.b_0@GRAD@MERGE', 'linear_16.w_0', 'linear_13.b_0_fp32_master_0', 'linear_9.w_0_fp32_master_0_moment2_0', 'layer_norm_17.b_0_beta2_pow_acc_0', 'linear_41.w_0_fp32_master_0', 'layer_norm_11.w_0_moment2_0', 'linear_46.w_0@GRAD@MERGE', 'layer_norm_7.b_0_moment2_0', 'linear_13.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_123.tmp_0', 'layer_norm_9.w_0_moment1_0', 'linear_43.b_0_fp32_master_0_moment1_0', 'layer_norm_15.w_0_beta2_pow_acc_0', 'linear_3.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_43.b_0_fp32_master_0_moment2_0', 'linear_31.w_0_fp32_master_0', 'linear_26.w_0@GRAD@MERGE', 'linear_44.b_0@GRAD@MERGE', 'linear_32.b_0', 'linear_25.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_18.w_0@GRAD@MERGE', 'linear_2.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_122.tmp_0', 'layer_norm_22.b_0_moment2_0', 'opt_opt_squared_l2_norm_63.tmp_0', 'linear_18.b_0_fp32_master_0_moment1_0', 'linear_6.b_0_fp32_master_0_moment1_0', 'linear_27.b_0_fp32_master_0_moment1_0', 'linear_0.w_0', 'linear_16.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_81.tmp_0', 'opt_tmp_77', 'opt_tmp_53', 'linear_10.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0_fp32_master_0', 'linear_20.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_44.w_0', 'linear_23.w_0_fp32_master_0', 'linear_3.w_0_fp32_master_0', 'layer_norm_8.b_0_beta2_pow_acc_0', 'layer_norm_19.b_0_beta1_pow_acc_0', 'linear_35.b_0', 'linear_19.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_63', 'linear_6.b_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_79.tmp_0', 'linear_42.b_0_fp32_master_0', 'linear_33.b_0_fp32_master_0_moment2_0', 'linear_8.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_34.w_0_fp32_master_0_moment2_0', 'layer_norm_12.b_0_beta1_pow_acc_0', 'layer_norm_0.w_0_beta1_pow_acc_0', 'layer_norm_16.b_0_beta2_pow_acc_0', 'linear_5.w_0_fp32_master_0_moment1_0', 'linear_3.b_0_fp32_master_0', 'linear_16.b_0', 'linear_35.b_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_38.tmp_0', 'layer_norm_17.b_0_moment2_0', 'opt_tmp_43', 'layer_norm_11.b_0_moment2_0', 'linear_20.w_0_fp32_master_0_moment1_0', 'linear_9.b_0_fp32_master_0_moment2_0', 'layer_norm_21.b_0', 'layer_norm_23.b_0@GRAD@MERGE', 'linear_40.w_0', 'linear_40.w_0_fp32_master_0_moment2_0', 'linear_15.w_0_fp32_master_0', 'opt_tmp_88', 'layer_norm_19.b_0_beta2_pow_acc_0', 'linear_5.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_102.tmp_0', 'linear_37.b_0_fp32_master_0_moment1_0', 'embedding_1.w_0', 'linear_15.b_0_fp32_master_0_moment1_0', 'find_infinite_scale.tmp_0', 'opt_opt_squared_l2_norm_145.tmp_0', 'linear_1.b_0', 'layer_norm_11.b_0_beta2_pow_acc_0', 'linear_23.w_0', 'linear_12.w_0@GRAD@MERGE', 'opt_tmp_3', 'linear_7.b_0', 'linear_27.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_stack_0.tmp_0', 'layer_norm_10.b_0_beta1_pow_acc_0', 'linear_19.w_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_7.tmp_0', 'layer_norm_9.b_0@GRAD@MERGE', 'linear_34.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_5.w_0_fp32_master_0_moment2_0', 'find_infinite_scale.@fp32_0@cast_int32', 'layer_norm_16.w_0', 'linear_41.b_0_fp32_master_0', 'opt_tmp_33', 'linear_17.w_0@GRAD@MERGE', 'layer_norm_2.b_0_moment2_0', 'opt_opt_squared_l2_norm_144.tmp_0', 'layer_norm_10.b_0_moment2_0', 'opt_opt_squared_l2_norm_31.tmp_0', 'linear_1.b_0@GRAD@MERGE', 'linear_30.w_0@GRAD@MERGE', 'linear_45.w_0', 'linear_40.b_0_fp32_master_0_moment2_0', 'layer_norm_8.w_0_beta2_pow_acc_0', 'layer_norm_6.b_0_moment2_0', 'layer_norm_0.w_0_beta2_pow_acc_0', 'linear_22.w_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_36.tmp_0', 'linear_44.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_17.tmp_0', 'opt_opt_squared_l2_norm_84.tmp_0', 'opt_tmp_75', 'layer_norm_12.w_0', 'linear_47.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_11.w_0_moment1_0', 'linear_35.w_0_fp32_master_0_beta1_pow_acc_0', 'embedding_0.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_7.b_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_12.b_0', 'layer_norm_6.w_0_moment1_0', 'layer_norm_9.b_0_beta2_pow_acc_0', 'opt_opt_sum_1.tmp_0', 'linear_2.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_25.tmp_0', 'layer_norm_4.b_0@GRAD@MERGE', 'layer_norm_19.b_0_moment2_0', 'linear_23.w_0_fp32_master_0_moment1_0', 'linear_42.b_0@GRAD@MERGE', 'linear_10.w_0_fp32_master_0_moment2_0', 'layer_norm_12.b_0@GRAD@MERGE', 'layer_norm_20.w_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_126.tmp_0', 'linear_44.b_0_fp32_master_0', 'linear_45.w_0_fp32_master_0', 'linear_19.w_0@GRAD@MERGE', 'linear_12.b_0_fp32_master_0_moment2_0', 'linear_24.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_32', 'linear_13.b_0@GRAD@MERGE', 'layer_norm_20.w_0@GRAD@MERGE', 'linear_7.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_27.w_0', 'linear_27.b_0_fp32_master_0_moment2_0', 'layer_norm_23.w_0_beta1_pow_acc_0', 'layer_norm_11.b_0_moment1_0', 'layer_norm_5.w_0_moment2_0', 'linear_29.b_0_fp32_master_0_moment1_0', 'opt_tmp_85', 'opt_opt_squared_l2_norm_120.tmp_0', 'linear_2.w_0_fp32_master_0_moment1_0', 'layer_norm_23.b_0_beta2_pow_acc_0', 'opt_tmp_25', 'layer_norm_2.w_0@GRAD@MERGE', 'opt_tmp_78', 'layer_norm_12.b_0_beta2_pow_acc_0', 'linear_24.b_0_fp32_master_0_moment2_0', 'opt_tmp_31', 'layer_norm_5.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_77.tmp_0', 'linear_7.w_0_fp32_master_0_moment2_0', 'layer_norm_12.b_0', 'linear_19.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.b_0', 'linear_10.w_0_fp32_master_0', 'linear_15.b_0_fp32_master_0_moment2_0', 'linear_20.b_0_fp32_master_0', 'linear_34.w_0_fp32_master_0_moment1_0', 'linear_18.b_0_fp32_master_0_moment2_0', 'linear_21.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_110.tmp_0', 'layer_norm_8.w_0', 'linear_3.b_0_fp32_master_0_moment1_0', 'layer_norm_23.w_0_moment2_0', 'linear_43.w_0', 'layer_norm_1.w_0', 'linear_21.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_17.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_33.w_0@GRAD@MERGE', 'linear_20.w_0_fp32_master_0', 'linear_5.b_0', 'opt_tmp_58', 'linear_45.b_0@GRAD@MERGE', 'layer_norm_6.b_0', 'linear_18.b_0', 'layer_norm_2.w_0_moment1_0', 'linear_4.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_32.w_0_fp32_master_0', 'layer_norm_13.b_0_beta1_pow_acc_0', 'linear_28.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_17.w_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_5.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_3.b_0@GRAD@MERGE', 'linear_13.w_0', 'opt_tmp_50', 'layer_norm_22.b_0', 'layer_norm_10.b_0_moment1_0', 'linear_46.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_21.tmp_0', 'linear_10.b_0', 'linear_9.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_129.tmp_0', 'opt_opt_squared_l2_norm_90.tmp_0', 'linear_13.b_0_fp32_master_0_moment2_0', 'layer_norm_23.w_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_89.tmp_0', 'opt_opt_squared_l2_norm_128.tmp_0', 'opt_tmp_4', 'linear_25.w_0_fp32_master_0_moment2_0', 'linear_40.b_0_fp32_master_0', 'layer_norm_4.b_0_moment1_0', 'opt_opt_squared_l2_norm_14.tmp_0', 'linear_8.w_0_fp32_master_0_moment2_0', 'layer_norm_4.b_0_moment2_0', 'layer_norm_14.w_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_140.tmp_0', 'opt_opt_squared_l2_norm_76.tmp_0', 'linear_1.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_23.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_8.w_0_moment1_0', 'linear_36.w_0', 'linear_26.w_0_fp32_master_0_moment1_0', 'linear_38.b_0_fp32_master_0_moment2_0', 'layer_norm_18.b_0_moment1_0', 'linear_29.w_0_fp32_master_0_moment2_0', 'linear_1.b_0_fp32_master_0', 'linear_23.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_4.w_0_moment2_0', 'layer_norm_17.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_23.tmp_0', 'linear_39.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_5.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_9.b_0_moment1_0', 'linear_45.b_0_fp32_master_0', 'opt_opt_squared_l2_norm_24.tmp_0', 'linear_22.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_24.b_0_fp32_master_0', 'linear_34.w_0_fp32_master_0', 'linear_25.w_0@GRAD@MERGE', 'layer_norm_0.b_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_112.tmp_0', 'linear_22.w_0_fp32_master_0', 'opt_tmp_28', 'layer_norm_14.b_0_beta1_pow_acc_0', 'linear_27.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_29.w_0', 'linear_41.w_0_fp32_master_0_moment1_0', 'linear_34.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_20.w_0@GRAD@MERGE', 'layer_norm_16.w_0_beta1_pow_acc_0', 'linear_35.w_0_fp32_master_0', 'linear_16.w_0_fp32_master_0', 'layer_norm_5.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_42.tmp_0', 'layer_norm_8.b_0_moment1_0', 'linear_39.w_0', 'layer_norm_17.w_0_beta2_pow_acc_0', 'layer_norm_8.w_0_beta1_pow_acc_0', 'opt_tmp_57', 'layer_norm_0.b_0_moment1_0', 'opt_opt_squared_l2_norm_26.tmp_0', 'layer_norm_7.b_0_moment1_0', 'layer_norm_16.b_0@GRAD@MERGE', 'linear_19.b_0_fp32_master_0', 'linear_41.w_0@GRAD@MERGE', 'linear_12.b_0_fp32_master_0', 'layer_norm_22.w_0_moment2_0', 'layer_norm_10.b_0@GRAD@MERGE', 'linear_1.w_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_31.b_0', 'linear_42.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_6.w_0', 'embedding_1.w_0_fp32_master_0_moment2_0', 'linear_45.w_0@GRAD@MERGE', 'linear_14.w_0_fp32_master_0_moment2_0', 'linear_46.b_0', 'opt_opt_squared_l2_norm_113.tmp_0', 'linear_29.b_0_fp32_master_0', 'layer_norm_1.w_0_beta2_pow_acc_0', 'layer_norm_2.b_0_beta1_pow_acc_0', 'linear_39.b_0@GRAD@MERGE', 'linear_16.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_60', 'linear_33.b_0', 'linear_4.b_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_5.tmp_0', 'linear_41.b_0_fp32_master_0_moment1_0', 'opt_tmp_34', 'linear_9.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_74.tmp_0', 'opt_tmp_23', 'opt_opt_squared_l2_norm_134.tmp_0', 'layer_norm_21.w_0@GRAD@MERGE', 'linear_10.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_12.w_0_fp32_master_0', 'linear_13.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_43.w_0_fp32_master_0_moment1_0', 'linear_44.w_0_fp32_master_0', 'layer_norm_8.b_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_141.tmp_0', 'opt_tmp_80', 'linear_27.w_0_fp32_master_0', 'opt_tmp_72', 'linear_37.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_16.b_0@GRAD@MERGE', 'layer_norm_20.b_0@GRAD@MERGE', 'layer_norm_9.b_0_beta1_pow_acc_0', 'opt_tmp_48', 'linear_30.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_31.b_0_fp32_master_0', 'layer_norm_16.b_0_moment1_0', 'linear_28.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_45.b_0_fp32_master_0_moment2_0', 'linear_19.w_0', 'linear_35.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_44.b_0', 'linear_15.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_37.w_0@GRAD@MERGE', 'linear_28.w_0_fp32_master_0_moment1_0', 'linear_13.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_6.w_0_fp32_master_0', 'linear_27.w_0_fp32_master_0_moment2_0', 'opt_tmp_83', 'opt_tmp_24', 'linear_14.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_16.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_43.b_0', 'layer_norm_1.w_0_beta1_pow_acc_0', 'linear_19.b_0@GRAD@MERGE', 'linear_37.b_0', 'linear_39.b_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_33.tmp_0', 'layer_norm_8.b_0', 'linear_0.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_26.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_30.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_121.tmp_0', 'linear_8.b_0@GRAD@MERGE', 'linear_42.w_0_fp32_master_0_moment2_0', 'linear_44.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_12.b_0_moment2_0', 'layer_norm_18.b_0_beta2_pow_acc_0', 'linear_21.b_0', 'linear_37.w_0_fp32_master_0_moment2_0', 'linear_42.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_29.b_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_20.tmp_0', 'opt_opt_squared_l2_norm_109.tmp_0', 'linear_30.b_0_fp32_master_0_moment1_0', 'opt_tmp_11', 'opt_opt_squared_l2_norm_138.tmp_0', 'opt_tmp_47', 'layer_norm_19.w_0_moment2_0', 'layer_norm_1.b_0_moment2_0', 'layer_norm_7.w_0_moment2_0', 'linear_35.w_0_fp32_master_0_moment1_0', 'linear_40.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_10.b_0', 'linear_6.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_1.w_0_moment1_0', 'opt_tmp_52', 'layer_norm_5.b_0_moment1_0', 'opt_tmp_1', 'opt_tmp_45', 'linear_30.b_0_fp32_master_0_moment2_0', 'linear_38.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_19.b_0', 'linear_22.w_0', 'linear_24.b_0@GRAD@MERGE', 'linear_41.b_0_fp32_master_0_moment2_0', 'linear_46.w_0_fp32_master_0_moment2_0', 'linear_10.w_0', 'linear_24.b_0_fp32_master_0_moment1_0', 'layer_norm_12.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_52.tmp_0', 'linear_38.w_0_fp32_master_0_moment2_0', 'linear_24.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_22.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_62.tmp_0', 'layer_norm_10.w_0@GRAD@MERGE', 'layer_norm_5.b_0_moment2_0', 'linear_8.w_0_fp32_master_0', 'layer_norm_3.w_0', 'opt_tmp_62', 'linear_5.b_0_fp32_master_0', 'opt_opt_squared_l2_norm_51.tmp_0', 'opt_opt_squared_l2_norm_72.tmp_0', 'linear_12.b_0_fp32_master_0_moment1_0', 'num_bad_steps_0', 'opt_opt_sqrt_0.tmp_0', 'linear_27.w_0@GRAD@MERGE', 'opt_tmp_59', 'opt_opt_squared_l2_norm_58.tmp_0', 'linear_42.w_0', 'linear_44.b_0_fp32_master_0_moment1_0', 'linear_24.w_0@GRAD@MERGE', 'layer_norm_15.b_0@GRAD@MERGE', 'layer_norm_0.w_0@GRAD@MERGE', 'linear_46.w_0_fp32_master_0', 'linear_27.b_0_fp32_master_0', 'linear_43.w_0_fp32_master_0_moment2_0', 'layer_norm_18.w_0_moment2_0', 'opt_opt_squared_l2_norm_108.tmp_0', 'opt_tmp_71', 'linear_24.w_0_fp32_master_0_moment2_0', 'layer_norm_20.w_0', 'linear_37.b_0_fp32_master_0', 'linear_28.w_0@GRAD@MERGE', 'layer_norm_11.w_0_beta1_pow_acc_0', 'linear_38.b_0', 'linear_11.b_0_fp32_master_0_moment2_0', 'linear_18.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_14.w_0_fp32_master_0_moment1_0', 'linear_11.b_0_fp32_master_0_moment1_0', 'layer_norm_14.w_0_moment1_0', 'linear_46.b_0_fp32_master_0_moment2_0', 'linear_15.w_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_18.tmp_0', 'opt_opt_squared_l2_norm_19.tmp_0', 'linear_8.w_0', 'linear_4.w_0_fp32_master_0_moment1_0', 'layer_norm_17.b_0_beta1_pow_acc_0', 'linear_5.b_0_fp32_master_0_moment2_0', 'linear_7.b_0_fp32_master_0_moment1_0', 'linear_21.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_104.tmp_0', 'linear_21.b_0_fp32_master_0_moment1_0', 'layer_norm_22.b_0@GRAD@MERGE', 'opt_tmp_92', 'linear_2.w_0', 'layer_norm_23.w_0@GRAD@MERGE', 'layer_norm_5.b_0_beta2_pow_acc_0', 'linear_21.b_0_fp32_master_0_moment2_0', 'linear_0.b_0', 'opt_opt_squared_l2_norm_116.tmp_0', 'linear_18.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_47.tmp_0', 'linear_31.w_0_fp32_master_0_moment2_0', 'layer_norm_22.w_0', 'linear_11.b_0@GRAD@MERGE', 'linear_33.b_0_fp32_master_0_moment1_0', 'layer_norm_16.w_0_moment2_0', 'linear_39.w_0_fp32_master_0_moment1_0', 'opt_tmp_49', 'opt_tmp_42', 'linear_33.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_10.w_0@GRAD@MERGE', 'layer_norm_14.w_0_moment2_0', 'opt_tmp_87', 'opt_tmp_64', 'linear_29.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_17.b_0_moment1_0', 'layer_norm_1.b_0_beta1_pow_acc_0', 'layer_norm_0.b_0', 'linear_2.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_60.tmp_0', 'linear_40.b_0@GRAD@MERGE', 'linear_36.w_0@GRAD@MERGE', 'linear_31.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_37.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_15.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_20.b_0_beta1_pow_acc_0', 'linear_11.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_43.b_0@GRAD@MERGE', 'linear_9.w_0_fp32_master_0', 'linear_19.b_0_fp32_master_0_moment2_0', 'opt_tmp_65', 'layer_norm_17.b_0', 'linear_2.w_0_fp32_master_0_moment2_0', 'linear_37.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_35.w_0_fp32_master_0_moment2_0', 'layer_norm_0.w_0_moment1_0', 'opt_opt_squared_l2_norm_117.tmp_0', 'layer_norm_3.b_0_beta1_pow_acc_0', 'embedding_1.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_18.b_0_fp32_master_0', 'linear_46.b_0@GRAD@MERGE', 'linear_33.w_0_fp32_master_0_moment1_0', 'opt_tmp_40', 'linear_21.w_0_fp32_master_0_moment1_0', 'linear_28.b_0_fp32_master_0_beta2_pow_acc_0', 'embedding_0.w_0@GRAD@MERGE', 'linear_32.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_sum_2.tmp_0', 'opt_opt_squared_l2_norm_103.tmp_0', 'linear_45.w_0_fp32_master_0_moment1_0', 'linear_13.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_45.tmp_0', 'opt_opt_squared_l2_norm_61.tmp_0', 'linear_26.b_0@GRAD@MERGE', 'linear_39.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_7.b_0_fp32_master_0', 'linear_11.w_0', 'layer_norm_2.w_0_beta1_pow_acc_0', 'opt_tmp_89', 'linear_7.b_0_fp32_master_0_moment2_0', 'layer_norm_16.w_0@GRAD@MERGE', 'opt_tmp_35', 'layer_norm_10.w_0_moment1_0', 'linear_41.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_124.tmp_0', 'opt_opt_squared_l2_norm_6.tmp_0', 'opt_opt_squared_l2_norm_106.tmp_0', 'opt_opt_squared_l2_norm_101.tmp_0', 'linear_8.b_0_fp32_master_0_moment2_0', 'opt_tmp_10', 'linear_26.w_0_fp32_master_0', 'linear_9.w_0_fp32_master_0_moment1_0', 'layer_norm_5.w_0_beta1_pow_acc_0', 'opt_tmp_6', 'linear_32.w_0@GRAD@MERGE', 'layer_norm_23.b_0_moment2_0', 'linear_46.b_0_fp32_master_0_moment1_0', 'opt_tmp_38', 'layer_norm_15.b_0_beta2_pow_acc_0', 'linear_41.w_0_fp32_master_0_moment2_0', 'linear_23.b_0_fp32_master_0', 'linear_7.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_29.tmp_0', 'opt_opt_squared_l2_norm_127.tmp_0', 'linear_47.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_13.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_18.w_0', 'layer_norm_7.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_3.tmp_0', 'opt_opt_squared_l2_norm_35.tmp_0', 'linear_39.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_8.b_0_fp32_master_0_moment1_0', 'linear_19.w_0_fp32_master_0', 'linear_16.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_80.tmp_0', 'linear_21.b_0@GRAD@MERGE', 'linear_25.b_0_fp32_master_0_moment2_0', 'linear_0.b_0_fp32_master_0', 'layer_norm_14.b_0_beta2_pow_acc_0', 'linear_28.w_0_fp32_master_0', 'linear_42.b_0_fp32_master_0_moment1_0', 'linear_36.w_0_fp32_master_0_moment2_0', 'opt_tmp_93', 'linear_15.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_29', 'linear_34.w_0', 'linear_6.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_21.w_0_moment2_0', 'linear_47.b_0_fp32_master_0', 'linear_22.b_0@GRAD@MERGE', 'opt_tmp_15', 'linear_2.b_0', 'linear_11.w_0@GRAD@MERGE', 'linear_34.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_6.b_0@GRAD@MERGE', 'linear_23.b_0@GRAD@MERGE', 'opt_tmp_56', 'linear_40.b_0_fp32_master_0_moment1_0', 'layer_norm_6.w_0_moment2_0', 'find_infinite_scale.@fp32_0', 'layer_norm_10.w_0_beta1_pow_acc_0', 'linear_30.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_142.tmp_0', 'layer_norm_3.b_0', 'layer_norm_16.b_0_beta1_pow_acc_0', 'linear_43.w_0_fp32_master_0', 'linear_30.b_0', 'opt_opt_squared_l2_norm_32.tmp_0', 'opt_opt_squared_l2_norm_56.tmp_0', 'linear_33.w_0_fp32_master_0_moment2_0', 'layer_norm_11.b_0', 'opt_opt_squared_l2_norm_30.tmp_0', 'layer_norm_20.b_0_beta2_pow_acc_0', 'layer_norm_1.b_0@GRAD@MERGE', 'layer_norm_15.w_0_beta1_pow_acc_0', 'opt_tmp_94', 'linear_30.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_41.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_10.w_0', 'layer_norm_7.w_0', 'opt_opt_squared_l2_norm_100.tmp_0', 'opt_opt_squared_l2_norm_119.tmp_0', 'linear_3.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_2', 'linear_27.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_9.b_0', 'layer_norm_22.w_0_beta1_pow_acc_0', 'linear_43.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_45.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_15.tmp_0', 'linear_32.w_0', 'layer_norm_21.w_0_beta2_pow_acc_0', 'layer_norm_13.w_0@GRAD@MERGE', 'memcopy__0', 'linear_11.b_0_fp32_master_0_beta1_pow_acc_0', 'embedding_0.w_0_fp32_master_0_moment1_0', 'layer_norm_22.b_0_beta1_pow_acc_0', 'opt_tmp_86', 'layer_norm_7.w_0_beta2_pow_acc_0', 'opt_tmp_96', 'linear_11.w_0_fp32_master_0', 'linear_25.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_9.b_0', 'linear_23.w_0@GRAD@MERGE', 'linear_24.b_0', 'layer_norm_16.b_0_moment2_0', 'layer_norm_23.w_0', 'embedding_1.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_18.w_0_fp32_master_0_moment2_0', 'linear_35.w_0@GRAD@MERGE', 'layer_norm_1.b_0_beta2_pow_acc_0', 'linear_21.w_0@GRAD@MERGE', 'linear_2.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_3.b_0_moment1_0', 'layer_norm_23.b_0_moment1_0', 'layer_norm_13.w_0_moment1_0', 'linear_8.w_0@GRAD@MERGE', 'layer_norm_17.w_0@GRAD@MERGE', 'opt_tmp_16', 'opt_opt_squared_l2_norm_135.tmp_0', 'layer_norm_22.w_0_beta2_pow_acc_0', 'opt_tmp_98', 'linear_19.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_12.w_0_beta2_pow_acc_0', 'linear_12.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_32.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_11.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_13.tmp_0', 'linear_18.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_37.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_44', 'linear_17.b_0_fp32_master_0_moment2_0', 'opt_tmp_51', 'layer_norm_13.b_0', 'embedding_0.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_21.b_0_moment2_0', 'linear_21.w_0', 'opt_opt_squared_l2_norm_91.tmp_0', 'linear_47.b_0', 'linear_27.b_0@GRAD@MERGE', 'linear_44.w_0_fp32_master_0_moment1_0', 'opt_tmp_39', 'layer_norm_7.w_0@GRAD@MERGE', 'linear_9.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_1.tmp_0', 'layer_norm_1.w_0@GRAD@MERGE', 'opt_tmp_37', 'linear_19.b_0_fp32_master_0_moment1_0', 'opt_tmp_5', 'linear_3.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_85.tmp_0', 'linear_24.w_0_fp32_master_0_moment1_0', 'linear_6.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_86.tmp_0', 'layer_norm_15.w_0', 'opt_opt_squared_l2_norm_136.tmp_0', 'linear_47.w_0', 'linear_9.w_0', 'opt_opt_squared_l2_norm_55.tmp_0', 'opt_opt_squared_l2_norm_68.tmp_0', 'layer_norm_11.w_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_97.tmp_0', 'linear_41.w_0', 'layer_norm_15.b_0_moment1_0', 'layer_norm_14.w_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_0.tmp_0', 'opt_opt_squared_l2_norm_75.tmp_0', 'linear_45.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_7.w_0_moment1_0', 'linear_2.b_0_fp32_master_0_moment2_0', 'linear_39.b_0_fp32_master_0', 'opt_opt_sum_0.tmp_0', 'layer_norm_17.w_0', 'linear_29.b_0', 'opt_opt_squared_l2_norm_87.tmp_0', 'linear_17.w_0_fp32_master_0', 'layer_norm_2.w_0', 'linear_39.b_0', 'linear_32.w_0_fp32_master_0_moment2_0', 'layer_norm_5.w_0_beta2_pow_acc_0', 'layer_norm_18.w_0_moment1_0', 'linear_27.w_0_fp32_master_0_moment1_0', 'linear_33.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.w_0@GRAD@MERGE', 'linear_0.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_40.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_9', 'linear_14.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_40.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_3.w_0', 'linear_17.w_0', 'opt_opt_stack_2.tmp_0', 'opt_opt_squared_l2_norm_27.tmp_0', 'opt_opt_squared_l2_norm_37.tmp_0', 'linear_45.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_21.w_0_moment1_0', 'opt_opt_squared_l2_norm_96.tmp_0', 'linear_24.w_0', 'num_good_steps_0', 'opt_opt_squared_l2_norm_41.tmp_0', 'embedding_1.w_0_fp32_master_0', 'layer_norm_14.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_12.tmp_0', 'linear_25.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_8.w_0_moment2_0', 'opt_opt_squared_l2_norm_94.tmp_0', 'layer_norm_0.b_0@GRAD@MERGE', 'linear_4.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_18.w_0@GRAD@MERGE', 'linear_2.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_20.w_0_moment1_0', 'opt_tmp_54', 'linear_15.w_0', 'linear_1.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.b_0', 'linear_34.b_0_fp32_master_0', 'layer_norm_11.w_0@GRAD@MERGE', 'layer_norm_21.b_0_beta1_pow_acc_0', 'linear_22.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_23.b_0', 'opt_tmp_67', 'linear_25.b_0_fp32_master_0_moment1_0', 'linear_34.b_0', 'linear_21.w_0_fp32_master_0_moment2_0', 'linear_7.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_18', 'linear_36.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_84', 'linear_24.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_74', 'linear_20.b_0@GRAD@MERGE', 'layer_norm_19.b_0_moment1_0', 'linear_42.w_0_fp32_master_0', 'linear_46.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_25.w_0_fp32_master_0_moment1_0', 'linear_43.w_0@GRAD@MERGE', 'layer_norm_0.w_0', 'linear_42.w_0@GRAD@MERGE', 'layer_norm_1.w_0_moment2_0', 'linear_26.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_14.b_0', 'linear_42.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_20.b_0_moment1_0', 'layer_norm_14.b_0_moment2_0', 'linear_27.b_0', 'linear_13.w_0_fp32_master_0_moment2_0', 'layer_norm_12.w_0_beta1_pow_acc_0', 'linear_4.b_0', 'layer_norm_14.b_0@GRAD@MERGE', 'linear_16.w_0@GRAD@MERGE', 'layer_norm_13.b_0_beta2_pow_acc_0', 'linear_37.w_0', 'linear_20.b_0_fp32_master_0_moment2_0', 'layer_norm_5.w_0', 'linear_25.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_15.b_0', 'embedding_0.w_0', 'linear_33.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_16.w_0_moment1_0', 'linear_39.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_44.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_fill_constant_1.tmp_0', 'opt_tmp_7', 'linear_38.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_31.b_0_fp32_master_0_moment1_0', 'embedding_0.w_0_fp32_master_0', 'linear_15.w_0@GRAD@MERGE', 'linear_12.b_0@GRAD@MERGE', 'layer_norm_22.b_0_moment1_0', 'layer_norm_6.w_0@GRAD@MERGE', 'linear_29.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.b_0_fp32_master_0_beta1_pow_acc_0', 'concat.tmp_0', 'linear_11.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_13.b_0_moment1_0', 'linear_3.b_0_fp32_master_0_moment2_0', 'linear_6.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_73.tmp_0', 'opt_tmp_22', 'layer_norm_15.b_0_beta1_pow_acc_0', 'linear_41.b_0', 'opt_tmp_26', 'layer_norm_14.b_0_moment1_0', 'opt_tmp_27', 'opt_tmp_76', 'layer_norm_3.b_0@GRAD@MERGE', 'linear_17.b_0', 'layer_norm_3.w_0@GRAD@MERGE', 'linear_31.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_7.b_0_beta1_pow_acc_0', 'linear_2.b_0@GRAD@MERGE', 'linear_11.w_0_fp32_master_0_moment2_0', 'opt_tmp_17', 'opt_opt_squared_l2_norm_50.tmp_0', 'linear_46.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_9.w_0@GRAD@MERGE', 'layer_norm_3.w_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_4.tmp_0', 'opt_tmp_21', 'layer_norm_1.b_0_moment1_0', 'layer_norm_14.w_0', 'layer_norm_10.w_0_moment2_0', 'linear_47.w_0_fp32_master_0_moment2_0', 'layer_norm_13.b_0_moment2_0', 'layer_norm_18.b_0', 'linear_42.b_0', 'linear_19.w_0_fp32_master_0_moment2_0', 'layer_norm_19.w_0@GRAD@MERGE', 'linear_12.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_6.b_0', 'layer_norm_20.w_0_beta1_pow_acc_0', 'linear_35.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_15.w_0_moment2_0', 'linear_43.b_0_fp32_master_0', 'linear_18.w_0_fp32_master_0_moment1_0', 'linear_13.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_28.tmp_0', 'layer_norm_9.w_0_moment2_0', 'linear_35.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_29.b_0@GRAD@MERGE', 'layer_norm_22.b_0_beta2_pow_acc_0', 'linear_32.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_11.w_0_fp32_master_0_moment1_0', 'linear_41.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_15.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_23.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_28.w_0', 'opt_opt_squared_l2_norm_22.tmp_0', 'layer_norm_0.b_0_beta1_pow_acc_0', 'linear_31.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_6.w_0_beta2_pow_acc_0', 'linear_4.b_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0_moment2_0', 'layer_norm_2.b_0_moment1_0', 'linear_45.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_21.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_33.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.b_0@GRAD@MERGE', 'linear_30.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_31.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_19.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_107.tmp_0', 'linear_16.w_0_fp32_master_0_moment2_0', 'linear_26.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_28.b_0_fp32_master_0', 'linear_28.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_143.tmp_0', 'linear_16.b_0_fp32_master_0_moment1_0', 'linear_34.b_0@GRAD@MERGE', 'linear_0.w_0_fp32_master_0_moment1_0', 'linear_11.b_0_fp32_master_0', 'linear_32.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_6.w_0_beta1_pow_acc_0', 'linear_6.b_0_fp32_master_0', 'linear_0.b_0@GRAD@MERGE', 'linear_29.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_39.w_0@GRAD@MERGE', 'linear_36.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.b_0_fp32_master_0', 'layer_norm_10.w_0_beta2_pow_acc_0', 'layer_norm_17.w_0_beta1_pow_acc_0', 'linear_28.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_98.tmp_0', 'opt_tmp_36', 'layer_norm_2.w_0_moment2_0', 'opt_tmp_12', 'opt_tmp_55', 'linear_22.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_19.b_0@GRAD@MERGE', 'opt_tmp_14', 'linear_23.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_15.b_0', 'opt_opt_squared_l2_norm_40.tmp_0', 'linear_0.w_0_fp32_master_0_moment2_0', 'linear_9.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_137.tmp_0', 'linear_10.b_0@GRAD@MERGE', 'linear_34.w_0@GRAD@MERGE', 'linear_31.w_0', 'linear_16.w_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_71.tmp_0', 'linear_25.w_0_fp32_master_0', 'layer_norm_6.w_0', 'layer_norm_23.w_0_moment1_0', 'layer_norm_2.b_0@GRAD@MERGE', 'linear_0.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_18.w_0_beta1_pow_acc_0', 'linear_22.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_42.b_0_fp32_master_0_moment2_0', 'linear_29.w_0_fp32_master_0', 'linear_9.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_4.b_0_fp32_master_0', 'linear_20.b_0_fp32_master_0_moment1_0', 'linear_26.b_0_fp32_master_0', 'layer_norm_0.w_0_moment2_0', 'opt_opt_squared_l2_norm_115.tmp_0', 'linear_7.w_0@GRAD@MERGE', 'linear_29.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_1.b_0_fp32_master_0_moment2_0', 'linear_6.w_0_fp32_master_0_moment1_0', 'layer_norm_4.w_0_beta2_pow_acc_0', 'linear_25.b_0_fp32_master_0', 'layer_norm_4.w_0@GRAD@MERGE', 'linear_7.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_99.tmp_0', 'linear_26.w_0', 'opt_tmp_0', 'linear_39.w_0_fp32_master_0_moment2_0', 'layer_norm_4.w_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_8.tmp_0', 'linear_39.b_0_fp32_master_0_moment1_0', 'linear_10.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_78.tmp_0', 'linear_23.w_0_fp32_master_0_moment2_0', 'linear_37.b_0_fp32_master_0_moment2_0', 'layer_norm_11.w_0', 'linear_32.b_0_fp32_master_0_moment1_0', 'linear_7.w_0_fp32_master_0_moment1_0', 'linear_36.b_0_fp32_master_0', 'layer_norm_15.w_0_moment1_0', 'linear_36.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_39.tmp_0', 'linear_6.w_0@GRAD@MERGE', 'linear_14.w_0', 'layer_norm_20.b_0', 'opt_opt_squared_l2_norm_95.tmp_0', 'opt_opt_squared_l2_norm_66.tmp_0', 'linear_3.b_0', 'layer_norm_21.w_0', 'linear_30.w_0_fp32_master_0', 'linear_17.b_0@GRAD@MERGE', 'layer_norm_13.w_0', 'opt_opt_squared_l2_norm_118.tmp_0', 'linear_25.b_0@GRAD@MERGE', 'linear_47.b_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_83.tmp_0', 'linear_33.w_0', 'layer_norm_8.w_0@GRAD@MERGE', 'linear_47.w_0@GRAD@MERGE', 'layer_norm_9.w_0_beta2_pow_acc_0', 'layer_norm_11.b_0_beta1_pow_acc_0', 'linear_34.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_88.tmp_0', 'opt_opt_squared_l2_norm_59.tmp_0', 'linear_12.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_36.b_0@GRAD@MERGE', 'linear_3.w_0_fp32_master_0_moment2_0', 'find_infinite_scale.@fp16_0', 'linear_46.w_0_fp32_master_0_moment1_0', 'linear_40.w_0@GRAD@MERGE', 'linear_20.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_46.tmp_0', 'linear_5.w_0', 'linear_35.b_0@GRAD@MERGE', 'linear_14.b_0_fp32_master_0_moment1_0', 'opt_opt_squared_l2_norm_43.tmp_0', 'linear_26.w_0_fp32_master_0_moment2_0'], Scope=['_generated_var_0']} = conditional_block(inputs={Cond=['gradient_merge_cond'], Input=['linear_16.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_43.b_0', 'linear_44.w_0_fp32_master_0_moment1_0', 'layer_norm_1.w_0_beta1_pow_acc_0', 'layer_norm_7.w_0@GRAD@MERGE', 'linear_19.b_0@GRAD@MERGE', 'linear_9.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_37.b_0', 'linear_39.b_0_fp32_master_0_moment2_0', 'linear_3.w_0_fp32_master_0', 'layer_norm_1.w_0@GRAD@MERGE', 'layer_norm_8.b_0', 'linear_0.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_13.w_0_fp32_master_0_moment1_0', 'layer_norm_8.b_0_beta2_pow_acc_0', 'linear_14.b_0_fp32_master_0_moment2_0', 'linear_1.b_0_fp32_master_0_moment1_0', 'linear_26.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_19.b_0_fp32_master_0_moment1_0', 'layer_norm_19.b_0_beta1_pow_acc_0', 'linear_35.b_0', 'layer_norm_7.b_0', 'linear_19.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_30.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_22.w_0_fp32_master_0_moment2_0', 'linear_3.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_39.w_0_fp32_master_0', 'embedding_1.w_0_fp32_master_0_moment1_0', 'linear_24.w_0_fp32_master_0_moment1_0', 'linear_8.b_0@GRAD@MERGE', 'linear_6.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_9.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_42.w_0_fp32_master_0_moment2_0', 'linear_6.b_0_fp32_master_0_moment2_0', 'layer_norm_15.w_0', 'linear_8.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_19.w_0_moment1_0', 'linear_42.b_0_fp32_master_0', 'linear_31.w_0_fp32_master_0_moment1_0', 'linear_33.w_0_fp32_master_0', 'linear_44.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_33.b_0_fp32_master_0_moment2_0', 'layer_norm_12.b_0_moment2_0', 'linear_47.w_0', 'linear_8.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_18.b_0_beta2_pow_acc_0', 'linear_9.w_0', 'linear_14.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_34.w_0_fp32_master_0_moment2_0', 'linear_21.b_0', 'layer_norm_11.w_0_beta2_pow_acc_0', 'linear_37.w_0_fp32_master_0_moment2_0', 'linear_42.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_41.w_0', 'layer_norm_10.b_0_beta2_pow_acc_0', 'layer_norm_3.w_0_beta2_pow_acc_0', 'linear_8.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_12.b_0_beta1_pow_acc_0', 'layer_norm_0.w_0_beta1_pow_acc_0', 'layer_norm_15.b_0_moment1_0', 'layer_norm_14.w_0_beta2_pow_acc_0', 'linear_29.b_0_fp32_master_0_moment2_0', 'layer_norm_16.b_0_beta2_pow_acc_0', 'linear_5.w_0_fp32_master_0_moment1_0', 'linear_3.b_0_fp32_master_0', 'linear_23.b_0_fp32_master_0_moment1_0', 'linear_30.b_0_fp32_master_0_moment1_0', 'linear_10.b_0_fp32_master_0_moment2_0', 'linear_8.w_0_fp32_master_0_moment1_0', 'linear_16.b_0', 'linear_35.b_0_fp32_master_0_moment2_0', 'linear_45.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_17.b_0_moment2_0', 'layer_norm_7.w_0_moment1_0', 'linear_2.b_0_fp32_master_0_moment2_0', 'embedding_1.w_0@GRAD@MERGE', 'linear_39.b_0_fp32_master_0', 'linear_38.w_0', 'layer_norm_19.w_0_moment2_0', 'layer_norm_1.b_0_moment2_0', 'layer_norm_11.b_0_moment2_0', 'linear_20.w_0_fp32_master_0_moment1_0', 'linear_36.b_0_fp32_master_0_moment2_0', 'layer_norm_17.w_0', 'linear_9.b_0_fp32_master_0_moment2_0', 'linear_29.b_0', 'linear_17.w_0_fp32_master_0', 'layer_norm_21.b_0', 'layer_norm_23.b_0@GRAD@MERGE', 'layer_norm_21.b_0@GRAD@MERGE', 'linear_14.b_0_fp32_master_0', 'layer_norm_7.w_0_moment2_0', 'linear_40.w_0', 'layer_norm_9.b_0_moment2_0', 'linear_14.w_0_fp32_master_0', 'linear_35.w_0_fp32_master_0_moment1_0', 'layer_norm_2.w_0', 'linear_40.w_0_fp32_master_0_moment2_0', 'linear_40.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_17.w_0_moment1_0', 'layer_norm_10.b_0', 'linear_39.b_0', 'linear_15.w_0_fp32_master_0', 'linear_32.w_0_fp32_master_0_moment2_0', 'layer_norm_5.w_0_beta2_pow_acc_0', 'linear_6.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_19.b_0_beta2_pow_acc_0', 'layer_norm_1.w_0_moment1_0', 'layer_norm_5.b_0_moment1_0', 'linear_28.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_18.w_0_moment1_0', 'linear_5.w_0_fp32_master_0', 'linear_27.w_0_fp32_master_0_moment1_0', 'linear_33.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.w_0@GRAD@MERGE', 'linear_0.w_0_fp32_master_0_beta1_pow_acc_0', 'embedding_1.w_0', 'linear_15.b_0_fp32_master_0_moment1_0', 'linear_30.b_0_fp32_master_0_moment2_0', 'linear_22.b_0_fp32_master_0_moment2_0', 'linear_37.b_0_fp32_master_0_moment1_0', 'layer_norm_22.w_0@GRAD@MERGE', 'linear_22.w_0', 'linear_24.b_0@GRAD@MERGE', 'linear_38.w_0_fp32_master_0_moment1_0', 'linear_38.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_40.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_41.b_0_fp32_master_0_moment2_0', 'linear_46.w_0_fp32_master_0_moment2_0', 'linear_6.w_0_fp32_master_0_moment2_0', 'linear_10.w_0', 'linear_24.b_0_fp32_master_0_moment1_0', 'layer_norm_12.w_0_moment2_0', 'layer_norm_12.w_0@GRAD@MERGE', 'linear_14.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_40.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_3.w_0', 'linear_7.w_0_fp32_master_0', 'linear_38.w_0_fp32_master_0_moment2_0', 'linear_24.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_43.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_17.w_0', 'linear_44.w_0@GRAD@MERGE', 'linear_22.b_0_fp32_master_0_moment1_0', 'linear_1.b_0', 'layer_norm_11.b_0_beta2_pow_acc_0', 'linear_23.w_0', 'learning_rate_0', 'linear_45.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_21.w_0_moment1_0', 'linear_12.w_0@GRAD@MERGE', 'layer_norm_10.w_0@GRAD@MERGE', 'linear_7.b_0', 'linear_8.w_0_fp32_master_0', 'layer_norm_3.w_0', 'layer_norm_5.b_0_moment2_0', 'linear_24.w_0', 'linear_27.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_10.b_0_beta1_pow_acc_0', 'linear_6.w_0_fp32_master_0_beta2_pow_acc_0', 'num_good_steps_0', 'linear_8.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_11.b_0', 'linear_19.w_0_fp32_master_0_moment1_0', 'embedding_1.w_0_fp32_master_0', 'layer_norm_14.w_0@GRAD@MERGE', 'linear_13.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_9.b_0@GRAD@MERGE', 'layer_norm_5.b_0', 'linear_34.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_2.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_25.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_32.b_0_fp32_master_0_moment2_0', 'linear_5.w_0_fp32_master_0_moment2_0', 'linear_37.b_0@GRAD@MERGE', 'layer_norm_16.w_0', 'linear_1.w_0@GRAD@MERGE', 'layer_norm_18.w_0', 'linear_5.b_0_fp32_master_0', 'layer_norm_8.w_0_moment2_0', 'linear_41.b_0_fp32_master_0', 'linear_20.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_30.b_0@GRAD@MERGE', 'layer_norm_0.b_0@GRAD@MERGE', 'linear_4.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_18.w_0@GRAD@MERGE', 'linear_42.w_0_fp32_master_0_moment1_0', 'linear_2.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_17.w_0@GRAD@MERGE', 'layer_norm_2.b_0_moment2_0', 'linear_17.b_0_fp32_master_0', 'linear_36.b_0_fp32_master_0_moment1_0', 'layer_norm_10.b_0_moment2_0', 'linear_20.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_3.w_0_moment2_0', 'linear_1.b_0@GRAD@MERGE', 'linear_12.b_0_fp32_master_0_moment1_0', 'num_bad_steps_0', 'linear_30.w_0@GRAD@MERGE', 'linear_45.w_0', 'layer_norm_20.w_0_moment1_0', 'linear_27.w_0@GRAD@MERGE', 'linear_15.w_0', 'linear_40.b_0_fp32_master_0_moment2_0', 'layer_norm_8.w_0_beta2_pow_acc_0', 'linear_1.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_47.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_6.b_0_moment2_0', 'linear_42.w_0', 'linear_8.b_0', 'layer_norm_0.w_0_beta2_pow_acc_0', 'linear_34.b_0_fp32_master_0', 'layer_norm_21.w_0_beta1_pow_acc_0', 'layer_norm_4.b_0_beta2_pow_acc_0', 'linear_22.w_0_fp32_master_0_moment1_0', 'linear_44.b_0_fp32_master_0_moment1_0', 'linear_24.w_0@GRAD@MERGE', 'linear_30.b_0_fp32_master_0', 'layer_norm_11.w_0@GRAD@MERGE', 'linear_28.b_0_fp32_master_0_moment2_0', 'linear_35.w_0', 'linear_43.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_44.w_0_fp32_master_0_moment2_0', 'linear_17.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_14.w_0@GRAD@MERGE', 'layer_norm_15.b_0@GRAD@MERGE', 'layer_norm_21.b_0_beta1_pow_acc_0', 'linear_22.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_0.w_0@GRAD@MERGE', 'layer_norm_12.w_0', 'linear_38.b_0_fp32_master_0_moment1_0', 'linear_46.w_0_fp32_master_0', 'linear_27.b_0_fp32_master_0', 'linear_47.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_11.w_0_moment1_0', 'linear_43.w_0_fp32_master_0_moment2_0', 'linear_35.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_22.w_0_moment1_0', 'layer_norm_18.w_0_moment2_0', 'linear_33.b_0@GRAD@MERGE', 'linear_9.b_0_fp32_master_0', 'embedding_0.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_23.b_0', 'linear_46.b_0_fp32_master_0', 'linear_38.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_5.b_0@GRAD@MERGE', 'linear_21.b_0_fp32_master_0', 'linear_7.b_0@GRAD@MERGE', 'linear_24.w_0_fp32_master_0_moment2_0', 'layer_norm_6.b_0_beta1_pow_acc_0', 'layer_norm_15.w_0@GRAD@MERGE', 'layer_norm_20.w_0', 'linear_25.b_0_fp32_master_0_moment1_0', 'linear_37.b_0_fp32_master_0', 'linear_26.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_21.w_0_fp32_master_0_moment2_0', 'linear_28.w_0@GRAD@MERGE', 'linear_34.b_0', 'layer_norm_11.w_0_beta1_pow_acc_0', 'layer_norm_18.b_0_moment2_0', 'linear_3.w_0@GRAD@MERGE', 'linear_7.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_47.b_0_fp32_master_0_moment2_0', 'linear_12.b_0', 'layer_norm_6.w_0_moment1_0', 'linear_5.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_9.b_0_beta2_pow_acc_0', 'linear_38.b_0', 'layer_norm_19.w_0_beta1_pow_acc_0', 'linear_45.b_0', 'embedding_0.w_0_fp32_master_0_moment2_0', 'linear_11.b_0_fp32_master_0_moment2_0', 'linear_18.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_14.w_0_fp32_master_0_moment1_0', 'linear_36.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_2.w_0@GRAD@MERGE', 'linear_11.b_0_fp32_master_0_moment1_0', 'linear_24.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_14.w_0_moment1_0', 'linear_46.b_0_fp32_master_0_moment2_0', 'linear_15.w_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0', 'layer_norm_4.b_0@GRAD@MERGE', 'layer_norm_19.b_0_moment2_0', 'linear_32.b_0_fp32_master_0', 'linear_23.w_0_fp32_master_0_moment1_0', 'linear_20.b_0@GRAD@MERGE', 'linear_42.b_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'layer_norm_19.b_0_moment1_0', 'linear_10.w_0_fp32_master_0_moment2_0', 'linear_8.w_0', 'linear_4.w_0_fp32_master_0_moment1_0', 'layer_norm_2.w_0_beta2_pow_acc_0', 'linear_5.b_0_fp32_master_0_moment2_0', 'layer_norm_12.b_0@GRAD@MERGE', 'layer_norm_17.b_0_beta1_pow_acc_0', 'linear_42.w_0_fp32_master_0', 'layer_norm_20.w_0_beta2_pow_acc_0', 'linear_44.b_0_fp32_master_0_moment2_0', 'layer_norm_16.b_0', 'linear_7.b_0_fp32_master_0_moment1_0', 'linear_21.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_13.w_0_beta2_pow_acc_0', 'linear_44.b_0_fp32_master_0', 'linear_42.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_23.b_0_fp32_master_0_moment2_0', 'linear_45.w_0_fp32_master_0', 'linear_46.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_25.w_0_fp32_master_0_moment1_0', 'linear_43.w_0@GRAD@MERGE', 'linear_19.w_0@GRAD@MERGE', 'linear_21.b_0_fp32_master_0_moment1_0', 'layer_norm_22.b_0@GRAD@MERGE', 'layer_norm_0.w_0', 'linear_5.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_2.w_0', 'linear_4.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_42.w_0@GRAD@MERGE', 'layer_norm_1.w_0_moment2_0', 'linear_26.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_14.b_0', 'linear_42.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_20.b_0_moment1_0', 'layer_norm_14.b_0_moment2_0', 'layer_norm_23.w_0@GRAD@MERGE', 'linear_27.b_0', 'layer_norm_5.b_0_beta2_pow_acc_0', 'linear_13.w_0_fp32_master_0_moment2_0', 'linear_21.b_0_fp32_master_0_moment2_0', 'linear_29.w_0_fp32_master_0_moment1_0', 'linear_12.b_0_fp32_master_0_moment2_0', 'linear_0.b_0', 'linear_43.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.w_0_fp32_master_0', 'linear_24.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_15.w_0_fp32_master_0_moment2_0', 'linear_13.b_0@GRAD@MERGE', 'linear_18.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_20.w_0@GRAD@MERGE', 'linear_7.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_27.w_0', 'layer_norm_12.w_0_beta1_pow_acc_0', 'linear_4.b_0', 'linear_31.w_0_fp32_master_0_moment2_0', 'linear_27.b_0_fp32_master_0_moment2_0', 'layer_norm_5.w_0_moment1_0', 'layer_norm_14.b_0@GRAD@MERGE', 'layer_norm_3.w_0_moment1_0', 'linear_36.b_0', 'layer_norm_22.w_0', 'linear_11.b_0@GRAD@MERGE', 'linear_38.w_0_fp32_master_0', 'linear_11.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_23.w_0_beta1_pow_acc_0', 'linear_16.w_0@GRAD@MERGE', 'layer_norm_11.b_0_moment1_0', 'layer_norm_13.b_0_beta2_pow_acc_0', 'linear_34.b_0_fp32_master_0_moment1_0', 'linear_20.b_0_fp32_master_0_moment2_0', 'linear_37.w_0', 'layer_norm_5.w_0_moment2_0', 'linear_29.b_0_fp32_master_0_moment1_0', 'layer_norm_5.w_0', 'linear_33.b_0_fp32_master_0_moment1_0', 'layer_norm_16.w_0_moment2_0', 'linear_39.w_0_fp32_master_0_moment1_0', 'linear_25.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_15.b_0', 'linear_33.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_19.w_0', 'linear_17.w_0_fp32_master_0_moment2_0', 'linear_10.w_0@GRAD@MERGE', 'embedding_0.w_0', 'layer_norm_12.w_0_moment1_0', 'layer_norm_14.w_0_moment2_0', 'layer_norm_20.b_0_moment2_0', 'linear_33.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_16.w_0_moment1_0', 'linear_29.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_2.w_0_fp32_master_0_moment1_0', 'linear_10.b_0_fp32_master_0', 'linear_39.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_44.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_23.b_0_beta2_pow_acc_0', 'linear_12.w_0', 'layer_norm_4.w_0', 'linear_38.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_7.w_0', 'layer_norm_17.b_0_moment1_0', 'linear_31.b_0_fp32_master_0_moment1_0', 'layer_norm_2.w_0@GRAD@MERGE', 'linear_44.w_0', 'embedding_0.w_0_fp32_master_0', 'linear_15.w_0@GRAD@MERGE', 'linear_12.b_0@GRAD@MERGE', 'layer_norm_0.b_0', 'layer_norm_1.b_0_beta1_pow_acc_0', 'layer_norm_5.b_0_beta1_pow_acc_0', 'layer_norm_22.b_0_moment1_0', 'layer_norm_12.b_0_beta2_pow_acc_0', 'linear_2.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_6.w_0@GRAD@MERGE', 'linear_35.b_0_fp32_master_0_moment1_0', 'linear_40.b_0@GRAD@MERGE', 'linear_24.b_0_fp32_master_0_moment2_0', 'layer_norm_5.b_0@GRAD@MERGE', 'linear_7.w_0_fp32_master_0_moment2_0', 'linear_29.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_36.w_0@GRAD@MERGE', 'layer_norm_12.b_0', 'linear_19.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_31.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.b_0', 'linear_10.w_0_fp32_master_0', 'linear_15.b_0_fp32_master_0_moment2_0', 'linear_37.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_20.b_0_fp32_master_0', 'linear_15.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_34.w_0_fp32_master_0_moment1_0', 'linear_18.b_0_fp32_master_0_moment2_0', 'layer_norm_20.b_0_beta1_pow_acc_0', 'layer_norm_13.w_0_beta1_pow_acc_0', 'linear_11.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_21.w_0_fp32_master_0', 'linear_1.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_13.b_0_moment1_0', 'linear_11.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_3.b_0_fp32_master_0_moment2_0', 'layer_norm_8.w_0', 'linear_26.b_0_fp32_master_0_moment2_0', 'linear_6.b_0@GRAD@MERGE', 'linear_3.b_0_fp32_master_0_moment1_0', 'linear_24.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_23.w_0_moment2_0', 'linear_43.b_0@GRAD@MERGE', 'linear_27.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_43.w_0', 'layer_norm_1.w_0', 'linear_9.w_0_fp32_master_0', 'linear_21.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_21.b_0_moment1_0', 'linear_37.w_0_fp32_master_0', 'linear_19.b_0_fp32_master_0_moment2_0', 'layer_norm_15.b_0_beta1_pow_acc_0', 'linear_41.b_0', 'linear_17.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_33.w_0@GRAD@MERGE', 'layer_norm_17.w_0_moment2_0', 'layer_norm_17.b_0', 'linear_20.w_0_fp32_master_0', 'linear_2.w_0_fp32_master_0_moment2_0', 'linear_5.b_0', 'layer_norm_14.b_0_moment1_0', 'layer_norm_3.b_0@GRAD@MERGE', 'linear_45.b_0@GRAD@MERGE', 'layer_norm_4.b_0', 'linear_36.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_6.b_0', 'linear_37.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_9.w_0@GRAD@MERGE', 'linear_2.b_0_fp32_master_0_moment1_0', 'linear_19.b_0', 'linear_18.b_0', 'layer_norm_2.w_0_moment1_0', 'linear_33.b_0_fp32_master_0', 'linear_17.b_0', 'linear_35.w_0_fp32_master_0_moment2_0', 'layer_norm_0.w_0_moment1_0', 'linear_10.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_3.w_0@GRAD@MERGE', 'linear_4.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_32.w_0_fp32_master_0', 'layer_norm_13.b_0_beta1_pow_acc_0', 'linear_28.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_17.w_0_fp32_master_0_moment1_0', 'linear_38.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_10.w_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_18.b_0@GRAD@MERGE', 'linear_5.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_3.b_0@GRAD@MERGE', 'layer_norm_3.b_0_beta1_pow_acc_0', 'embedding_1.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0', 'layer_norm_21.b_0_beta2_pow_acc_0', 'layer_norm_7.w_0_beta1_pow_acc_0', 'linear_31.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_7.b_0_beta1_pow_acc_0', 'linear_2.b_0@GRAD@MERGE', 'linear_18.b_0_fp32_master_0', 'linear_46.b_0@GRAD@MERGE', 'linear_11.w_0_fp32_master_0_moment2_0', 'linear_18.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_1.b_0', 'linear_33.w_0_fp32_master_0_moment1_0', 'linear_46.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_46.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_22.b_0', 'layer_norm_23.b_0_beta1_pow_acc_0', 'layer_norm_9.w_0@GRAD@MERGE', 'linear_21.w_0_fp32_master_0_moment1_0', 'linear_28.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_10.b_0_moment1_0', 'linear_46.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_19.b_0', 'layer_norm_3.w_0_beta1_pow_acc_0', 'linear_17.w_0_fp32_master_0_beta1_pow_acc_0', 'embedding_0.w_0@GRAD@MERGE', 'linear_32.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_1.b_0_moment1_0', 'linear_12.w_0_fp32_master_0_moment1_0', 'linear_45.w_0_fp32_master_0_moment2_0', 'linear_36.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_45.w_0_fp32_master_0_moment1_0', 'layer_norm_14.w_0', 'linear_13.w_0@GRAD@MERGE', 'layer_norm_10.w_0_moment2_0', 'layer_norm_0.b_0_moment2_0', 'linear_16.b_0_fp32_master_0', 'linear_24.w_0_fp32_master_0', 'linear_47.w_0_fp32_master_0_moment2_0', 'linear_10.b_0', 'linear_25.b_0', 'layer_norm_9.w_0', 'linear_26.b_0@GRAD@MERGE', 'linear_39.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_9.b_0@GRAD@MERGE', 'layer_norm_13.b_0_moment2_0', 'layer_norm_18.b_0', 'linear_30.w_0_fp32_master_0_moment1_0', 'linear_15.b_0_fp32_master_0', 'layer_norm_6.b_0_moment1_0', 'linear_42.b_0', 'linear_13.b_0_fp32_master_0_moment2_0', 'linear_17.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_7.b_0_fp32_master_0', 'linear_3.w_0_fp32_master_0_moment1_0', 'layer_norm_23.w_0_beta2_pow_acc_0', 'linear_19.w_0_fp32_master_0_moment2_0', 'linear_11.w_0', 'layer_norm_19.w_0@GRAD@MERGE', 'layer_norm_2.w_0_beta1_pow_acc_0', 'linear_25.w_0_fp32_master_0_moment2_0', 'linear_40.b_0_fp32_master_0', 'layer_norm_4.b_0_moment1_0', 'linear_12.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_6.b_0', 'layer_norm_2.b_0', 'linear_36.w_0_fp32_master_0_moment1_0', 'linear_44.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_20.w_0_beta1_pow_acc_0', 'linear_22.w_0@GRAD@MERGE', 'linear_7.b_0_fp32_master_0_moment2_0', 'linear_35.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_16.w_0@GRAD@MERGE', 'linear_1.w_0', 'linear_8.w_0_fp32_master_0_moment2_0', 'layer_norm_15.w_0_moment2_0', 'layer_norm_10.w_0_moment1_0', 'layer_norm_13.w_0_moment2_0', 'linear_43.b_0_fp32_master_0', 'linear_18.w_0_fp32_master_0_moment1_0', 'linear_28.b_0', 'linear_41.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0_fp32_master_0', 'layer_norm_4.b_0_moment2_0', 'layer_norm_14.w_0_beta1_pow_acc_0', 'layer_norm_9.w_0_moment2_0', 'linear_35.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_38.w_0@GRAD@MERGE', 'linear_29.b_0@GRAD@MERGE', 'layer_norm_19.w_0_beta2_pow_acc_0', 'linear_1.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_8.b_0_fp32_master_0_moment2_0', 'layer_norm_22.b_0_beta2_pow_acc_0', 'linear_32.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_11.w_0_fp32_master_0_moment1_0', 'linear_41.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_15.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_26.w_0_fp32_master_0', 'linear_23.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_35.b_0_fp32_master_0', 'layer_norm_8.w_0_moment1_0', 'linear_23.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_28.w_0', 'linear_5.b_0_fp32_master_0_moment1_0', 'linear_9.w_0_fp32_master_0_moment1_0', 'layer_norm_12.b_0_moment1_0', 'layer_norm_5.w_0_beta1_pow_acc_0', 'layer_norm_0.b_0_beta1_pow_acc_0', 'linear_31.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_26.w_0_fp32_master_0_moment1_0', 'linear_36.w_0', 'linear_38.b_0_fp32_master_0_moment2_0', 'linear_32.w_0@GRAD@MERGE', 'layer_norm_18.w_0_beta2_pow_acc_0', 'layer_norm_18.b_0_moment1_0', 'layer_norm_23.b_0_moment2_0', 'linear_30.w_0', 'layer_norm_6.w_0_beta2_pow_acc_0', 'linear_22.b_0_fp32_master_0', 'linear_46.b_0_fp32_master_0_moment1_0', 'linear_18.w_0_fp32_master_0', 'layer_norm_15.b_0_beta2_pow_acc_0', 'linear_41.w_0_fp32_master_0_moment2_0', 'linear_18.b_0@GRAD@MERGE', 'linear_4.b_0_fp32_master_0_moment1_0', 'linear_20.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_23.b_0_fp32_master_0', 'layer_norm_16.w_0_beta2_pow_acc_0', 'linear_1.w_0_fp32_master_0_moment2_0', 'linear_29.w_0_fp32_master_0_moment2_0', 'linear_7.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_3.b_0_beta2_pow_acc_0', 'layer_norm_18.b_0_beta1_pow_acc_0', 'layer_norm_2.b_0_beta2_pow_acc_0', 'linear_1.b_0_fp32_master_0', 'layer_norm_2.b_0_moment1_0', 'linear_23.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_3.b_0_moment2_0', 'layer_norm_4.w_0_moment2_0', 'layer_norm_17.b_0@GRAD@MERGE', 'linear_45.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_39.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_21.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_47.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_13.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_15.b_0@GRAD@MERGE', 'linear_5.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_18.w_0', 'layer_norm_7.b_0@GRAD@MERGE', 'layer_norm_9.b_0_moment1_0', 'linear_21.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_33.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.b_0_fp32_master_0_moment1_0', 'linear_39.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_4.b_0@GRAD@MERGE', 'linear_45.b_0_fp32_master_0', 'linear_19.w_0_fp32_master_0', 'linear_31.b_0@GRAD@MERGE', 'linear_16.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_22.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_30.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_41.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_21.b_0@GRAD@MERGE', 'linear_24.b_0_fp32_master_0', 'linear_31.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_19.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_34.w_0_fp32_master_0', 'linear_16.w_0_fp32_master_0_moment2_0', 'linear_25.b_0_fp32_master_0_moment2_0', 'linear_25.w_0@GRAD@MERGE', 'layer_norm_0.b_0_beta2_pow_acc_0', 'linear_34.b_0_fp32_master_0_moment2_0', 'linear_26.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_22.w_0_fp32_master_0', 'linear_0.b_0_fp32_master_0', 'layer_norm_14.b_0_beta2_pow_acc_0', 'linear_28.w_0_fp32_master_0', 'linear_47.w_0_fp32_master_0_moment1_0', 'linear_28.b_0_fp32_master_0', 'linear_47.w_0_fp32_master_0', 'linear_28.b_0_fp32_master_0_moment1_0', 'linear_4.w_0', 'linear_42.b_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0_moment2_0', 'linear_36.w_0_fp32_master_0_moment2_0', 'linear_25.w_0', 'layer_norm_14.b_0_beta1_pow_acc_0', 'linear_16.b_0_fp32_master_0_moment1_0', 'linear_34.b_0@GRAD@MERGE', 'linear_0.w_0_fp32_master_0_moment1_0', 'linear_11.b_0_fp32_master_0', 'linear_27.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_32.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_29.w_0', 'linear_15.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_6.w_0_beta1_pow_acc_0', 'linear_34.w_0', 'linear_6.b_0_fp32_master_0', 'linear_6.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_8.b_0@GRAD@MERGE', 'linear_0.b_0@GRAD@MERGE', 'linear_2.b_0_fp32_master_0', 'linear_29.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_40.b_0', 'linear_41.w_0_fp32_master_0_moment1_0', 'linear_37.w_0_fp32_master_0_moment1_0', 'layer_norm_21.w_0_moment2_0', 'linear_3.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_34.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_39.w_0@GRAD@MERGE', 'linear_44.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_47.b_0_fp32_master_0', 'linear_20.w_0@GRAD@MERGE', 'linear_22.b_0@GRAD@MERGE', 'layer_norm_8.b_0_moment2_0', 'linear_38.b_0@GRAD@MERGE', 'layer_norm_16.w_0_beta1_pow_acc_0', 'linear_26.w_0_fp32_master_0_moment2_0', 'linear_35.w_0_fp32_master_0', 'linear_16.w_0_fp32_master_0', 'linear_36.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_2.b_0', 'layer_norm_5.w_0@GRAD@MERGE', 'linear_0.w_0@GRAD@MERGE', 'linear_8.b_0_fp32_master_0', 'layer_norm_10.w_0_beta2_pow_acc_0', 'layer_norm_17.w_0_beta1_pow_acc_0', 'linear_11.w_0@GRAD@MERGE', 'layer_norm_7.b_0_beta2_pow_acc_0', 'linear_28.b_0@GRAD@MERGE', 'layer_norm_8.b_0_moment1_0', 'layer_norm_6.b_0@GRAD@MERGE', 'linear_23.b_0@GRAD@MERGE', 'linear_34.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_40.b_0_fp32_master_0_moment1_0', 'linear_39.w_0', 'linear_20.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_6.w_0_moment2_0', 'layer_norm_17.w_0_beta2_pow_acc_0', 'linear_31.w_0@GRAD@MERGE', 'linear_16.b_0_fp32_master_0_moment2_0', 'layer_norm_2.w_0_moment2_0', 'linear_31.b_0_fp32_master_0_moment2_0', 'linear_14.b_0', 'layer_norm_8.w_0_beta1_pow_acc_0', 'layer_norm_10.w_0_beta1_pow_acc_0', 'linear_30.w_0_fp32_master_0_moment2_0', 'linear_22.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_19.b_0@GRAD@MERGE', 'layer_norm_0.b_0_moment1_0', 'layer_norm_4.b_0_beta1_pow_acc_0', 'linear_29.w_0@GRAD@MERGE', 'layer_norm_3.b_0', 'layer_norm_16.b_0_beta1_pow_acc_0', 'layer_norm_9.w_0_beta1_pow_acc_0', 'linear_43.w_0_fp32_master_0', 'linear_14.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_7.b_0_moment1_0', 'linear_23.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_16.b_0@GRAD@MERGE', 'linear_15.b_0', 'linear_30.b_0', 'linear_19.b_0_fp32_master_0', 'linear_0.w_0_fp32_master_0_moment2_0', 'linear_9.b_0_fp32_master_0_moment1_0', 'linear_10.b_0@GRAD@MERGE', 'linear_34.w_0@GRAD@MERGE', 'linear_31.w_0', 'linear_16.w_0_fp32_master_0_moment1_0', 'linear_41.w_0@GRAD@MERGE', 'linear_33.w_0_fp32_master_0_moment2_0', 'linear_12.b_0_fp32_master_0', 'layer_norm_22.w_0_moment2_0', 'linear_32.w_0_fp32_master_0_moment1_0', 'layer_norm_10.b_0@GRAD@MERGE', 'linear_41.b_0@GRAD@MERGE', 'linear_0.b_0_fp32_master_0_moment2_0', 'layer_norm_11.b_0', 'layer_norm_6.w_0', 'linear_25.w_0_fp32_master_0', 'layer_norm_4.w_0_moment1_0', 'layer_norm_15.b_0_moment2_0', 'layer_norm_2.b_0@GRAD@MERGE', 'linear_47.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_0.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_23.w_0_moment1_0', 'linear_17.b_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0_moment1_0', 'layer_norm_18.w_0_beta1_pow_acc_0', 'linear_22.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_42.b_0_fp32_master_0_moment2_0', 'layer_norm_1.b_0@GRAD@MERGE', 'layer_norm_20.b_0_beta2_pow_acc_0', 'linear_20.b_0', 'linear_12.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_29.w_0_fp32_master_0', 'linear_10.b_0_fp32_master_0_moment1_0', 'layer_norm_15.w_0_beta1_pow_acc_0', 'linear_9.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_31.b_0', 'linear_4.b_0_fp32_master_0', 'linear_6.w_0', 'linear_20.b_0_fp32_master_0_moment1_0', 'linear_26.b_0_fp32_master_0', 'linear_38.b_0_fp32_master_0', 'layer_norm_0.w_0_moment2_0', 'linear_42.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_30.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_41.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_7.w_0@GRAD@MERGE', 'linear_29.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_10.w_0', 'embedding_1.w_0_fp32_master_0_moment2_0', 'layer_norm_13.b_0@GRAD@MERGE', 'linear_45.w_0@GRAD@MERGE', 'linear_1.b_0_fp32_master_0_moment2_0', 'linear_14.w_0_fp32_master_0_moment2_0', 'layer_norm_7.w_0', 'linear_40.w_0_fp32_master_0', 'linear_6.w_0_fp32_master_0_moment1_0', 'linear_26.b_0', 'layer_norm_23.b_0', 'linear_3.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_40.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_46.b_0', 'layer_norm_4.w_0_beta2_pow_acc_0', 'linear_29.b_0_fp32_master_0', 'linear_47.b_0_fp32_master_0_moment1_0', 'layer_norm_1.w_0_beta2_pow_acc_0', 'layer_norm_4.w_0@GRAD@MERGE', 'linear_25.b_0_fp32_master_0', 'layer_norm_2.b_0_beta1_pow_acc_0', 'linear_45.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_22.b_0', 'linear_46.w_0', 'linear_39.b_0@GRAD@MERGE', 'linear_7.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_16.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.b_0_fp32_master_0_moment1_0', 'linear_26.w_0', 'linear_27.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_20.w_0_moment2_0', 'layer_norm_9.b_0', 'linear_33.b_0', 'linear_39.w_0_fp32_master_0_moment2_0', 'layer_norm_22.w_0_beta1_pow_acc_0', 'linear_4.b_0_fp32_master_0_moment2_0', 'linear_43.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_4.w_0_beta1_pow_acc_0', 'linear_45.b_0_fp32_master_0_moment1_0', 'linear_26.b_0_fp32_master_0_moment1_0', 'linear_28.w_0_fp32_master_0_moment2_0', 'layer_norm_6.b_0_beta2_pow_acc_0', 'linear_32.w_0', 'linear_39.b_0_fp32_master_0_moment1_0', 'linear_10.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_21.w_0_beta2_pow_acc_0', 'layer_norm_13.w_0@GRAD@MERGE', 'linear_23.w_0_fp32_master_0_moment2_0', 'linear_37.b_0_fp32_master_0_moment2_0', 'layer_norm_11.w_0', 'loss_scaling_0', 'linear_11.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_41.b_0_fp32_master_0_moment1_0', 'linear_32.b_0_fp32_master_0_moment1_0', 'embedding_0.w_0_fp32_master_0_moment1_0', 'layer_norm_22.b_0_beta1_pow_acc_0', 'linear_7.w_0_fp32_master_0_moment1_0', 'linear_36.b_0_fp32_master_0', 'linear_40.w_0_fp32_master_0_moment1_0', 'linear_4.w_0_fp32_master_0_moment2_0', 'linear_14.b_0@GRAD@MERGE', 'linear_20.w_0', 'linear_32.b_0@GRAD@MERGE', 'linear_16.w_0', 'layer_norm_15.w_0_moment1_0', 'linear_9.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_7.w_0_beta2_pow_acc_0', 'linear_36.w_0_fp32_master_0', 'linear_13.b_0_fp32_master_0', 'linear_9.w_0_fp32_master_0_moment2_0', 'layer_norm_17.b_0_beta2_pow_acc_0', 'linear_6.w_0@GRAD@MERGE', 'linear_14.w_0', 'linear_41.w_0_fp32_master_0', 'layer_norm_20.b_0', 'linear_3.b_0', 'layer_norm_11.w_0_moment2_0', 'linear_11.w_0_fp32_master_0', 'layer_norm_21.w_0', 'layer_norm_21.w_0@GRAD@MERGE', 'linear_30.w_0_fp32_master_0', 'linear_46.w_0@GRAD@MERGE', 'linear_17.b_0@GRAD@MERGE', 'layer_norm_7.b_0_moment2_0', 'linear_25.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_10.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_9.b_0', 'linear_23.w_0@GRAD@MERGE', 'linear_13.b_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0', 'linear_13.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_24.b_0', 'layer_norm_13.w_0', 'layer_norm_9.w_0_moment1_0', 'layer_norm_16.b_0_moment2_0', 'linear_43.w_0_fp32_master_0_moment1_0', 'linear_25.b_0@GRAD@MERGE', 'linear_44.w_0_fp32_master_0', 'linear_47.b_0@GRAD@MERGE', 'layer_norm_8.b_0_beta1_pow_acc_0', 'layer_norm_23.w_0', 'linear_27.w_0_fp32_master_0', 'linear_43.b_0_fp32_master_0_moment1_0', 'embedding_1.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_18.w_0_fp32_master_0_moment2_0', 'linear_35.w_0@GRAD@MERGE', 'linear_33.w_0', 'layer_norm_15.w_0_beta2_pow_acc_0', 'layer_norm_8.w_0@GRAD@MERGE', 'layer_norm_1.b_0_beta2_pow_acc_0', 'linear_3.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_37.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_21.w_0@GRAD@MERGE', 'linear_47.w_0@GRAD@MERGE', 'linear_2.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_3.b_0_moment1_0', 'layer_norm_23.b_0_moment1_0', 'layer_norm_9.w_0_beta2_pow_acc_0', 'layer_norm_13.w_0_moment1_0', 'linear_27.b_0@GRAD@MERGE', 'linear_16.b_0@GRAD@MERGE', 'layer_norm_11.b_0_beta1_pow_acc_0', 'linear_43.b_0_fp32_master_0_moment2_0', 'linear_31.w_0_fp32_master_0', 'linear_8.w_0@GRAD@MERGE', 'linear_26.w_0@GRAD@MERGE', 'layer_norm_17.w_0@GRAD@MERGE', 'layer_norm_20.b_0@GRAD@MERGE', 'layer_norm_9.b_0_beta1_pow_acc_0', 'linear_44.b_0@GRAD@MERGE', 'linear_34.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_32.b_0', 'linear_25.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_22.w_0_beta2_pow_acc_0', 'linear_30.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_19.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_31.b_0_fp32_master_0', 'layer_norm_18.w_0@GRAD@MERGE', 'layer_norm_12.w_0_beta2_pow_acc_0', 'layer_norm_16.b_0_moment1_0', 'linear_12.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_2.w_0_fp32_master_0', 'linear_36.b_0@GRAD@MERGE', 'linear_28.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_45.b_0_fp32_master_0_moment2_0', 'linear_19.w_0', 'linear_12.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_32.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_3.w_0_fp32_master_0_moment2_0', 'layer_norm_11.b_0@GRAD@MERGE', 'linear_35.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_44.b_0', 'layer_norm_22.b_0_moment2_0', 'linear_18.b_0_fp32_master_0_moment1_0', 'linear_15.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_18.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_46.w_0_fp32_master_0_moment1_0', 'linear_6.b_0_fp32_master_0_moment1_0', 'linear_40.w_0@GRAD@MERGE', 'linear_37.w_0@GRAD@MERGE', 'linear_0.w_0', 'linear_20.w_0_fp32_master_0_moment2_0', 'linear_17.b_0_fp32_master_0_moment2_0', 'linear_27.b_0_fp32_master_0_moment1_0', 'linear_28.w_0_fp32_master_0_moment1_0', 'linear_37.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_13.b_0', 'linear_5.w_0', 'linear_13.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_16.w_0_fp32_master_0_beta1_pow_acc_0', 'embedding_0.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_21.b_0_moment2_0', 'linear_6.w_0_fp32_master_0', 'linear_21.w_0', 'linear_27.w_0_fp32_master_0_moment2_0', 'linear_35.b_0@GRAD@MERGE', 'linear_14.b_0_fp32_master_0_moment1_0', 'linear_47.b_0', 'linear_10.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0_fp32_master_0', 'linear_14.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_23.w_0_fp32_master_0']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 2, op_role_var = [], sub_block = block[1], with_quant_attr = False)
}
{ // block_idx:1  parent_idx:0  forward_idx:-1  backward_idx:-1

    {Out=['embedding_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['embedding_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['embedding_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['embedding_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_0.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_0.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_1.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_1.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_2.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_2.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_3.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_3.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_4.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_4.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_5.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_5.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_6.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_6.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_13.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_7.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_7.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_14.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_14.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_14.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_15.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_15.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_15.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_8.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_8.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_16.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_16.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_16.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_17.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_17.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_17.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_9.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_9.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_18.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_18.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_18.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_19.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_19.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_19.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_10.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_10.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_20.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_20.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_20.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_21.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_21.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_21.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_11.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_11.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_22.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_22.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_22.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_23.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_23.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_23.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_12.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_12.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_24.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_24.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_24.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_25.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_25.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_25.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_13.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_13.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_26.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_26.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_26.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_27.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_27.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_27.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_14.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_14.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_14.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_14.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_28.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_28.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_28.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_28.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_29.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_29.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_29.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_29.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_15.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_15.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_15.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_15.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_30.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_30.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_30.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_30.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_31.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_31.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_31.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_31.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_16.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_16.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_16.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_16.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_32.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_32.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_32.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_32.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_33.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_33.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_33.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_33.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_17.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_17.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_17.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_17.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_34.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_34.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_34.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_34.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_35.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_35.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_35.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_35.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_18.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_18.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_18.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_18.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_36.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_36.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_36.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_36.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_37.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_37.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_37.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_37.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_19.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_19.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_19.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_19.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_38.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_38.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_38.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_38.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_39.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_39.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_39.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_39.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_20.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_20.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_20.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_20.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_40.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_40.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_40.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_40.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_41.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_41.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_41.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_41.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_21.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_21.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_21.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_21.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_42.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_42.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_42.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_42.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_43.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_43.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_43.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_43.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_22.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_22.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_22.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_22.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_44.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_44.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_44.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_44.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_45.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_45.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_45.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_45.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_23.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_23.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_23.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_23.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_46.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_46.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_46.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_46.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_47.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_47.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_47.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_47.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_47.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_47.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_47.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_47.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_47.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_47.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_47.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_47.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_47.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_47.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_47.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_47.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_46.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_46.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_46.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_46.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_46.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_46.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_46.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_46.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_46.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_46.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_46.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_46.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_23.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_23.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_23.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_23.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_23.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_23.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_23.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_23.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_23.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_23.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_23.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_23.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_45.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_45.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_45.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_45.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_45.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_45.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_45.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_45.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_45.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_45.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_45.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_45.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_44.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_44.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_44.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_44.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_44.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_44.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_44.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_44.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_44.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_44.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_44.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_44.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_22.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_22.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_22.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_22.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_22.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_22.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_22.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_22.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_22.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_22.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_22.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_22.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_43.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_43.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_43.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_43.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_43.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_43.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_43.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_43.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_43.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_43.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_43.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_43.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_42.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_42.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_42.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_42.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_42.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_42.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_42.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_42.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_42.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_42.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_42.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_42.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_21.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_21.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_21.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_21.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_21.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_21.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_21.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_21.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_21.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_21.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_21.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_21.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_41.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_41.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_41.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_41.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_41.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_41.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_41.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_41.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_41.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_41.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_41.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_41.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_40.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_40.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_40.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_40.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_40.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_40.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_40.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_40.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_40.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_40.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_40.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_40.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_20.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_20.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_20.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_20.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_20.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_20.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_20.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_20.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_20.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_20.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_20.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_20.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_39.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_39.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_39.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_39.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_39.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_39.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_39.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_39.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_39.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_39.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_39.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_39.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_38.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_38.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_38.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_38.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_38.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_38.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_38.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_38.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_38.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_38.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_38.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_38.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_19.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_19.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_19.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_19.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_19.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_19.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_19.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_19.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_19.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_19.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_19.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_19.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_37.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_37.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_37.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_37.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_37.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_37.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_37.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_37.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_37.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_37.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_37.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_37.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_36.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_36.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_36.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_36.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_36.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_36.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_36.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_36.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_36.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_36.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_36.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_36.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_18.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_18.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_18.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_18.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_18.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_18.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_18.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_18.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_18.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_18.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_18.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_18.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_35.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_35.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_35.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_35.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_35.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_35.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_35.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_35.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_35.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_35.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_35.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_35.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_34.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_34.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_34.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_34.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_34.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_34.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_34.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_34.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_34.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_34.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_34.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_34.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_17.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_17.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_17.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_17.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_17.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_17.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_17.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_17.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_17.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_17.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_17.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_17.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_33.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_33.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_33.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_33.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_33.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_33.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_33.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_33.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_33.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_33.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_33.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_33.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_32.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_32.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_32.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_32.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_32.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_32.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_32.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_32.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_32.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_32.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_32.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_32.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_16.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_16.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_16.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_16.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_16.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_16.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_16.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_16.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_16.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_16.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_16.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_16.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_31.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_31.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_31.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_31.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_31.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_31.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_31.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_31.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_31.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_31.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_31.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_31.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_30.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_30.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_30.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_30.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_30.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_30.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_30.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_30.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_30.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_30.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_30.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_30.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_15.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_15.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_15.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_15.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_15.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_15.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_15.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_15.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_15.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_15.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_15.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_15.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_29.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_29.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_29.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_29.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_29.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_29.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_29.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_29.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_29.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_29.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_29.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_29.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_28.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_28.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_28.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_28.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_28.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_28.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_28.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_28.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_28.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_28.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_28.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_28.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_14.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_14.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_14.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_14.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_14.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_14.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_14.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_14.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_14.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_14.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_14.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_14.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_27.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_27.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_27.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_27.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_27.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_27.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_27.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_27.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_27.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_26.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_26.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_26.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_26.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_26.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_26.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_26.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_26.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_26.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_13.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_13.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_13.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_13.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_13.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_13.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_13.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_13.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_25.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_25.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_25.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_25.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_25.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_25.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_25.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_25.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_25.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_24.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_24.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_24.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_24.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_24.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_24.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_24.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_24.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_24.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_12.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_12.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_12.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_12.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_12.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_12.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_12.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_12.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_23.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_23.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_23.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_23.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_23.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_23.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_23.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_23.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_23.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_22.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_22.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_22.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_22.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_22.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_22.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_22.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_22.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_22.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_11.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_11.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_11.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_11.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_11.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_11.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_11.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_11.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_21.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_21.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_21.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_21.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_21.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_21.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_21.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_21.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_21.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_20.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_20.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_20.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_20.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_20.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_20.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_20.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_20.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_20.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_10.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_10.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_10.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_10.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_10.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_10.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_10.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_10.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_19.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_19.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_19.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_19.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_19.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_19.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_19.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_19.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_19.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_18.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_18.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_18.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_18.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_18.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_18.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_18.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_18.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_18.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_9.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_9.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_9.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_9.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_9.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_9.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_9.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_9.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_17.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_17.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_17.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_17.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_17.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_17.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_17.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_17.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_17.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_16.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_16.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_16.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_16.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_16.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_16.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_16.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_16.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_16.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_8.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_8.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_8.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_8.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_8.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_8.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_8.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_8.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_15.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_15.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_15.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_15.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_15.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_15.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_15.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_15.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_15.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_14.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_14.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_14.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_14.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_14.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_14.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_14.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_14.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_14.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_7.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_7.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_7.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_7.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_7.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_7.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_7.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_7.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_13.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_13.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_13.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_13.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_13.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_12.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_12.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_12.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_6.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_6.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_6.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_6.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_6.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_6.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_6.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_6.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_11.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_11.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_11.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_10.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_10.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_10.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_5.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_5.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_5.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_5.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_5.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_5.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_5.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_5.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_9.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_9.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_9.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_8.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_8.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_8.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_4.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_4.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_4.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_4.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_4.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_4.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_4.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_4.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_7.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_7.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_7.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_6.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_6.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_6.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_3.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_3.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_3.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_3.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_3.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_3.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_3.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_5.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_5.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_5.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_4.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_4.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_4.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_2.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_2.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_2.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_2.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_2.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_2.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_2.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_3.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_3.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_2.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_2.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_1.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_1.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_1.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_1.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_1.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_1.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_1.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_1.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_1.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_0.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_0.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_0.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_0.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_0.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_0.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_0.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_0.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_0.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['embedding_1.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['embedding_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 30, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['embedding_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['embedding_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['embedding_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['embedding_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp32_0'], Out=['layer_norm_0.w_0@GRAD@MERGE', 'layer_norm_0.b_0@GRAD@MERGE', 'layer_norm_1.w_0@GRAD@MERGE', 'layer_norm_1.b_0@GRAD@MERGE', 'layer_norm_2.w_0@GRAD@MERGE', 'layer_norm_2.b_0@GRAD@MERGE', 'layer_norm_3.w_0@GRAD@MERGE', 'layer_norm_3.b_0@GRAD@MERGE', 'layer_norm_4.w_0@GRAD@MERGE', 'layer_norm_4.b_0@GRAD@MERGE', 'layer_norm_5.w_0@GRAD@MERGE', 'layer_norm_5.b_0@GRAD@MERGE', 'layer_norm_6.w_0@GRAD@MERGE', 'layer_norm_6.b_0@GRAD@MERGE', 'layer_norm_7.w_0@GRAD@MERGE', 'layer_norm_7.b_0@GRAD@MERGE', 'layer_norm_8.w_0@GRAD@MERGE', 'layer_norm_8.b_0@GRAD@MERGE', 'layer_norm_9.w_0@GRAD@MERGE', 'layer_norm_9.b_0@GRAD@MERGE', 'layer_norm_10.w_0@GRAD@MERGE', 'layer_norm_10.b_0@GRAD@MERGE', 'layer_norm_11.w_0@GRAD@MERGE', 'layer_norm_11.b_0@GRAD@MERGE', 'layer_norm_12.w_0@GRAD@MERGE', 'layer_norm_12.b_0@GRAD@MERGE', 'layer_norm_13.w_0@GRAD@MERGE', 'layer_norm_13.b_0@GRAD@MERGE', 'layer_norm_14.w_0@GRAD@MERGE', 'layer_norm_14.b_0@GRAD@MERGE', 'layer_norm_15.w_0@GRAD@MERGE', 'layer_norm_15.b_0@GRAD@MERGE', 'layer_norm_16.w_0@GRAD@MERGE', 'layer_norm_16.b_0@GRAD@MERGE', 'layer_norm_17.w_0@GRAD@MERGE', 'layer_norm_17.b_0@GRAD@MERGE', 'layer_norm_18.w_0@GRAD@MERGE', 'layer_norm_18.b_0@GRAD@MERGE', 'layer_norm_19.w_0@GRAD@MERGE', 'layer_norm_19.b_0@GRAD@MERGE', 'layer_norm_20.w_0@GRAD@MERGE', 'layer_norm_20.b_0@GRAD@MERGE', 'layer_norm_21.w_0@GRAD@MERGE', 'layer_norm_21.b_0@GRAD@MERGE', 'layer_norm_22.w_0@GRAD@MERGE', 'layer_norm_22.b_0@GRAD@MERGE', 'layer_norm_23.w_0@GRAD@MERGE', 'layer_norm_23.b_0@GRAD@MERGE']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['layer_norm_0.w_0@GRAD@MERGE', 'layer_norm_0.b_0@GRAD@MERGE', 'layer_norm_1.w_0@GRAD@MERGE', 'layer_norm_1.b_0@GRAD@MERGE', 'layer_norm_2.w_0@GRAD@MERGE', 'layer_norm_2.b_0@GRAD@MERGE', 'layer_norm_3.w_0@GRAD@MERGE', 'layer_norm_3.b_0@GRAD@MERGE', 'layer_norm_4.w_0@GRAD@MERGE', 'layer_norm_4.b_0@GRAD@MERGE', 'layer_norm_5.w_0@GRAD@MERGE', 'layer_norm_5.b_0@GRAD@MERGE', 'layer_norm_6.w_0@GRAD@MERGE', 'layer_norm_6.b_0@GRAD@MERGE', 'layer_norm_7.w_0@GRAD@MERGE', 'layer_norm_7.b_0@GRAD@MERGE', 'layer_norm_8.w_0@GRAD@MERGE', 'layer_norm_8.b_0@GRAD@MERGE', 'layer_norm_9.w_0@GRAD@MERGE', 'layer_norm_9.b_0@GRAD@MERGE', 'layer_norm_10.w_0@GRAD@MERGE', 'layer_norm_10.b_0@GRAD@MERGE', 'layer_norm_11.w_0@GRAD@MERGE', 'layer_norm_11.b_0@GRAD@MERGE', 'layer_norm_12.w_0@GRAD@MERGE', 'layer_norm_12.b_0@GRAD@MERGE', 'layer_norm_13.w_0@GRAD@MERGE', 'layer_norm_13.b_0@GRAD@MERGE', 'layer_norm_14.w_0@GRAD@MERGE', 'layer_norm_14.b_0@GRAD@MERGE', 'layer_norm_15.w_0@GRAD@MERGE', 'layer_norm_15.b_0@GRAD@MERGE', 'layer_norm_16.w_0@GRAD@MERGE', 'layer_norm_16.b_0@GRAD@MERGE', 'layer_norm_17.w_0@GRAD@MERGE', 'layer_norm_17.b_0@GRAD@MERGE', 'layer_norm_18.w_0@GRAD@MERGE', 'layer_norm_18.b_0@GRAD@MERGE', 'layer_norm_19.w_0@GRAD@MERGE', 'layer_norm_19.b_0@GRAD@MERGE', 'layer_norm_20.w_0@GRAD@MERGE', 'layer_norm_20.b_0@GRAD@MERGE', 'layer_norm_21.w_0@GRAD@MERGE', 'layer_norm_21.b_0@GRAD@MERGE', 'layer_norm_22.w_0@GRAD@MERGE', 'layer_norm_22.b_0@GRAD@MERGE', 'layer_norm_23.w_0@GRAD@MERGE', 'layer_norm_23.b_0@GRAD@MERGE']}, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp32_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp32_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp32_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp32_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, ring_id = 32, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp32_0']} = cast(inputs={X=['find_infinite_scale.@fp32_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, out_dtype = 0, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['embedding_0.w_0@GRAD@MERGE', 'embedding_1.w_0@GRAD@MERGE', 'linear_0.w_0@GRAD@MERGE', 'linear_0.b_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_1.b_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_2.b_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_3.b_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_4.b_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'linear_5.b_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_6.b_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_7.b_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_8.b_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_9.b_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_10.b_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_11.b_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'linear_12.b_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'linear_13.b_0@GRAD@MERGE', 'linear_14.w_0@GRAD@MERGE', 'linear_14.b_0@GRAD@MERGE', 'linear_15.w_0@GRAD@MERGE', 'linear_15.b_0@GRAD@MERGE', 'linear_16.w_0@GRAD@MERGE', 'linear_16.b_0@GRAD@MERGE', 'linear_17.w_0@GRAD@MERGE', 'linear_17.b_0@GRAD@MERGE', 'linear_18.w_0@GRAD@MERGE', 'linear_18.b_0@GRAD@MERGE', 'linear_19.w_0@GRAD@MERGE', 'linear_19.b_0@GRAD@MERGE', 'linear_20.w_0@GRAD@MERGE', 'linear_20.b_0@GRAD@MERGE', 'linear_21.w_0@GRAD@MERGE', 'linear_21.b_0@GRAD@MERGE', 'linear_22.w_0@GRAD@MERGE', 'linear_22.b_0@GRAD@MERGE', 'linear_23.w_0@GRAD@MERGE', 'linear_23.b_0@GRAD@MERGE', 'linear_24.w_0@GRAD@MERGE', 'linear_24.b_0@GRAD@MERGE', 'linear_25.w_0@GRAD@MERGE', 'linear_25.b_0@GRAD@MERGE', 'linear_26.w_0@GRAD@MERGE', 'linear_26.b_0@GRAD@MERGE', 'linear_27.w_0@GRAD@MERGE', 'linear_27.b_0@GRAD@MERGE', 'linear_28.w_0@GRAD@MERGE', 'linear_28.b_0@GRAD@MERGE', 'linear_29.w_0@GRAD@MERGE', 'linear_29.b_0@GRAD@MERGE', 'linear_30.w_0@GRAD@MERGE', 'linear_30.b_0@GRAD@MERGE', 'linear_31.w_0@GRAD@MERGE', 'linear_31.b_0@GRAD@MERGE', 'linear_32.w_0@GRAD@MERGE', 'linear_32.b_0@GRAD@MERGE', 'linear_33.w_0@GRAD@MERGE', 'linear_33.b_0@GRAD@MERGE', 'linear_34.w_0@GRAD@MERGE', 'linear_34.b_0@GRAD@MERGE', 'linear_35.w_0@GRAD@MERGE', 'linear_35.b_0@GRAD@MERGE', 'linear_36.w_0@GRAD@MERGE', 'linear_36.b_0@GRAD@MERGE', 'linear_37.w_0@GRAD@MERGE', 'linear_37.b_0@GRAD@MERGE', 'linear_38.w_0@GRAD@MERGE', 'linear_38.b_0@GRAD@MERGE', 'linear_39.w_0@GRAD@MERGE', 'linear_39.b_0@GRAD@MERGE', 'linear_40.w_0@GRAD@MERGE', 'linear_40.b_0@GRAD@MERGE', 'linear_41.w_0@GRAD@MERGE', 'linear_41.b_0@GRAD@MERGE', 'linear_42.w_0@GRAD@MERGE', 'linear_42.b_0@GRAD@MERGE', 'linear_43.w_0@GRAD@MERGE', 'linear_43.b_0@GRAD@MERGE', 'linear_44.w_0@GRAD@MERGE', 'linear_44.b_0@GRAD@MERGE', 'linear_45.w_0@GRAD@MERGE', 'linear_45.b_0@GRAD@MERGE', 'linear_46.w_0@GRAD@MERGE', 'linear_46.b_0@GRAD@MERGE', 'linear_47.w_0@GRAD@MERGE', 'linear_47.b_0@GRAD@MERGE']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['embedding_0.w_0@GRAD@MERGE', 'embedding_1.w_0@GRAD@MERGE', 'linear_0.w_0@GRAD@MERGE', 'linear_0.b_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_1.b_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_2.b_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_3.b_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_4.b_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'linear_5.b_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_6.b_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_7.b_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_8.b_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_9.b_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_10.b_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_11.b_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'linear_12.b_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'linear_13.b_0@GRAD@MERGE', 'linear_14.w_0@GRAD@MERGE', 'linear_14.b_0@GRAD@MERGE', 'linear_15.w_0@GRAD@MERGE', 'linear_15.b_0@GRAD@MERGE', 'linear_16.w_0@GRAD@MERGE', 'linear_16.b_0@GRAD@MERGE', 'linear_17.w_0@GRAD@MERGE', 'linear_17.b_0@GRAD@MERGE', 'linear_18.w_0@GRAD@MERGE', 'linear_18.b_0@GRAD@MERGE', 'linear_19.w_0@GRAD@MERGE', 'linear_19.b_0@GRAD@MERGE', 'linear_20.w_0@GRAD@MERGE', 'linear_20.b_0@GRAD@MERGE', 'linear_21.w_0@GRAD@MERGE', 'linear_21.b_0@GRAD@MERGE', 'linear_22.w_0@GRAD@MERGE', 'linear_22.b_0@GRAD@MERGE', 'linear_23.w_0@GRAD@MERGE', 'linear_23.b_0@GRAD@MERGE', 'linear_24.w_0@GRAD@MERGE', 'linear_24.b_0@GRAD@MERGE', 'linear_25.w_0@GRAD@MERGE', 'linear_25.b_0@GRAD@MERGE', 'linear_26.w_0@GRAD@MERGE', 'linear_26.b_0@GRAD@MERGE', 'linear_27.w_0@GRAD@MERGE', 'linear_27.b_0@GRAD@MERGE', 'linear_28.w_0@GRAD@MERGE', 'linear_28.b_0@GRAD@MERGE', 'linear_29.w_0@GRAD@MERGE', 'linear_29.b_0@GRAD@MERGE', 'linear_30.w_0@GRAD@MERGE', 'linear_30.b_0@GRAD@MERGE', 'linear_31.w_0@GRAD@MERGE', 'linear_31.b_0@GRAD@MERGE', 'linear_32.w_0@GRAD@MERGE', 'linear_32.b_0@GRAD@MERGE', 'linear_33.w_0@GRAD@MERGE', 'linear_33.b_0@GRAD@MERGE', 'linear_34.w_0@GRAD@MERGE', 'linear_34.b_0@GRAD@MERGE', 'linear_35.w_0@GRAD@MERGE', 'linear_35.b_0@GRAD@MERGE', 'linear_36.w_0@GRAD@MERGE', 'linear_36.b_0@GRAD@MERGE', 'linear_37.w_0@GRAD@MERGE', 'linear_37.b_0@GRAD@MERGE', 'linear_38.w_0@GRAD@MERGE', 'linear_38.b_0@GRAD@MERGE', 'linear_39.w_0@GRAD@MERGE', 'linear_39.b_0@GRAD@MERGE', 'linear_40.w_0@GRAD@MERGE', 'linear_40.b_0@GRAD@MERGE', 'linear_41.w_0@GRAD@MERGE', 'linear_41.b_0@GRAD@MERGE', 'linear_42.w_0@GRAD@MERGE', 'linear_42.b_0@GRAD@MERGE', 'linear_43.w_0@GRAD@MERGE', 'linear_43.b_0@GRAD@MERGE', 'linear_44.w_0@GRAD@MERGE', 'linear_44.b_0@GRAD@MERGE', 'linear_45.w_0@GRAD@MERGE', 'linear_45.b_0@GRAD@MERGE', 'linear_46.w_0@GRAD@MERGE', 'linear_46.b_0@GRAD@MERGE', 'linear_47.w_0@GRAD@MERGE', 'linear_47.b_0@GRAD@MERGE']}, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, ring_id = 32, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp32_0', 'find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['layer_norm_0.w_0@GRAD@MERGE', 'layer_norm_0.b_0@GRAD@MERGE', 'layer_norm_1.w_0@GRAD@MERGE', 'layer_norm_1.b_0@GRAD@MERGE', 'layer_norm_2.w_0@GRAD@MERGE', 'layer_norm_2.b_0@GRAD@MERGE', 'layer_norm_3.w_0@GRAD@MERGE', 'layer_norm_3.b_0@GRAD@MERGE', 'layer_norm_4.w_0@GRAD@MERGE', 'layer_norm_4.b_0@GRAD@MERGE', 'layer_norm_5.w_0@GRAD@MERGE', 'layer_norm_5.b_0@GRAD@MERGE', 'layer_norm_6.w_0@GRAD@MERGE', 'layer_norm_6.b_0@GRAD@MERGE', 'layer_norm_7.w_0@GRAD@MERGE', 'layer_norm_7.b_0@GRAD@MERGE', 'layer_norm_8.w_0@GRAD@MERGE', 'layer_norm_8.b_0@GRAD@MERGE', 'layer_norm_9.w_0@GRAD@MERGE', 'layer_norm_9.b_0@GRAD@MERGE', 'layer_norm_10.w_0@GRAD@MERGE', 'layer_norm_10.b_0@GRAD@MERGE', 'layer_norm_11.w_0@GRAD@MERGE', 'layer_norm_11.b_0@GRAD@MERGE', 'layer_norm_12.w_0@GRAD@MERGE', 'layer_norm_12.b_0@GRAD@MERGE', 'layer_norm_13.w_0@GRAD@MERGE', 'layer_norm_13.b_0@GRAD@MERGE', 'layer_norm_14.w_0@GRAD@MERGE', 'layer_norm_14.b_0@GRAD@MERGE', 'layer_norm_15.w_0@GRAD@MERGE', 'layer_norm_15.b_0@GRAD@MERGE', 'layer_norm_16.w_0@GRAD@MERGE', 'layer_norm_16.b_0@GRAD@MERGE', 'layer_norm_17.w_0@GRAD@MERGE', 'layer_norm_17.b_0@GRAD@MERGE', 'layer_norm_18.w_0@GRAD@MERGE', 'layer_norm_18.b_0@GRAD@MERGE', 'layer_norm_19.w_0@GRAD@MERGE', 'layer_norm_19.b_0@GRAD@MERGE', 'layer_norm_20.w_0@GRAD@MERGE', 'layer_norm_20.b_0@GRAD@MERGE', 'layer_norm_21.w_0@GRAD@MERGE', 'layer_norm_21.b_0@GRAD@MERGE', 'layer_norm_22.w_0@GRAD@MERGE', 'layer_norm_22.b_0@GRAD@MERGE', 'layer_norm_23.w_0@GRAD@MERGE', 'layer_norm_23.b_0@GRAD@MERGE'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['layer_norm_0.w_0@GRAD@MERGE', 'layer_norm_0.b_0@GRAD@MERGE', 'layer_norm_1.w_0@GRAD@MERGE', 'layer_norm_1.b_0@GRAD@MERGE', 'layer_norm_2.w_0@GRAD@MERGE', 'layer_norm_2.b_0@GRAD@MERGE', 'layer_norm_3.w_0@GRAD@MERGE', 'layer_norm_3.b_0@GRAD@MERGE', 'layer_norm_4.w_0@GRAD@MERGE', 'layer_norm_4.b_0@GRAD@MERGE', 'layer_norm_5.w_0@GRAD@MERGE', 'layer_norm_5.b_0@GRAD@MERGE', 'layer_norm_6.w_0@GRAD@MERGE', 'layer_norm_6.b_0@GRAD@MERGE', 'layer_norm_7.w_0@GRAD@MERGE', 'layer_norm_7.b_0@GRAD@MERGE', 'layer_norm_8.w_0@GRAD@MERGE', 'layer_norm_8.b_0@GRAD@MERGE', 'layer_norm_9.w_0@GRAD@MERGE', 'layer_norm_9.b_0@GRAD@MERGE', 'layer_norm_10.w_0@GRAD@MERGE', 'layer_norm_10.b_0@GRAD@MERGE', 'layer_norm_11.w_0@GRAD@MERGE', 'layer_norm_11.b_0@GRAD@MERGE', 'layer_norm_12.w_0@GRAD@MERGE', 'layer_norm_12.b_0@GRAD@MERGE', 'layer_norm_13.w_0@GRAD@MERGE', 'layer_norm_13.b_0@GRAD@MERGE', 'layer_norm_14.w_0@GRAD@MERGE', 'layer_norm_14.b_0@GRAD@MERGE', 'layer_norm_15.w_0@GRAD@MERGE', 'layer_norm_15.b_0@GRAD@MERGE', 'layer_norm_16.w_0@GRAD@MERGE', 'layer_norm_16.b_0@GRAD@MERGE', 'layer_norm_17.w_0@GRAD@MERGE', 'layer_norm_17.b_0@GRAD@MERGE', 'layer_norm_18.w_0@GRAD@MERGE', 'layer_norm_18.b_0@GRAD@MERGE', 'layer_norm_19.w_0@GRAD@MERGE', 'layer_norm_19.b_0@GRAD@MERGE', 'layer_norm_20.w_0@GRAD@MERGE', 'layer_norm_20.b_0@GRAD@MERGE', 'layer_norm_21.w_0@GRAD@MERGE', 'layer_norm_21.b_0@GRAD@MERGE', 'layer_norm_22.w_0@GRAD@MERGE', 'layer_norm_22.b_0@GRAD@MERGE', 'layer_norm_23.w_0@GRAD@MERGE', 'layer_norm_23.b_0@GRAD@MERGE']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, stop_update = False, with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['embedding_0.w_0@GRAD@MERGE', 'embedding_1.w_0@GRAD@MERGE', 'linear_0.w_0@GRAD@MERGE', 'linear_0.b_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_1.b_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_2.b_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_3.b_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_4.b_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'linear_5.b_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_6.b_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_7.b_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_8.b_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_9.b_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_10.b_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_11.b_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'linear_12.b_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'linear_13.b_0@GRAD@MERGE', 'linear_14.w_0@GRAD@MERGE', 'linear_14.b_0@GRAD@MERGE', 'linear_15.w_0@GRAD@MERGE', 'linear_15.b_0@GRAD@MERGE', 'linear_16.w_0@GRAD@MERGE', 'linear_16.b_0@GRAD@MERGE', 'linear_17.w_0@GRAD@MERGE', 'linear_17.b_0@GRAD@MERGE', 'linear_18.w_0@GRAD@MERGE', 'linear_18.b_0@GRAD@MERGE', 'linear_19.w_0@GRAD@MERGE', 'linear_19.b_0@GRAD@MERGE', 'linear_20.w_0@GRAD@MERGE', 'linear_20.b_0@GRAD@MERGE', 'linear_21.w_0@GRAD@MERGE', 'linear_21.b_0@GRAD@MERGE', 'linear_22.w_0@GRAD@MERGE', 'linear_22.b_0@GRAD@MERGE', 'linear_23.w_0@GRAD@MERGE', 'linear_23.b_0@GRAD@MERGE', 'linear_24.w_0@GRAD@MERGE', 'linear_24.b_0@GRAD@MERGE', 'linear_25.w_0@GRAD@MERGE', 'linear_25.b_0@GRAD@MERGE', 'linear_26.w_0@GRAD@MERGE', 'linear_26.b_0@GRAD@MERGE', 'linear_27.w_0@GRAD@MERGE', 'linear_27.b_0@GRAD@MERGE', 'linear_28.w_0@GRAD@MERGE', 'linear_28.b_0@GRAD@MERGE', 'linear_29.w_0@GRAD@MERGE', 'linear_29.b_0@GRAD@MERGE', 'linear_30.w_0@GRAD@MERGE', 'linear_30.b_0@GRAD@MERGE', 'linear_31.w_0@GRAD@MERGE', 'linear_31.b_0@GRAD@MERGE', 'linear_32.w_0@GRAD@MERGE', 'linear_32.b_0@GRAD@MERGE', 'linear_33.w_0@GRAD@MERGE', 'linear_33.b_0@GRAD@MERGE', 'linear_34.w_0@GRAD@MERGE', 'linear_34.b_0@GRAD@MERGE', 'linear_35.w_0@GRAD@MERGE', 'linear_35.b_0@GRAD@MERGE', 'linear_36.w_0@GRAD@MERGE', 'linear_36.b_0@GRAD@MERGE', 'linear_37.w_0@GRAD@MERGE', 'linear_37.b_0@GRAD@MERGE', 'linear_38.w_0@GRAD@MERGE', 'linear_38.b_0@GRAD@MERGE', 'linear_39.w_0@GRAD@MERGE', 'linear_39.b_0@GRAD@MERGE', 'linear_40.w_0@GRAD@MERGE', 'linear_40.b_0@GRAD@MERGE', 'linear_41.w_0@GRAD@MERGE', 'linear_41.b_0@GRAD@MERGE', 'linear_42.w_0@GRAD@MERGE', 'linear_42.b_0@GRAD@MERGE', 'linear_43.w_0@GRAD@MERGE', 'linear_43.b_0@GRAD@MERGE', 'linear_44.w_0@GRAD@MERGE', 'linear_44.b_0@GRAD@MERGE', 'linear_45.w_0@GRAD@MERGE', 'linear_45.b_0@GRAD@MERGE', 'linear_46.w_0@GRAD@MERGE', 'linear_46.b_0@GRAD@MERGE', 'linear_47.w_0@GRAD@MERGE', 'linear_47.b_0@GRAD@MERGE'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['embedding_0.w_0@GRAD@MERGE', 'embedding_1.w_0@GRAD@MERGE', 'linear_0.w_0@GRAD@MERGE', 'linear_0.b_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_1.b_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_2.b_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_3.b_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_4.b_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'linear_5.b_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_6.b_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_7.b_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_8.b_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_9.b_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_10.b_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_11.b_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'linear_12.b_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'linear_13.b_0@GRAD@MERGE', 'linear_14.w_0@GRAD@MERGE', 'linear_14.b_0@GRAD@MERGE', 'linear_15.w_0@GRAD@MERGE', 'linear_15.b_0@GRAD@MERGE', 'linear_16.w_0@GRAD@MERGE', 'linear_16.b_0@GRAD@MERGE', 'linear_17.w_0@GRAD@MERGE', 'linear_17.b_0@GRAD@MERGE', 'linear_18.w_0@GRAD@MERGE', 'linear_18.b_0@GRAD@MERGE', 'linear_19.w_0@GRAD@MERGE', 'linear_19.b_0@GRAD@MERGE', 'linear_20.w_0@GRAD@MERGE', 'linear_20.b_0@GRAD@MERGE', 'linear_21.w_0@GRAD@MERGE', 'linear_21.b_0@GRAD@MERGE', 'linear_22.w_0@GRAD@MERGE', 'linear_22.b_0@GRAD@MERGE', 'linear_23.w_0@GRAD@MERGE', 'linear_23.b_0@GRAD@MERGE', 'linear_24.w_0@GRAD@MERGE', 'linear_24.b_0@GRAD@MERGE', 'linear_25.w_0@GRAD@MERGE', 'linear_25.b_0@GRAD@MERGE', 'linear_26.w_0@GRAD@MERGE', 'linear_26.b_0@GRAD@MERGE', 'linear_27.w_0@GRAD@MERGE', 'linear_27.b_0@GRAD@MERGE', 'linear_28.w_0@GRAD@MERGE', 'linear_28.b_0@GRAD@MERGE', 'linear_29.w_0@GRAD@MERGE', 'linear_29.b_0@GRAD@MERGE', 'linear_30.w_0@GRAD@MERGE', 'linear_30.b_0@GRAD@MERGE', 'linear_31.w_0@GRAD@MERGE', 'linear_31.b_0@GRAD@MERGE', 'linear_32.w_0@GRAD@MERGE', 'linear_32.b_0@GRAD@MERGE', 'linear_33.w_0@GRAD@MERGE', 'linear_33.b_0@GRAD@MERGE', 'linear_34.w_0@GRAD@MERGE', 'linear_34.b_0@GRAD@MERGE', 'linear_35.w_0@GRAD@MERGE', 'linear_35.b_0@GRAD@MERGE', 'linear_36.w_0@GRAD@MERGE', 'linear_36.b_0@GRAD@MERGE', 'linear_37.w_0@GRAD@MERGE', 'linear_37.b_0@GRAD@MERGE', 'linear_38.w_0@GRAD@MERGE', 'linear_38.b_0@GRAD@MERGE', 'linear_39.w_0@GRAD@MERGE', 'linear_39.b_0@GRAD@MERGE', 'linear_40.w_0@GRAD@MERGE', 'linear_40.b_0@GRAD@MERGE', 'linear_41.w_0@GRAD@MERGE', 'linear_41.b_0@GRAD@MERGE', 'linear_42.w_0@GRAD@MERGE', 'linear_42.b_0@GRAD@MERGE', 'linear_43.w_0@GRAD@MERGE', 'linear_43.b_0@GRAD@MERGE', 'linear_44.w_0@GRAD@MERGE', 'linear_44.b_0@GRAD@MERGE', 'linear_45.w_0@GRAD@MERGE', 'linear_45.b_0@GRAD@MERGE', 'linear_46.w_0@GRAD@MERGE', 'linear_46.b_0@GRAD@MERGE', 'linear_47.w_0@GRAD@MERGE', 'linear_47.b_0@GRAD@MERGE']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_0.tmp_0']} = squared_l2_norm(inputs={X=['embedding_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_1.tmp_0']} = squared_l2_norm(inputs={X=['embedding_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_2.tmp_0']} = squared_l2_norm(inputs={X=['linear_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_3.tmp_0']} = squared_l2_norm(inputs={X=['linear_0.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_4.tmp_0']} = squared_l2_norm(inputs={X=['linear_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_5.tmp_0']} = squared_l2_norm(inputs={X=['linear_1.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_6.tmp_0']} = squared_l2_norm(inputs={X=['linear_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_7.tmp_0']} = squared_l2_norm(inputs={X=['linear_2.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_8.tmp_0']} = squared_l2_norm(inputs={X=['linear_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_9.tmp_0']} = squared_l2_norm(inputs={X=['linear_3.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_10.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_11.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_0.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_12.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_13.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_1.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_14.tmp_0']} = squared_l2_norm(inputs={X=['linear_4.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_15.tmp_0']} = squared_l2_norm(inputs={X=['linear_4.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_16.tmp_0']} = squared_l2_norm(inputs={X=['linear_5.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_17.tmp_0']} = squared_l2_norm(inputs={X=['linear_5.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_18.tmp_0']} = squared_l2_norm(inputs={X=['linear_6.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_19.tmp_0']} = squared_l2_norm(inputs={X=['linear_6.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_20.tmp_0']} = squared_l2_norm(inputs={X=['linear_7.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_21.tmp_0']} = squared_l2_norm(inputs={X=['linear_7.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_22.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_23.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_2.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_24.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_25.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_3.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_26.tmp_0']} = squared_l2_norm(inputs={X=['linear_8.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_27.tmp_0']} = squared_l2_norm(inputs={X=['linear_8.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_28.tmp_0']} = squared_l2_norm(inputs={X=['linear_9.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_29.tmp_0']} = squared_l2_norm(inputs={X=['linear_9.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_30.tmp_0']} = squared_l2_norm(inputs={X=['linear_10.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_31.tmp_0']} = squared_l2_norm(inputs={X=['linear_10.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_32.tmp_0']} = squared_l2_norm(inputs={X=['linear_11.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_33.tmp_0']} = squared_l2_norm(inputs={X=['linear_11.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_34.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_4.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_35.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_4.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_36.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_5.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_37.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_5.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_38.tmp_0']} = squared_l2_norm(inputs={X=['linear_12.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_39.tmp_0']} = squared_l2_norm(inputs={X=['linear_12.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_40.tmp_0']} = squared_l2_norm(inputs={X=['linear_13.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_41.tmp_0']} = squared_l2_norm(inputs={X=['linear_13.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_42.tmp_0']} = squared_l2_norm(inputs={X=['linear_14.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_43.tmp_0']} = squared_l2_norm(inputs={X=['linear_14.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_44.tmp_0']} = squared_l2_norm(inputs={X=['linear_15.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_45.tmp_0']} = squared_l2_norm(inputs={X=['linear_15.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_46.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_6.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_47.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_6.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_48.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_7.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_49.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_7.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_50.tmp_0']} = squared_l2_norm(inputs={X=['linear_16.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_51.tmp_0']} = squared_l2_norm(inputs={X=['linear_16.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_52.tmp_0']} = squared_l2_norm(inputs={X=['linear_17.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_53.tmp_0']} = squared_l2_norm(inputs={X=['linear_17.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_54.tmp_0']} = squared_l2_norm(inputs={X=['linear_18.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_55.tmp_0']} = squared_l2_norm(inputs={X=['linear_18.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_56.tmp_0']} = squared_l2_norm(inputs={X=['linear_19.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_57.tmp_0']} = squared_l2_norm(inputs={X=['linear_19.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_58.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_8.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_59.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_8.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_60.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_9.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_61.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_9.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_62.tmp_0']} = squared_l2_norm(inputs={X=['linear_20.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_63.tmp_0']} = squared_l2_norm(inputs={X=['linear_20.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_64.tmp_0']} = squared_l2_norm(inputs={X=['linear_21.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_65.tmp_0']} = squared_l2_norm(inputs={X=['linear_21.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_66.tmp_0']} = squared_l2_norm(inputs={X=['linear_22.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_67.tmp_0']} = squared_l2_norm(inputs={X=['linear_22.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_68.tmp_0']} = squared_l2_norm(inputs={X=['linear_23.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_69.tmp_0']} = squared_l2_norm(inputs={X=['linear_23.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_70.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_10.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_71.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_10.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_72.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_11.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_73.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_11.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_74.tmp_0']} = squared_l2_norm(inputs={X=['linear_24.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_75.tmp_0']} = squared_l2_norm(inputs={X=['linear_24.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_76.tmp_0']} = squared_l2_norm(inputs={X=['linear_25.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_77.tmp_0']} = squared_l2_norm(inputs={X=['linear_25.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_78.tmp_0']} = squared_l2_norm(inputs={X=['linear_26.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_79.tmp_0']} = squared_l2_norm(inputs={X=['linear_26.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_80.tmp_0']} = squared_l2_norm(inputs={X=['linear_27.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_81.tmp_0']} = squared_l2_norm(inputs={X=['linear_27.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_82.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_12.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_83.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_12.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_84.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_13.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_85.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_13.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_86.tmp_0']} = squared_l2_norm(inputs={X=['linear_28.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_87.tmp_0']} = squared_l2_norm(inputs={X=['linear_28.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_88.tmp_0']} = squared_l2_norm(inputs={X=['linear_29.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_89.tmp_0']} = squared_l2_norm(inputs={X=['linear_29.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_90.tmp_0']} = squared_l2_norm(inputs={X=['linear_30.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_91.tmp_0']} = squared_l2_norm(inputs={X=['linear_30.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_92.tmp_0']} = squared_l2_norm(inputs={X=['linear_31.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_93.tmp_0']} = squared_l2_norm(inputs={X=['linear_31.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_94.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_14.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_95.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_14.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_96.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_15.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_97.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_15.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_98.tmp_0']} = squared_l2_norm(inputs={X=['linear_32.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_99.tmp_0']} = squared_l2_norm(inputs={X=['linear_32.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_100.tmp_0']} = squared_l2_norm(inputs={X=['linear_33.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_101.tmp_0']} = squared_l2_norm(inputs={X=['linear_33.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_102.tmp_0']} = squared_l2_norm(inputs={X=['linear_34.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_103.tmp_0']} = squared_l2_norm(inputs={X=['linear_34.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_104.tmp_0']} = squared_l2_norm(inputs={X=['linear_35.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_105.tmp_0']} = squared_l2_norm(inputs={X=['linear_35.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_106.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_16.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_107.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_16.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_108.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_17.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_109.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_17.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_110.tmp_0']} = squared_l2_norm(inputs={X=['linear_36.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_111.tmp_0']} = squared_l2_norm(inputs={X=['linear_36.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_112.tmp_0']} = squared_l2_norm(inputs={X=['linear_37.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_113.tmp_0']} = squared_l2_norm(inputs={X=['linear_37.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_114.tmp_0']} = squared_l2_norm(inputs={X=['linear_38.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_115.tmp_0']} = squared_l2_norm(inputs={X=['linear_38.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_116.tmp_0']} = squared_l2_norm(inputs={X=['linear_39.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_117.tmp_0']} = squared_l2_norm(inputs={X=['linear_39.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_118.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_18.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_119.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_18.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_120.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_19.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_121.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_19.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_122.tmp_0']} = squared_l2_norm(inputs={X=['linear_40.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_123.tmp_0']} = squared_l2_norm(inputs={X=['linear_40.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_124.tmp_0']} = squared_l2_norm(inputs={X=['linear_41.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_125.tmp_0']} = squared_l2_norm(inputs={X=['linear_41.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_126.tmp_0']} = squared_l2_norm(inputs={X=['linear_42.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_127.tmp_0']} = squared_l2_norm(inputs={X=['linear_42.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_128.tmp_0']} = squared_l2_norm(inputs={X=['linear_43.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_129.tmp_0']} = squared_l2_norm(inputs={X=['linear_43.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_130.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_20.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_131.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_20.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_132.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_21.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_133.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_21.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_134.tmp_0']} = squared_l2_norm(inputs={X=['linear_44.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_135.tmp_0']} = squared_l2_norm(inputs={X=['linear_44.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_136.tmp_0']} = squared_l2_norm(inputs={X=['linear_45.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_137.tmp_0']} = squared_l2_norm(inputs={X=['linear_45.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_138.tmp_0']} = squared_l2_norm(inputs={X=['linear_46.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_139.tmp_0']} = squared_l2_norm(inputs={X=['linear_46.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_140.tmp_0']} = squared_l2_norm(inputs={X=['linear_47.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_141.tmp_0']} = squared_l2_norm(inputs={X=['linear_47.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_142.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_22.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_143.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_22.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_144.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_23.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_145.tmp_0']} = squared_l2_norm(inputs={X=['layer_norm_23.b_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Y=['opt_opt_stack_0.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_0.tmp_0', 'opt_opt_squared_l2_norm_1.tmp_0', 'opt_opt_squared_l2_norm_2.tmp_0', 'opt_opt_squared_l2_norm_3.tmp_0', 'opt_opt_squared_l2_norm_4.tmp_0', 'opt_opt_squared_l2_norm_5.tmp_0', 'opt_opt_squared_l2_norm_6.tmp_0', 'opt_opt_squared_l2_norm_7.tmp_0', 'opt_opt_squared_l2_norm_8.tmp_0', 'opt_opt_squared_l2_norm_9.tmp_0', 'opt_opt_squared_l2_norm_14.tmp_0', 'opt_opt_squared_l2_norm_15.tmp_0', 'opt_opt_squared_l2_norm_16.tmp_0', 'opt_opt_squared_l2_norm_17.tmp_0', 'opt_opt_squared_l2_norm_18.tmp_0', 'opt_opt_squared_l2_norm_19.tmp_0', 'opt_opt_squared_l2_norm_20.tmp_0', 'opt_opt_squared_l2_norm_21.tmp_0', 'opt_opt_squared_l2_norm_26.tmp_0', 'opt_opt_squared_l2_norm_27.tmp_0', 'opt_opt_squared_l2_norm_28.tmp_0', 'opt_opt_squared_l2_norm_29.tmp_0', 'opt_opt_squared_l2_norm_30.tmp_0', 'opt_opt_squared_l2_norm_31.tmp_0', 'opt_opt_squared_l2_norm_32.tmp_0', 'opt_opt_squared_l2_norm_33.tmp_0', 'opt_opt_squared_l2_norm_38.tmp_0', 'opt_opt_squared_l2_norm_39.tmp_0', 'opt_opt_squared_l2_norm_40.tmp_0', 'opt_opt_squared_l2_norm_41.tmp_0', 'opt_opt_squared_l2_norm_42.tmp_0', 'opt_opt_squared_l2_norm_43.tmp_0', 'opt_opt_squared_l2_norm_44.tmp_0', 'opt_opt_squared_l2_norm_45.tmp_0', 'opt_opt_squared_l2_norm_50.tmp_0', 'opt_opt_squared_l2_norm_51.tmp_0', 'opt_opt_squared_l2_norm_52.tmp_0', 'opt_opt_squared_l2_norm_53.tmp_0', 'opt_opt_squared_l2_norm_54.tmp_0', 'opt_opt_squared_l2_norm_55.tmp_0', 'opt_opt_squared_l2_norm_56.tmp_0', 'opt_opt_squared_l2_norm_57.tmp_0', 'opt_opt_squared_l2_norm_62.tmp_0', 'opt_opt_squared_l2_norm_63.tmp_0', 'opt_opt_squared_l2_norm_64.tmp_0', 'opt_opt_squared_l2_norm_65.tmp_0', 'opt_opt_squared_l2_norm_66.tmp_0', 'opt_opt_squared_l2_norm_67.tmp_0', 'opt_opt_squared_l2_norm_68.tmp_0', 'opt_opt_squared_l2_norm_69.tmp_0', 'opt_opt_squared_l2_norm_74.tmp_0', 'opt_opt_squared_l2_norm_75.tmp_0', 'opt_opt_squared_l2_norm_76.tmp_0', 'opt_opt_squared_l2_norm_77.tmp_0', 'opt_opt_squared_l2_norm_78.tmp_0', 'opt_opt_squared_l2_norm_79.tmp_0', 'opt_opt_squared_l2_norm_80.tmp_0', 'opt_opt_squared_l2_norm_81.tmp_0', 'opt_opt_squared_l2_norm_86.tmp_0', 'opt_opt_squared_l2_norm_87.tmp_0', 'opt_opt_squared_l2_norm_88.tmp_0', 'opt_opt_squared_l2_norm_89.tmp_0', 'opt_opt_squared_l2_norm_90.tmp_0', 'opt_opt_squared_l2_norm_91.tmp_0', 'opt_opt_squared_l2_norm_92.tmp_0', 'opt_opt_squared_l2_norm_93.tmp_0', 'opt_opt_squared_l2_norm_98.tmp_0', 'opt_opt_squared_l2_norm_99.tmp_0', 'opt_opt_squared_l2_norm_100.tmp_0', 'opt_opt_squared_l2_norm_101.tmp_0', 'opt_opt_squared_l2_norm_102.tmp_0', 'opt_opt_squared_l2_norm_103.tmp_0', 'opt_opt_squared_l2_norm_104.tmp_0', 'opt_opt_squared_l2_norm_105.tmp_0', 'opt_opt_squared_l2_norm_110.tmp_0', 'opt_opt_squared_l2_norm_111.tmp_0', 'opt_opt_squared_l2_norm_112.tmp_0', 'opt_opt_squared_l2_norm_113.tmp_0', 'opt_opt_squared_l2_norm_114.tmp_0', 'opt_opt_squared_l2_norm_115.tmp_0', 'opt_opt_squared_l2_norm_116.tmp_0', 'opt_opt_squared_l2_norm_117.tmp_0', 'opt_opt_squared_l2_norm_122.tmp_0', 'opt_opt_squared_l2_norm_123.tmp_0', 'opt_opt_squared_l2_norm_124.tmp_0', 'opt_opt_squared_l2_norm_125.tmp_0', 'opt_opt_squared_l2_norm_126.tmp_0', 'opt_opt_squared_l2_norm_127.tmp_0', 'opt_opt_squared_l2_norm_128.tmp_0', 'opt_opt_squared_l2_norm_129.tmp_0', 'opt_opt_squared_l2_norm_134.tmp_0', 'opt_opt_squared_l2_norm_135.tmp_0', 'opt_opt_squared_l2_norm_136.tmp_0', 'opt_opt_squared_l2_norm_137.tmp_0', 'opt_opt_squared_l2_norm_138.tmp_0', 'opt_opt_squared_l2_norm_139.tmp_0', 'opt_opt_squared_l2_norm_140.tmp_0', 'opt_opt_squared_l2_norm_141.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_sum_0.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_0.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_tmp_0']} = cast(inputs={X=['opt_opt_sum_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 5, with_quant_attr = False)
    {Y=['opt_opt_stack_1.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_10.tmp_0', 'opt_opt_squared_l2_norm_11.tmp_0', 'opt_opt_squared_l2_norm_12.tmp_0', 'opt_opt_squared_l2_norm_13.tmp_0', 'opt_opt_squared_l2_norm_22.tmp_0', 'opt_opt_squared_l2_norm_23.tmp_0', 'opt_opt_squared_l2_norm_24.tmp_0', 'opt_opt_squared_l2_norm_25.tmp_0', 'opt_opt_squared_l2_norm_34.tmp_0', 'opt_opt_squared_l2_norm_35.tmp_0', 'opt_opt_squared_l2_norm_36.tmp_0', 'opt_opt_squared_l2_norm_37.tmp_0', 'opt_opt_squared_l2_norm_46.tmp_0', 'opt_opt_squared_l2_norm_47.tmp_0', 'opt_opt_squared_l2_norm_48.tmp_0', 'opt_opt_squared_l2_norm_49.tmp_0', 'opt_opt_squared_l2_norm_58.tmp_0', 'opt_opt_squared_l2_norm_59.tmp_0', 'opt_opt_squared_l2_norm_60.tmp_0', 'opt_opt_squared_l2_norm_61.tmp_0', 'opt_opt_squared_l2_norm_70.tmp_0', 'opt_opt_squared_l2_norm_71.tmp_0', 'opt_opt_squared_l2_norm_72.tmp_0', 'opt_opt_squared_l2_norm_73.tmp_0', 'opt_opt_squared_l2_norm_82.tmp_0', 'opt_opt_squared_l2_norm_83.tmp_0', 'opt_opt_squared_l2_norm_84.tmp_0', 'opt_opt_squared_l2_norm_85.tmp_0', 'opt_opt_squared_l2_norm_94.tmp_0', 'opt_opt_squared_l2_norm_95.tmp_0', 'opt_opt_squared_l2_norm_96.tmp_0', 'opt_opt_squared_l2_norm_97.tmp_0', 'opt_opt_squared_l2_norm_106.tmp_0', 'opt_opt_squared_l2_norm_107.tmp_0', 'opt_opt_squared_l2_norm_108.tmp_0', 'opt_opt_squared_l2_norm_109.tmp_0', 'opt_opt_squared_l2_norm_118.tmp_0', 'opt_opt_squared_l2_norm_119.tmp_0', 'opt_opt_squared_l2_norm_120.tmp_0', 'opt_opt_squared_l2_norm_121.tmp_0', 'opt_opt_squared_l2_norm_130.tmp_0', 'opt_opt_squared_l2_norm_131.tmp_0', 'opt_opt_squared_l2_norm_132.tmp_0', 'opt_opt_squared_l2_norm_133.tmp_0', 'opt_opt_squared_l2_norm_142.tmp_0', 'opt_opt_squared_l2_norm_143.tmp_0', 'opt_opt_squared_l2_norm_144.tmp_0', 'opt_opt_squared_l2_norm_145.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_sum_1.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_1.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Y=['opt_opt_stack_2.tmp_0']} = stack(inputs={X=['opt_tmp_0', 'opt_opt_sum_1.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_sum_2.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_2.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_opt_sum_2.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['opt_opt_sum_2.tmp_0']}, op_device = , op_namescope = /auto_parallel/global_norm_synchronization, op_role = 2, ring_id = 0, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_opt_sum_2.tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_1']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['embedding_0.w_0@GRAD@MERGE'], Y=['opt_tmp_1']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_2']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['embedding_1.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['embedding_1.w_0@GRAD@MERGE'], Y=['opt_tmp_2']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_3']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_0.w_0@GRAD@MERGE'], Y=['opt_tmp_3']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_4']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_0.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_0.b_0@GRAD@MERGE'], Y=['opt_tmp_4']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_5']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_1.w_0@GRAD@MERGE'], Y=['opt_tmp_5']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_6']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_1.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_1.b_0@GRAD@MERGE'], Y=['opt_tmp_6']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_7']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_2.w_0@GRAD@MERGE'], Y=['opt_tmp_7']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_8']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_2.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_2.b_0@GRAD@MERGE'], Y=['opt_tmp_8']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_9']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_3.w_0@GRAD@MERGE'], Y=['opt_tmp_9']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_10']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_3.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_3.b_0@GRAD@MERGE'], Y=['opt_tmp_10']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_0.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_0.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_0.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_0.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_1.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_1.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_1.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_1.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_11']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_4.w_0@GRAD@MERGE'], Y=['opt_tmp_11']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_12']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_4.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_4.b_0@GRAD@MERGE'], Y=['opt_tmp_12']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_13']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_5.w_0@GRAD@MERGE'], Y=['opt_tmp_13']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_14']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_5.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_5.b_0@GRAD@MERGE'], Y=['opt_tmp_14']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_15']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_6.w_0@GRAD@MERGE'], Y=['opt_tmp_15']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_16']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_6.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_6.b_0@GRAD@MERGE'], Y=['opt_tmp_16']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_17']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_7.w_0@GRAD@MERGE'], Y=['opt_tmp_17']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_18']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_7.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_7.b_0@GRAD@MERGE'], Y=['opt_tmp_18']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_2.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_2.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_2.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_2.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_3.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_3.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_3.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_3.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_19']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_8.w_0@GRAD@MERGE'], Y=['opt_tmp_19']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_20']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_8.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_8.b_0@GRAD@MERGE'], Y=['opt_tmp_20']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_21']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_9.w_0@GRAD@MERGE'], Y=['opt_tmp_21']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_22']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_9.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_9.b_0@GRAD@MERGE'], Y=['opt_tmp_22']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_23']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_10.w_0@GRAD@MERGE'], Y=['opt_tmp_23']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_24']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_10.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_10.b_0@GRAD@MERGE'], Y=['opt_tmp_24']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_25']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_11.w_0@GRAD@MERGE'], Y=['opt_tmp_25']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_26']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_11.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_11.b_0@GRAD@MERGE'], Y=['opt_tmp_26']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_4.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_4.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_4.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_4.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_5.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_5.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_5.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_5.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_27']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_12.w_0@GRAD@MERGE'], Y=['opt_tmp_27']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_28']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_12.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_12.b_0@GRAD@MERGE'], Y=['opt_tmp_28']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_29']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_13.w_0@GRAD@MERGE'], Y=['opt_tmp_29']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_30']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_13.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_13.b_0@GRAD@MERGE'], Y=['opt_tmp_30']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_31']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_14.w_0@GRAD@MERGE'], Y=['opt_tmp_31']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_32']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_14.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_14.b_0@GRAD@MERGE'], Y=['opt_tmp_32']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_33']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_15.w_0@GRAD@MERGE'], Y=['opt_tmp_33']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_34']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_15.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_15.b_0@GRAD@MERGE'], Y=['opt_tmp_34']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_6.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_6.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_6.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_6.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_7.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_7.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_7.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_7.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_35']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_16.w_0@GRAD@MERGE'], Y=['opt_tmp_35']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_36']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_16.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_16.b_0@GRAD@MERGE'], Y=['opt_tmp_36']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_37']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_17.w_0@GRAD@MERGE'], Y=['opt_tmp_37']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_38']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_17.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_17.b_0@GRAD@MERGE'], Y=['opt_tmp_38']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_39']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_18.w_0@GRAD@MERGE'], Y=['opt_tmp_39']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_40']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_18.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_18.b_0@GRAD@MERGE'], Y=['opt_tmp_40']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_41']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_19.w_0@GRAD@MERGE'], Y=['opt_tmp_41']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_42']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_19.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_19.b_0@GRAD@MERGE'], Y=['opt_tmp_42']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_8.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_8.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_8.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_8.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_9.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_9.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_9.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_9.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_43']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_20.w_0@GRAD@MERGE'], Y=['opt_tmp_43']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_44']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_20.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_20.b_0@GRAD@MERGE'], Y=['opt_tmp_44']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_45']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_21.w_0@GRAD@MERGE'], Y=['opt_tmp_45']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_46']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_21.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_21.b_0@GRAD@MERGE'], Y=['opt_tmp_46']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_47']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_22.w_0@GRAD@MERGE'], Y=['opt_tmp_47']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_48']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_22.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_22.b_0@GRAD@MERGE'], Y=['opt_tmp_48']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_49']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_23.w_0@GRAD@MERGE'], Y=['opt_tmp_49']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_50']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_23.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_23.b_0@GRAD@MERGE'], Y=['opt_tmp_50']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_10.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_10.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_10.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_10.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_11.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_11.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_11.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_11.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_51']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_24.w_0@GRAD@MERGE'], Y=['opt_tmp_51']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_52']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_24.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_24.b_0@GRAD@MERGE'], Y=['opt_tmp_52']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_53']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_25.w_0@GRAD@MERGE'], Y=['opt_tmp_53']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_54']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_25.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_25.b_0@GRAD@MERGE'], Y=['opt_tmp_54']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_55']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_26.w_0@GRAD@MERGE'], Y=['opt_tmp_55']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_56']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_26.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_26.b_0@GRAD@MERGE'], Y=['opt_tmp_56']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_57']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_27.w_0@GRAD@MERGE'], Y=['opt_tmp_57']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_58']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_27.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_27.b_0@GRAD@MERGE'], Y=['opt_tmp_58']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_12.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_12.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_12.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_12.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_13.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_13.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_13.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_13.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_59']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_28.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_28.w_0@GRAD@MERGE'], Y=['opt_tmp_59']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_60']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_28.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_28.b_0@GRAD@MERGE'], Y=['opt_tmp_60']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_61']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_29.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_29.w_0@GRAD@MERGE'], Y=['opt_tmp_61']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_62']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_29.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_29.b_0@GRAD@MERGE'], Y=['opt_tmp_62']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_63']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_30.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_30.w_0@GRAD@MERGE'], Y=['opt_tmp_63']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_64']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_30.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_30.b_0@GRAD@MERGE'], Y=['opt_tmp_64']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_65']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_31.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_31.w_0@GRAD@MERGE'], Y=['opt_tmp_65']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_66']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_31.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_31.b_0@GRAD@MERGE'], Y=['opt_tmp_66']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_14.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_14.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_14.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_14.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_15.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_15.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_15.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_15.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_67']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_32.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_32.w_0@GRAD@MERGE'], Y=['opt_tmp_67']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_68']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_32.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_32.b_0@GRAD@MERGE'], Y=['opt_tmp_68']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_69']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_33.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_33.w_0@GRAD@MERGE'], Y=['opt_tmp_69']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_70']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_33.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_33.b_0@GRAD@MERGE'], Y=['opt_tmp_70']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_71']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_34.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_34.w_0@GRAD@MERGE'], Y=['opt_tmp_71']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_72']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_34.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_34.b_0@GRAD@MERGE'], Y=['opt_tmp_72']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_73']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_35.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_35.w_0@GRAD@MERGE'], Y=['opt_tmp_73']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_74']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_35.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_35.b_0@GRAD@MERGE'], Y=['opt_tmp_74']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_16.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_16.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_16.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_16.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_17.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_17.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_17.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_17.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_75']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_36.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_36.w_0@GRAD@MERGE'], Y=['opt_tmp_75']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_76']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_36.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_36.b_0@GRAD@MERGE'], Y=['opt_tmp_76']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_77']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_37.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_37.w_0@GRAD@MERGE'], Y=['opt_tmp_77']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_78']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_37.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_37.b_0@GRAD@MERGE'], Y=['opt_tmp_78']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_79']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_38.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_38.w_0@GRAD@MERGE'], Y=['opt_tmp_79']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_80']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_38.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_38.b_0@GRAD@MERGE'], Y=['opt_tmp_80']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_81']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_39.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_39.w_0@GRAD@MERGE'], Y=['opt_tmp_81']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_82']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_39.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_39.b_0@GRAD@MERGE'], Y=['opt_tmp_82']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_18.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_18.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_18.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_18.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_19.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_19.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_19.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_19.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_83']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_40.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_40.w_0@GRAD@MERGE'], Y=['opt_tmp_83']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_84']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_40.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_40.b_0@GRAD@MERGE'], Y=['opt_tmp_84']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_85']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_41.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_41.w_0@GRAD@MERGE'], Y=['opt_tmp_85']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_86']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_41.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_41.b_0@GRAD@MERGE'], Y=['opt_tmp_86']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_87']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_42.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_42.w_0@GRAD@MERGE'], Y=['opt_tmp_87']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_88']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_42.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_42.b_0@GRAD@MERGE'], Y=['opt_tmp_88']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_89']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_43.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_43.w_0@GRAD@MERGE'], Y=['opt_tmp_89']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_90']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_43.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_43.b_0@GRAD@MERGE'], Y=['opt_tmp_90']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_20.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_20.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_20.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_20.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_21.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_21.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_21.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_21.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_91']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_44.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_44.w_0@GRAD@MERGE'], Y=['opt_tmp_91']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_92']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_44.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_44.b_0@GRAD@MERGE'], Y=['opt_tmp_92']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_93']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_45.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_45.w_0@GRAD@MERGE'], Y=['opt_tmp_93']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_94']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_45.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_45.b_0@GRAD@MERGE'], Y=['opt_tmp_94']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_95']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_46.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_46.w_0@GRAD@MERGE'], Y=['opt_tmp_95']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_96']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_46.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_46.b_0@GRAD@MERGE'], Y=['opt_tmp_96']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_97']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_47.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_47.w_0@GRAD@MERGE'], Y=['opt_tmp_97']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_98']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_47.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_47.b_0@GRAD@MERGE'], Y=['opt_tmp_98']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_22.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_22.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_22.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_22.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_23.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_23.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_23.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_23.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Beta1PowOut=['embedding_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_0.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['embedding_0.w_0_fp32_master_0'], Moment1Out=['embedding_0.w_0_fp32_master_0_moment1_0'], Moment2Out=['embedding_0.w_0_fp32_master_0_moment2_0'], ParamOut=['embedding_0.w_0']} = adamw(inputs={Beta1Pow=['embedding_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_0.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['embedding_0.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['embedding_0.w_0_fp32_master_0'], Moment1=['embedding_0.w_0_fp32_master_0_moment1_0'], Moment2=['embedding_0.w_0_fp32_master_0_moment2_0'], Param=['embedding_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['embedding_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_1.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['embedding_1.w_0_fp32_master_0'], Moment1Out=['embedding_1.w_0_fp32_master_0_moment1_0'], Moment2Out=['embedding_1.w_0_fp32_master_0_moment2_0'], ParamOut=['embedding_1.w_0']} = adamw(inputs={Beta1Pow=['embedding_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_1.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['embedding_1.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['embedding_1.w_0_fp32_master_0'], Moment1=['embedding_1.w_0_fp32_master_0_moment1_0'], Moment2=['embedding_1.w_0_fp32_master_0_moment2_0'], Param=['embedding_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_1/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_0.w_0_fp32_master_0'], Moment1Out=['linear_0.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_0.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_0.w_0']} = adamw(inputs={Beta1Pow=['linear_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_0.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_0.w_0_fp32_master_0'], Moment1=['linear_0.w_0_fp32_master_0_moment1_0'], Moment2=['linear_0.w_0_fp32_master_0_moment2_0'], Param=['linear_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_2/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_0.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_0.b_0_fp32_master_0'], Moment1Out=['linear_0.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_0.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_0.b_0']} = adamw(inputs={Beta1Pow=['linear_0.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_0.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_0.b_0_fp32_master_0'], Moment1=['linear_0.b_0_fp32_master_0_moment1_0'], Moment2=['linear_0.b_0_fp32_master_0_moment2_0'], Param=['linear_0.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_3/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_1.w_0_fp32_master_0'], Moment1Out=['linear_1.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_1.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_1.w_0']} = adamw(inputs={Beta1Pow=['linear_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_1.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_1.w_0_fp32_master_0'], Moment1=['linear_1.w_0_fp32_master_0_moment1_0'], Moment2=['linear_1.w_0_fp32_master_0_moment2_0'], Param=['linear_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_4/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_1.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_1.b_0_fp32_master_0'], Moment1Out=['linear_1.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_1.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_1.b_0']} = adamw(inputs={Beta1Pow=['linear_1.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_1.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_1.b_0_fp32_master_0'], Moment1=['linear_1.b_0_fp32_master_0_moment1_0'], Moment2=['linear_1.b_0_fp32_master_0_moment2_0'], Param=['linear_1.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_5/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_2.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_2.w_0_fp32_master_0'], Moment1Out=['linear_2.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_2.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_2.w_0']} = adamw(inputs={Beta1Pow=['linear_2.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_2.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_2.w_0_fp32_master_0'], Moment1=['linear_2.w_0_fp32_master_0_moment1_0'], Moment2=['linear_2.w_0_fp32_master_0_moment2_0'], Param=['linear_2.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_6/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_2.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_2.b_0_fp32_master_0'], Moment1Out=['linear_2.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_2.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_2.b_0']} = adamw(inputs={Beta1Pow=['linear_2.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_2.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_2.b_0_fp32_master_0'], Moment1=['linear_2.b_0_fp32_master_0_moment1_0'], Moment2=['linear_2.b_0_fp32_master_0_moment2_0'], Param=['linear_2.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_7/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_3.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_3.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_3.w_0_fp32_master_0'], Moment1Out=['linear_3.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_3.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_3.w_0']} = adamw(inputs={Beta1Pow=['linear_3.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_3.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_3.w_0_fp32_master_0'], Moment1=['linear_3.w_0_fp32_master_0_moment1_0'], Moment2=['linear_3.w_0_fp32_master_0_moment2_0'], Param=['linear_3.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_8/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_3.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_3.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_3.b_0_fp32_master_0'], Moment1Out=['linear_3.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_3.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_3.b_0']} = adamw(inputs={Beta1Pow=['linear_3.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_3.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_3.b_0_fp32_master_0'], Moment1=['linear_3.b_0_fp32_master_0_moment1_0'], Moment2=['linear_3.b_0_fp32_master_0_moment2_0'], Param=['linear_3.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_9/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.w_0_moment1_0'], Moment2Out=['layer_norm_0.w_0_moment2_0'], ParamOut=['layer_norm_0.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_0.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.w_0_moment1_0'], Moment2=['layer_norm_0.w_0_moment2_0'], Param=['layer_norm_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_10/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_0.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_0.b_0_moment1_0'], Moment2Out=['layer_norm_0.b_0_moment2_0'], ParamOut=['layer_norm_0.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_0.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_0.b_0_moment1_0'], Moment2=['layer_norm_0.b_0_moment2_0'], Param=['layer_norm_0.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_11/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_1.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_1.w_0_moment1_0'], Moment2Out=['layer_norm_1.w_0_moment2_0'], ParamOut=['layer_norm_1.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_1.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_1.w_0_moment1_0'], Moment2=['layer_norm_1.w_0_moment2_0'], Param=['layer_norm_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_12/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_1.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_1.b_0_moment1_0'], Moment2Out=['layer_norm_1.b_0_moment2_0'], ParamOut=['layer_norm_1.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_1.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_1.b_0_moment1_0'], Moment2=['layer_norm_1.b_0_moment2_0'], Param=['layer_norm_1.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_13/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_4.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_4.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_4.w_0_fp32_master_0'], Moment1Out=['linear_4.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_4.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_4.w_0']} = adamw(inputs={Beta1Pow=['linear_4.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_4.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_4.w_0_fp32_master_0'], Moment1=['linear_4.w_0_fp32_master_0_moment1_0'], Moment2=['linear_4.w_0_fp32_master_0_moment2_0'], Param=['linear_4.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_14/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_4.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_4.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_4.b_0_fp32_master_0'], Moment1Out=['linear_4.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_4.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_4.b_0']} = adamw(inputs={Beta1Pow=['linear_4.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_4.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_4.b_0_fp32_master_0'], Moment1=['linear_4.b_0_fp32_master_0_moment1_0'], Moment2=['linear_4.b_0_fp32_master_0_moment2_0'], Param=['linear_4.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_15/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_5.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_5.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_5.w_0_fp32_master_0'], Moment1Out=['linear_5.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_5.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_5.w_0']} = adamw(inputs={Beta1Pow=['linear_5.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_5.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_5.w_0_fp32_master_0'], Moment1=['linear_5.w_0_fp32_master_0_moment1_0'], Moment2=['linear_5.w_0_fp32_master_0_moment2_0'], Param=['linear_5.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_16/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_5.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_5.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_5.b_0_fp32_master_0'], Moment1Out=['linear_5.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_5.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_5.b_0']} = adamw(inputs={Beta1Pow=['linear_5.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_5.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_5.b_0_fp32_master_0'], Moment1=['linear_5.b_0_fp32_master_0_moment1_0'], Moment2=['linear_5.b_0_fp32_master_0_moment2_0'], Param=['linear_5.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_17/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_6.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_6.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_6.w_0_fp32_master_0'], Moment1Out=['linear_6.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_6.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_6.w_0']} = adamw(inputs={Beta1Pow=['linear_6.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_6.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_6.w_0_fp32_master_0'], Moment1=['linear_6.w_0_fp32_master_0_moment1_0'], Moment2=['linear_6.w_0_fp32_master_0_moment2_0'], Param=['linear_6.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_18/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_6.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_6.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_6.b_0_fp32_master_0'], Moment1Out=['linear_6.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_6.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_6.b_0']} = adamw(inputs={Beta1Pow=['linear_6.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_6.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_6.b_0_fp32_master_0'], Moment1=['linear_6.b_0_fp32_master_0_moment1_0'], Moment2=['linear_6.b_0_fp32_master_0_moment2_0'], Param=['linear_6.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_19/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_7.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_7.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_7.w_0_fp32_master_0'], Moment1Out=['linear_7.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_7.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_7.w_0']} = adamw(inputs={Beta1Pow=['linear_7.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_7.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_7.w_0_fp32_master_0'], Moment1=['linear_7.w_0_fp32_master_0_moment1_0'], Moment2=['linear_7.w_0_fp32_master_0_moment2_0'], Param=['linear_7.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_20/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_7.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_7.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_7.b_0_fp32_master_0'], Moment1Out=['linear_7.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_7.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_7.b_0']} = adamw(inputs={Beta1Pow=['linear_7.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_7.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_7.b_0_fp32_master_0'], Moment1=['linear_7.b_0_fp32_master_0_moment1_0'], Moment2=['linear_7.b_0_fp32_master_0_moment2_0'], Param=['linear_7.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_21/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_2.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_2.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_2.w_0_moment1_0'], Moment2Out=['layer_norm_2.w_0_moment2_0'], ParamOut=['layer_norm_2.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_2.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_2.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_2.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_2.w_0_moment1_0'], Moment2=['layer_norm_2.w_0_moment2_0'], Param=['layer_norm_2.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_22/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_2.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_2.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_2.b_0_moment1_0'], Moment2Out=['layer_norm_2.b_0_moment2_0'], ParamOut=['layer_norm_2.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_2.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_2.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_2.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_2.b_0_moment1_0'], Moment2=['layer_norm_2.b_0_moment2_0'], Param=['layer_norm_2.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_23/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_3.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_3.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_3.w_0_moment1_0'], Moment2Out=['layer_norm_3.w_0_moment2_0'], ParamOut=['layer_norm_3.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_3.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_3.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_3.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_3.w_0_moment1_0'], Moment2=['layer_norm_3.w_0_moment2_0'], Param=['layer_norm_3.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_24/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_3.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_3.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_3.b_0_moment1_0'], Moment2Out=['layer_norm_3.b_0_moment2_0'], ParamOut=['layer_norm_3.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_3.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_3.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_3.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_3.b_0_moment1_0'], Moment2=['layer_norm_3.b_0_moment2_0'], Param=['layer_norm_3.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_25/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_8.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_8.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_8.w_0_fp32_master_0'], Moment1Out=['linear_8.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_8.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_8.w_0']} = adamw(inputs={Beta1Pow=['linear_8.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_8.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_8.w_0_fp32_master_0'], Moment1=['linear_8.w_0_fp32_master_0_moment1_0'], Moment2=['linear_8.w_0_fp32_master_0_moment2_0'], Param=['linear_8.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_26/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_8.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_8.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_8.b_0_fp32_master_0'], Moment1Out=['linear_8.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_8.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_8.b_0']} = adamw(inputs={Beta1Pow=['linear_8.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_8.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_8.b_0_fp32_master_0'], Moment1=['linear_8.b_0_fp32_master_0_moment1_0'], Moment2=['linear_8.b_0_fp32_master_0_moment2_0'], Param=['linear_8.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_27/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_9.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_9.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_9.w_0_fp32_master_0'], Moment1Out=['linear_9.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_9.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_9.w_0']} = adamw(inputs={Beta1Pow=['linear_9.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_9.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_9.w_0_fp32_master_0'], Moment1=['linear_9.w_0_fp32_master_0_moment1_0'], Moment2=['linear_9.w_0_fp32_master_0_moment2_0'], Param=['linear_9.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_28/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_9.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_9.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_9.b_0_fp32_master_0'], Moment1Out=['linear_9.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_9.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_9.b_0']} = adamw(inputs={Beta1Pow=['linear_9.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_9.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_9.b_0_fp32_master_0'], Moment1=['linear_9.b_0_fp32_master_0_moment1_0'], Moment2=['linear_9.b_0_fp32_master_0_moment2_0'], Param=['linear_9.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_29/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_10.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_10.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_10.w_0_fp32_master_0'], Moment1Out=['linear_10.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_10.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_10.w_0']} = adamw(inputs={Beta1Pow=['linear_10.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_10.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_10.w_0_fp32_master_0'], Moment1=['linear_10.w_0_fp32_master_0_moment1_0'], Moment2=['linear_10.w_0_fp32_master_0_moment2_0'], Param=['linear_10.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_30/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_10.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_10.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_10.b_0_fp32_master_0'], Moment1Out=['linear_10.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_10.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_10.b_0']} = adamw(inputs={Beta1Pow=['linear_10.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_10.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_10.b_0_fp32_master_0'], Moment1=['linear_10.b_0_fp32_master_0_moment1_0'], Moment2=['linear_10.b_0_fp32_master_0_moment2_0'], Param=['linear_10.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_31/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_11.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_11.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_11.w_0_fp32_master_0'], Moment1Out=['linear_11.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_11.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_11.w_0']} = adamw(inputs={Beta1Pow=['linear_11.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_11.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_11.w_0_fp32_master_0'], Moment1=['linear_11.w_0_fp32_master_0_moment1_0'], Moment2=['linear_11.w_0_fp32_master_0_moment2_0'], Param=['linear_11.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_32/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_11.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_11.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_11.b_0_fp32_master_0'], Moment1Out=['linear_11.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_11.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_11.b_0']} = adamw(inputs={Beta1Pow=['linear_11.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_11.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_11.b_0_fp32_master_0'], Moment1=['linear_11.b_0_fp32_master_0_moment1_0'], Moment2=['linear_11.b_0_fp32_master_0_moment2_0'], Param=['linear_11.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_33/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_4.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_4.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_4.w_0_moment1_0'], Moment2Out=['layer_norm_4.w_0_moment2_0'], ParamOut=['layer_norm_4.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_4.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_4.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_4.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_4.w_0_moment1_0'], Moment2=['layer_norm_4.w_0_moment2_0'], Param=['layer_norm_4.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_34/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_4.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_4.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_4.b_0_moment1_0'], Moment2Out=['layer_norm_4.b_0_moment2_0'], ParamOut=['layer_norm_4.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_4.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_4.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_4.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_4.b_0_moment1_0'], Moment2=['layer_norm_4.b_0_moment2_0'], Param=['layer_norm_4.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_35/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_5.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_5.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_5.w_0_moment1_0'], Moment2Out=['layer_norm_5.w_0_moment2_0'], ParamOut=['layer_norm_5.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_5.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_5.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_5.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_5.w_0_moment1_0'], Moment2=['layer_norm_5.w_0_moment2_0'], Param=['layer_norm_5.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_36/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_5.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_5.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_5.b_0_moment1_0'], Moment2Out=['layer_norm_5.b_0_moment2_0'], ParamOut=['layer_norm_5.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_5.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_5.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_5.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_5.b_0_moment1_0'], Moment2=['layer_norm_5.b_0_moment2_0'], Param=['layer_norm_5.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_37/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_12.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_12.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_12.w_0_fp32_master_0'], Moment1Out=['linear_12.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_12.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_12.w_0']} = adamw(inputs={Beta1Pow=['linear_12.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_12.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_12.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_12.w_0_fp32_master_0'], Moment1=['linear_12.w_0_fp32_master_0_moment1_0'], Moment2=['linear_12.w_0_fp32_master_0_moment2_0'], Param=['linear_12.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_38/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_12.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_12.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_12.b_0_fp32_master_0'], Moment1Out=['linear_12.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_12.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_12.b_0']} = adamw(inputs={Beta1Pow=['linear_12.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_12.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_12.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_12.b_0_fp32_master_0'], Moment1=['linear_12.b_0_fp32_master_0_moment1_0'], Moment2=['linear_12.b_0_fp32_master_0_moment2_0'], Param=['linear_12.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_39/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_13.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_13.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_13.w_0_fp32_master_0'], Moment1Out=['linear_13.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_13.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_13.w_0']} = adamw(inputs={Beta1Pow=['linear_13.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_13.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_13.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_13.w_0_fp32_master_0'], Moment1=['linear_13.w_0_fp32_master_0_moment1_0'], Moment2=['linear_13.w_0_fp32_master_0_moment2_0'], Param=['linear_13.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_40/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_13.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_13.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_13.b_0_fp32_master_0'], Moment1Out=['linear_13.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_13.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_13.b_0']} = adamw(inputs={Beta1Pow=['linear_13.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_13.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_13.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_13.b_0_fp32_master_0'], Moment1=['linear_13.b_0_fp32_master_0_moment1_0'], Moment2=['linear_13.b_0_fp32_master_0_moment2_0'], Param=['linear_13.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_41/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_14.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_14.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_14.w_0_fp32_master_0'], Moment1Out=['linear_14.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_14.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_14.w_0']} = adamw(inputs={Beta1Pow=['linear_14.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_14.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_14.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_14.w_0_fp32_master_0'], Moment1=['linear_14.w_0_fp32_master_0_moment1_0'], Moment2=['linear_14.w_0_fp32_master_0_moment2_0'], Param=['linear_14.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_42/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_14.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_14.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_14.b_0_fp32_master_0'], Moment1Out=['linear_14.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_14.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_14.b_0']} = adamw(inputs={Beta1Pow=['linear_14.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_14.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_14.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_14.b_0_fp32_master_0'], Moment1=['linear_14.b_0_fp32_master_0_moment1_0'], Moment2=['linear_14.b_0_fp32_master_0_moment2_0'], Param=['linear_14.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_43/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_15.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_15.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_15.w_0_fp32_master_0'], Moment1Out=['linear_15.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_15.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_15.w_0']} = adamw(inputs={Beta1Pow=['linear_15.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_15.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_15.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_15.w_0_fp32_master_0'], Moment1=['linear_15.w_0_fp32_master_0_moment1_0'], Moment2=['linear_15.w_0_fp32_master_0_moment2_0'], Param=['linear_15.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_44/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_15.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_15.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_15.b_0_fp32_master_0'], Moment1Out=['linear_15.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_15.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_15.b_0']} = adamw(inputs={Beta1Pow=['linear_15.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_15.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_15.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_15.b_0_fp32_master_0'], Moment1=['linear_15.b_0_fp32_master_0_moment1_0'], Moment2=['linear_15.b_0_fp32_master_0_moment2_0'], Param=['linear_15.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_45/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_6.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_6.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_6.w_0_moment1_0'], Moment2Out=['layer_norm_6.w_0_moment2_0'], ParamOut=['layer_norm_6.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_6.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_6.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_6.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_6.w_0_moment1_0'], Moment2=['layer_norm_6.w_0_moment2_0'], Param=['layer_norm_6.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_46/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_6.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_6.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_6.b_0_moment1_0'], Moment2Out=['layer_norm_6.b_0_moment2_0'], ParamOut=['layer_norm_6.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_6.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_6.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_6.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_6.b_0_moment1_0'], Moment2=['layer_norm_6.b_0_moment2_0'], Param=['layer_norm_6.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_47/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_7.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_7.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_7.w_0_moment1_0'], Moment2Out=['layer_norm_7.w_0_moment2_0'], ParamOut=['layer_norm_7.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_7.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_7.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_7.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_7.w_0_moment1_0'], Moment2=['layer_norm_7.w_0_moment2_0'], Param=['layer_norm_7.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_48/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_7.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_7.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_7.b_0_moment1_0'], Moment2Out=['layer_norm_7.b_0_moment2_0'], ParamOut=['layer_norm_7.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_7.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_7.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_7.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_7.b_0_moment1_0'], Moment2=['layer_norm_7.b_0_moment2_0'], Param=['layer_norm_7.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_49/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_16.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_16.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_16.w_0_fp32_master_0'], Moment1Out=['linear_16.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_16.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_16.w_0']} = adamw(inputs={Beta1Pow=['linear_16.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_16.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_16.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_16.w_0_fp32_master_0'], Moment1=['linear_16.w_0_fp32_master_0_moment1_0'], Moment2=['linear_16.w_0_fp32_master_0_moment2_0'], Param=['linear_16.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_50/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_16.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_16.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_16.b_0_fp32_master_0'], Moment1Out=['linear_16.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_16.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_16.b_0']} = adamw(inputs={Beta1Pow=['linear_16.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_16.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_16.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_16.b_0_fp32_master_0'], Moment1=['linear_16.b_0_fp32_master_0_moment1_0'], Moment2=['linear_16.b_0_fp32_master_0_moment2_0'], Param=['linear_16.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_51/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_17.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_17.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_17.w_0_fp32_master_0'], Moment1Out=['linear_17.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_17.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_17.w_0']} = adamw(inputs={Beta1Pow=['linear_17.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_17.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_17.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_17.w_0_fp32_master_0'], Moment1=['linear_17.w_0_fp32_master_0_moment1_0'], Moment2=['linear_17.w_0_fp32_master_0_moment2_0'], Param=['linear_17.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_52/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_17.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_17.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_17.b_0_fp32_master_0'], Moment1Out=['linear_17.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_17.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_17.b_0']} = adamw(inputs={Beta1Pow=['linear_17.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_17.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_17.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_17.b_0_fp32_master_0'], Moment1=['linear_17.b_0_fp32_master_0_moment1_0'], Moment2=['linear_17.b_0_fp32_master_0_moment2_0'], Param=['linear_17.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_53/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_18.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_18.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_18.w_0_fp32_master_0'], Moment1Out=['linear_18.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_18.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_18.w_0']} = adamw(inputs={Beta1Pow=['linear_18.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_18.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_18.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_18.w_0_fp32_master_0'], Moment1=['linear_18.w_0_fp32_master_0_moment1_0'], Moment2=['linear_18.w_0_fp32_master_0_moment2_0'], Param=['linear_18.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_54/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_18.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_18.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_18.b_0_fp32_master_0'], Moment1Out=['linear_18.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_18.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_18.b_0']} = adamw(inputs={Beta1Pow=['linear_18.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_18.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_18.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_18.b_0_fp32_master_0'], Moment1=['linear_18.b_0_fp32_master_0_moment1_0'], Moment2=['linear_18.b_0_fp32_master_0_moment2_0'], Param=['linear_18.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_55/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_19.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_19.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_19.w_0_fp32_master_0'], Moment1Out=['linear_19.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_19.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_19.w_0']} = adamw(inputs={Beta1Pow=['linear_19.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_19.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_19.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_19.w_0_fp32_master_0'], Moment1=['linear_19.w_0_fp32_master_0_moment1_0'], Moment2=['linear_19.w_0_fp32_master_0_moment2_0'], Param=['linear_19.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_56/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_19.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_19.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_19.b_0_fp32_master_0'], Moment1Out=['linear_19.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_19.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_19.b_0']} = adamw(inputs={Beta1Pow=['linear_19.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_19.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_19.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_19.b_0_fp32_master_0'], Moment1=['linear_19.b_0_fp32_master_0_moment1_0'], Moment2=['linear_19.b_0_fp32_master_0_moment2_0'], Param=['linear_19.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_57/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_8.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_8.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_8.w_0_moment1_0'], Moment2Out=['layer_norm_8.w_0_moment2_0'], ParamOut=['layer_norm_8.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_8.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_8.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_8.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_8.w_0_moment1_0'], Moment2=['layer_norm_8.w_0_moment2_0'], Param=['layer_norm_8.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_58/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_8.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_8.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_8.b_0_moment1_0'], Moment2Out=['layer_norm_8.b_0_moment2_0'], ParamOut=['layer_norm_8.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_8.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_8.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_8.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_8.b_0_moment1_0'], Moment2=['layer_norm_8.b_0_moment2_0'], Param=['layer_norm_8.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_59/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_9.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_9.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_9.w_0_moment1_0'], Moment2Out=['layer_norm_9.w_0_moment2_0'], ParamOut=['layer_norm_9.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_9.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_9.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_9.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_9.w_0_moment1_0'], Moment2=['layer_norm_9.w_0_moment2_0'], Param=['layer_norm_9.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_60/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_9.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_9.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_9.b_0_moment1_0'], Moment2Out=['layer_norm_9.b_0_moment2_0'], ParamOut=['layer_norm_9.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_9.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_9.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_9.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_9.b_0_moment1_0'], Moment2=['layer_norm_9.b_0_moment2_0'], Param=['layer_norm_9.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_61/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_20.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_20.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_20.w_0_fp32_master_0'], Moment1Out=['linear_20.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_20.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_20.w_0']} = adamw(inputs={Beta1Pow=['linear_20.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_20.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_20.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_20.w_0_fp32_master_0'], Moment1=['linear_20.w_0_fp32_master_0_moment1_0'], Moment2=['linear_20.w_0_fp32_master_0_moment2_0'], Param=['linear_20.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_62/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_20.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_20.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_20.b_0_fp32_master_0'], Moment1Out=['linear_20.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_20.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_20.b_0']} = adamw(inputs={Beta1Pow=['linear_20.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_20.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_20.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_20.b_0_fp32_master_0'], Moment1=['linear_20.b_0_fp32_master_0_moment1_0'], Moment2=['linear_20.b_0_fp32_master_0_moment2_0'], Param=['linear_20.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_63/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_21.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_21.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_21.w_0_fp32_master_0'], Moment1Out=['linear_21.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_21.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_21.w_0']} = adamw(inputs={Beta1Pow=['linear_21.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_21.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_21.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_21.w_0_fp32_master_0'], Moment1=['linear_21.w_0_fp32_master_0_moment1_0'], Moment2=['linear_21.w_0_fp32_master_0_moment2_0'], Param=['linear_21.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_64/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_21.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_21.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_21.b_0_fp32_master_0'], Moment1Out=['linear_21.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_21.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_21.b_0']} = adamw(inputs={Beta1Pow=['linear_21.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_21.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_21.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_21.b_0_fp32_master_0'], Moment1=['linear_21.b_0_fp32_master_0_moment1_0'], Moment2=['linear_21.b_0_fp32_master_0_moment2_0'], Param=['linear_21.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_65/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_22.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_22.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_22.w_0_fp32_master_0'], Moment1Out=['linear_22.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_22.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_22.w_0']} = adamw(inputs={Beta1Pow=['linear_22.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_22.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_22.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_22.w_0_fp32_master_0'], Moment1=['linear_22.w_0_fp32_master_0_moment1_0'], Moment2=['linear_22.w_0_fp32_master_0_moment2_0'], Param=['linear_22.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_66/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_22.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_22.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_22.b_0_fp32_master_0'], Moment1Out=['linear_22.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_22.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_22.b_0']} = adamw(inputs={Beta1Pow=['linear_22.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_22.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_22.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_22.b_0_fp32_master_0'], Moment1=['linear_22.b_0_fp32_master_0_moment1_0'], Moment2=['linear_22.b_0_fp32_master_0_moment2_0'], Param=['linear_22.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_67/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_23.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_23.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_23.w_0_fp32_master_0'], Moment1Out=['linear_23.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_23.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_23.w_0']} = adamw(inputs={Beta1Pow=['linear_23.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_23.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_23.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_23.w_0_fp32_master_0'], Moment1=['linear_23.w_0_fp32_master_0_moment1_0'], Moment2=['linear_23.w_0_fp32_master_0_moment2_0'], Param=['linear_23.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_68/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_23.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_23.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_23.b_0_fp32_master_0'], Moment1Out=['linear_23.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_23.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_23.b_0']} = adamw(inputs={Beta1Pow=['linear_23.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_23.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_23.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_23.b_0_fp32_master_0'], Moment1=['linear_23.b_0_fp32_master_0_moment1_0'], Moment2=['linear_23.b_0_fp32_master_0_moment2_0'], Param=['linear_23.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_69/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_10.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_10.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_10.w_0_moment1_0'], Moment2Out=['layer_norm_10.w_0_moment2_0'], ParamOut=['layer_norm_10.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_10.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_10.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_10.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_10.w_0_moment1_0'], Moment2=['layer_norm_10.w_0_moment2_0'], Param=['layer_norm_10.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_70/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_10.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_10.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_10.b_0_moment1_0'], Moment2Out=['layer_norm_10.b_0_moment2_0'], ParamOut=['layer_norm_10.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_10.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_10.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_10.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_10.b_0_moment1_0'], Moment2=['layer_norm_10.b_0_moment2_0'], Param=['layer_norm_10.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_71/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_11.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_11.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_11.w_0_moment1_0'], Moment2Out=['layer_norm_11.w_0_moment2_0'], ParamOut=['layer_norm_11.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_11.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_11.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_11.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_11.w_0_moment1_0'], Moment2=['layer_norm_11.w_0_moment2_0'], Param=['layer_norm_11.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_72/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_11.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_11.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_11.b_0_moment1_0'], Moment2Out=['layer_norm_11.b_0_moment2_0'], ParamOut=['layer_norm_11.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_11.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_11.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_11.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_11.b_0_moment1_0'], Moment2=['layer_norm_11.b_0_moment2_0'], Param=['layer_norm_11.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_73/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_24.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_24.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_24.w_0_fp32_master_0'], Moment1Out=['linear_24.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_24.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_24.w_0']} = adamw(inputs={Beta1Pow=['linear_24.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_24.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_24.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_24.w_0_fp32_master_0'], Moment1=['linear_24.w_0_fp32_master_0_moment1_0'], Moment2=['linear_24.w_0_fp32_master_0_moment2_0'], Param=['linear_24.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_74/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_24.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_24.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_24.b_0_fp32_master_0'], Moment1Out=['linear_24.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_24.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_24.b_0']} = adamw(inputs={Beta1Pow=['linear_24.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_24.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_24.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_24.b_0_fp32_master_0'], Moment1=['linear_24.b_0_fp32_master_0_moment1_0'], Moment2=['linear_24.b_0_fp32_master_0_moment2_0'], Param=['linear_24.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_75/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_25.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_25.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_25.w_0_fp32_master_0'], Moment1Out=['linear_25.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_25.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_25.w_0']} = adamw(inputs={Beta1Pow=['linear_25.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_25.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_25.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_25.w_0_fp32_master_0'], Moment1=['linear_25.w_0_fp32_master_0_moment1_0'], Moment2=['linear_25.w_0_fp32_master_0_moment2_0'], Param=['linear_25.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_76/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_25.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_25.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_25.b_0_fp32_master_0'], Moment1Out=['linear_25.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_25.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_25.b_0']} = adamw(inputs={Beta1Pow=['linear_25.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_25.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_25.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_25.b_0_fp32_master_0'], Moment1=['linear_25.b_0_fp32_master_0_moment1_0'], Moment2=['linear_25.b_0_fp32_master_0_moment2_0'], Param=['linear_25.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_77/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_26.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_26.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_26.w_0_fp32_master_0'], Moment1Out=['linear_26.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_26.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_26.w_0']} = adamw(inputs={Beta1Pow=['linear_26.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_26.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_26.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_26.w_0_fp32_master_0'], Moment1=['linear_26.w_0_fp32_master_0_moment1_0'], Moment2=['linear_26.w_0_fp32_master_0_moment2_0'], Param=['linear_26.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_78/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_26.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_26.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_26.b_0_fp32_master_0'], Moment1Out=['linear_26.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_26.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_26.b_0']} = adamw(inputs={Beta1Pow=['linear_26.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_26.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_26.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_26.b_0_fp32_master_0'], Moment1=['linear_26.b_0_fp32_master_0_moment1_0'], Moment2=['linear_26.b_0_fp32_master_0_moment2_0'], Param=['linear_26.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_79/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_27.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_27.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_27.w_0_fp32_master_0'], Moment1Out=['linear_27.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_27.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_27.w_0']} = adamw(inputs={Beta1Pow=['linear_27.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_27.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_27.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_27.w_0_fp32_master_0'], Moment1=['linear_27.w_0_fp32_master_0_moment1_0'], Moment2=['linear_27.w_0_fp32_master_0_moment2_0'], Param=['linear_27.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_80/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_27.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_27.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_27.b_0_fp32_master_0'], Moment1Out=['linear_27.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_27.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_27.b_0']} = adamw(inputs={Beta1Pow=['linear_27.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_27.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_27.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_27.b_0_fp32_master_0'], Moment1=['linear_27.b_0_fp32_master_0_moment1_0'], Moment2=['linear_27.b_0_fp32_master_0_moment2_0'], Param=['linear_27.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_81/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_12.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_12.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_12.w_0_moment1_0'], Moment2Out=['layer_norm_12.w_0_moment2_0'], ParamOut=['layer_norm_12.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_12.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_12.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_12.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_12.w_0_moment1_0'], Moment2=['layer_norm_12.w_0_moment2_0'], Param=['layer_norm_12.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_82/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_12.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_12.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_12.b_0_moment1_0'], Moment2Out=['layer_norm_12.b_0_moment2_0'], ParamOut=['layer_norm_12.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_12.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_12.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_12.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_12.b_0_moment1_0'], Moment2=['layer_norm_12.b_0_moment2_0'], Param=['layer_norm_12.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_83/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_13.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_13.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_13.w_0_moment1_0'], Moment2Out=['layer_norm_13.w_0_moment2_0'], ParamOut=['layer_norm_13.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_13.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_13.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_13.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_13.w_0_moment1_0'], Moment2=['layer_norm_13.w_0_moment2_0'], Param=['layer_norm_13.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_84/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_13.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_13.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_13.b_0_moment1_0'], Moment2Out=['layer_norm_13.b_0_moment2_0'], ParamOut=['layer_norm_13.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_13.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_13.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_13.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_13.b_0_moment1_0'], Moment2=['layer_norm_13.b_0_moment2_0'], Param=['layer_norm_13.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_85/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_28.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_28.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_28.w_0_fp32_master_0'], Moment1Out=['linear_28.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_28.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_28.w_0']} = adamw(inputs={Beta1Pow=['linear_28.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_28.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_28.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_28.w_0_fp32_master_0'], Moment1=['linear_28.w_0_fp32_master_0_moment1_0'], Moment2=['linear_28.w_0_fp32_master_0_moment2_0'], Param=['linear_28.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_86/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_28.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_28.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_28.b_0_fp32_master_0'], Moment1Out=['linear_28.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_28.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_28.b_0']} = adamw(inputs={Beta1Pow=['linear_28.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_28.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_28.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_28.b_0_fp32_master_0'], Moment1=['linear_28.b_0_fp32_master_0_moment1_0'], Moment2=['linear_28.b_0_fp32_master_0_moment2_0'], Param=['linear_28.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_87/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_29.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_29.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_29.w_0_fp32_master_0'], Moment1Out=['linear_29.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_29.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_29.w_0']} = adamw(inputs={Beta1Pow=['linear_29.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_29.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_29.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_29.w_0_fp32_master_0'], Moment1=['linear_29.w_0_fp32_master_0_moment1_0'], Moment2=['linear_29.w_0_fp32_master_0_moment2_0'], Param=['linear_29.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_88/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_29.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_29.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_29.b_0_fp32_master_0'], Moment1Out=['linear_29.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_29.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_29.b_0']} = adamw(inputs={Beta1Pow=['linear_29.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_29.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_29.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_29.b_0_fp32_master_0'], Moment1=['linear_29.b_0_fp32_master_0_moment1_0'], Moment2=['linear_29.b_0_fp32_master_0_moment2_0'], Param=['linear_29.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_89/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_30.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_30.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_30.w_0_fp32_master_0'], Moment1Out=['linear_30.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_30.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_30.w_0']} = adamw(inputs={Beta1Pow=['linear_30.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_30.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_30.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_30.w_0_fp32_master_0'], Moment1=['linear_30.w_0_fp32_master_0_moment1_0'], Moment2=['linear_30.w_0_fp32_master_0_moment2_0'], Param=['linear_30.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_90/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_30.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_30.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_30.b_0_fp32_master_0'], Moment1Out=['linear_30.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_30.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_30.b_0']} = adamw(inputs={Beta1Pow=['linear_30.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_30.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_30.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_30.b_0_fp32_master_0'], Moment1=['linear_30.b_0_fp32_master_0_moment1_0'], Moment2=['linear_30.b_0_fp32_master_0_moment2_0'], Param=['linear_30.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_91/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_31.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_31.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_31.w_0_fp32_master_0'], Moment1Out=['linear_31.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_31.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_31.w_0']} = adamw(inputs={Beta1Pow=['linear_31.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_31.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_31.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_31.w_0_fp32_master_0'], Moment1=['linear_31.w_0_fp32_master_0_moment1_0'], Moment2=['linear_31.w_0_fp32_master_0_moment2_0'], Param=['linear_31.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_92/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_31.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_31.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_31.b_0_fp32_master_0'], Moment1Out=['linear_31.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_31.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_31.b_0']} = adamw(inputs={Beta1Pow=['linear_31.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_31.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_31.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_31.b_0_fp32_master_0'], Moment1=['linear_31.b_0_fp32_master_0_moment1_0'], Moment2=['linear_31.b_0_fp32_master_0_moment2_0'], Param=['linear_31.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_93/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_14.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_14.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_14.w_0_moment1_0'], Moment2Out=['layer_norm_14.w_0_moment2_0'], ParamOut=['layer_norm_14.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_14.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_14.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_14.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_14.w_0_moment1_0'], Moment2=['layer_norm_14.w_0_moment2_0'], Param=['layer_norm_14.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_94/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_14.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_14.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_14.b_0_moment1_0'], Moment2Out=['layer_norm_14.b_0_moment2_0'], ParamOut=['layer_norm_14.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_14.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_14.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_14.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_14.b_0_moment1_0'], Moment2=['layer_norm_14.b_0_moment2_0'], Param=['layer_norm_14.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_95/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_15.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_15.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_15.w_0_moment1_0'], Moment2Out=['layer_norm_15.w_0_moment2_0'], ParamOut=['layer_norm_15.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_15.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_15.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_15.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_15.w_0_moment1_0'], Moment2=['layer_norm_15.w_0_moment2_0'], Param=['layer_norm_15.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_96/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_15.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_15.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_15.b_0_moment1_0'], Moment2Out=['layer_norm_15.b_0_moment2_0'], ParamOut=['layer_norm_15.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_15.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_15.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_15.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_15.b_0_moment1_0'], Moment2=['layer_norm_15.b_0_moment2_0'], Param=['layer_norm_15.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_97/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_32.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_32.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_32.w_0_fp32_master_0'], Moment1Out=['linear_32.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_32.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_32.w_0']} = adamw(inputs={Beta1Pow=['linear_32.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_32.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_32.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_32.w_0_fp32_master_0'], Moment1=['linear_32.w_0_fp32_master_0_moment1_0'], Moment2=['linear_32.w_0_fp32_master_0_moment2_0'], Param=['linear_32.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_98/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_32.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_32.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_32.b_0_fp32_master_0'], Moment1Out=['linear_32.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_32.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_32.b_0']} = adamw(inputs={Beta1Pow=['linear_32.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_32.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_32.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_32.b_0_fp32_master_0'], Moment1=['linear_32.b_0_fp32_master_0_moment1_0'], Moment2=['linear_32.b_0_fp32_master_0_moment2_0'], Param=['linear_32.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_99/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_33.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_33.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_33.w_0_fp32_master_0'], Moment1Out=['linear_33.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_33.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_33.w_0']} = adamw(inputs={Beta1Pow=['linear_33.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_33.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_33.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_33.w_0_fp32_master_0'], Moment1=['linear_33.w_0_fp32_master_0_moment1_0'], Moment2=['linear_33.w_0_fp32_master_0_moment2_0'], Param=['linear_33.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_100/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_33.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_33.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_33.b_0_fp32_master_0'], Moment1Out=['linear_33.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_33.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_33.b_0']} = adamw(inputs={Beta1Pow=['linear_33.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_33.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_33.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_33.b_0_fp32_master_0'], Moment1=['linear_33.b_0_fp32_master_0_moment1_0'], Moment2=['linear_33.b_0_fp32_master_0_moment2_0'], Param=['linear_33.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_101/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_34.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_34.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_34.w_0_fp32_master_0'], Moment1Out=['linear_34.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_34.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_34.w_0']} = adamw(inputs={Beta1Pow=['linear_34.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_34.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_34.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_34.w_0_fp32_master_0'], Moment1=['linear_34.w_0_fp32_master_0_moment1_0'], Moment2=['linear_34.w_0_fp32_master_0_moment2_0'], Param=['linear_34.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_102/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_34.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_34.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_34.b_0_fp32_master_0'], Moment1Out=['linear_34.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_34.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_34.b_0']} = adamw(inputs={Beta1Pow=['linear_34.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_34.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_34.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_34.b_0_fp32_master_0'], Moment1=['linear_34.b_0_fp32_master_0_moment1_0'], Moment2=['linear_34.b_0_fp32_master_0_moment2_0'], Param=['linear_34.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_103/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_35.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_35.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_35.w_0_fp32_master_0'], Moment1Out=['linear_35.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_35.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_35.w_0']} = adamw(inputs={Beta1Pow=['linear_35.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_35.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_35.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_35.w_0_fp32_master_0'], Moment1=['linear_35.w_0_fp32_master_0_moment1_0'], Moment2=['linear_35.w_0_fp32_master_0_moment2_0'], Param=['linear_35.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_104/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_35.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_35.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_35.b_0_fp32_master_0'], Moment1Out=['linear_35.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_35.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_35.b_0']} = adamw(inputs={Beta1Pow=['linear_35.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_35.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_35.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_35.b_0_fp32_master_0'], Moment1=['linear_35.b_0_fp32_master_0_moment1_0'], Moment2=['linear_35.b_0_fp32_master_0_moment2_0'], Param=['linear_35.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_105/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_16.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_16.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_16.w_0_moment1_0'], Moment2Out=['layer_norm_16.w_0_moment2_0'], ParamOut=['layer_norm_16.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_16.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_16.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_16.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_16.w_0_moment1_0'], Moment2=['layer_norm_16.w_0_moment2_0'], Param=['layer_norm_16.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_106/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_16.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_16.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_16.b_0_moment1_0'], Moment2Out=['layer_norm_16.b_0_moment2_0'], ParamOut=['layer_norm_16.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_16.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_16.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_16.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_16.b_0_moment1_0'], Moment2=['layer_norm_16.b_0_moment2_0'], Param=['layer_norm_16.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_107/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_17.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_17.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_17.w_0_moment1_0'], Moment2Out=['layer_norm_17.w_0_moment2_0'], ParamOut=['layer_norm_17.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_17.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_17.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_17.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_17.w_0_moment1_0'], Moment2=['layer_norm_17.w_0_moment2_0'], Param=['layer_norm_17.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_108/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_17.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_17.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_17.b_0_moment1_0'], Moment2Out=['layer_norm_17.b_0_moment2_0'], ParamOut=['layer_norm_17.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_17.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_17.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_17.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_17.b_0_moment1_0'], Moment2=['layer_norm_17.b_0_moment2_0'], Param=['layer_norm_17.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_109/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_36.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_36.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_36.w_0_fp32_master_0'], Moment1Out=['linear_36.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_36.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_36.w_0']} = adamw(inputs={Beta1Pow=['linear_36.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_36.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_36.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_36.w_0_fp32_master_0'], Moment1=['linear_36.w_0_fp32_master_0_moment1_0'], Moment2=['linear_36.w_0_fp32_master_0_moment2_0'], Param=['linear_36.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_110/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_36.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_36.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_36.b_0_fp32_master_0'], Moment1Out=['linear_36.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_36.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_36.b_0']} = adamw(inputs={Beta1Pow=['linear_36.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_36.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_36.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_36.b_0_fp32_master_0'], Moment1=['linear_36.b_0_fp32_master_0_moment1_0'], Moment2=['linear_36.b_0_fp32_master_0_moment2_0'], Param=['linear_36.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_111/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_37.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_37.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_37.w_0_fp32_master_0'], Moment1Out=['linear_37.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_37.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_37.w_0']} = adamw(inputs={Beta1Pow=['linear_37.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_37.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_37.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_37.w_0_fp32_master_0'], Moment1=['linear_37.w_0_fp32_master_0_moment1_0'], Moment2=['linear_37.w_0_fp32_master_0_moment2_0'], Param=['linear_37.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_112/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_37.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_37.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_37.b_0_fp32_master_0'], Moment1Out=['linear_37.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_37.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_37.b_0']} = adamw(inputs={Beta1Pow=['linear_37.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_37.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_37.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_37.b_0_fp32_master_0'], Moment1=['linear_37.b_0_fp32_master_0_moment1_0'], Moment2=['linear_37.b_0_fp32_master_0_moment2_0'], Param=['linear_37.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_113/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_38.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_38.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_38.w_0_fp32_master_0'], Moment1Out=['linear_38.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_38.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_38.w_0']} = adamw(inputs={Beta1Pow=['linear_38.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_38.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_38.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_38.w_0_fp32_master_0'], Moment1=['linear_38.w_0_fp32_master_0_moment1_0'], Moment2=['linear_38.w_0_fp32_master_0_moment2_0'], Param=['linear_38.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_114/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_38.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_38.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_38.b_0_fp32_master_0'], Moment1Out=['linear_38.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_38.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_38.b_0']} = adamw(inputs={Beta1Pow=['linear_38.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_38.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_38.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_38.b_0_fp32_master_0'], Moment1=['linear_38.b_0_fp32_master_0_moment1_0'], Moment2=['linear_38.b_0_fp32_master_0_moment2_0'], Param=['linear_38.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_115/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_39.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_39.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_39.w_0_fp32_master_0'], Moment1Out=['linear_39.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_39.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_39.w_0']} = adamw(inputs={Beta1Pow=['linear_39.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_39.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_39.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_39.w_0_fp32_master_0'], Moment1=['linear_39.w_0_fp32_master_0_moment1_0'], Moment2=['linear_39.w_0_fp32_master_0_moment2_0'], Param=['linear_39.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_116/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_39.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_39.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_39.b_0_fp32_master_0'], Moment1Out=['linear_39.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_39.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_39.b_0']} = adamw(inputs={Beta1Pow=['linear_39.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_39.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_39.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_39.b_0_fp32_master_0'], Moment1=['linear_39.b_0_fp32_master_0_moment1_0'], Moment2=['linear_39.b_0_fp32_master_0_moment2_0'], Param=['linear_39.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_117/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_18.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_18.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_18.w_0_moment1_0'], Moment2Out=['layer_norm_18.w_0_moment2_0'], ParamOut=['layer_norm_18.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_18.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_18.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_18.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_18.w_0_moment1_0'], Moment2=['layer_norm_18.w_0_moment2_0'], Param=['layer_norm_18.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_118/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_18.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_18.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_18.b_0_moment1_0'], Moment2Out=['layer_norm_18.b_0_moment2_0'], ParamOut=['layer_norm_18.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_18.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_18.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_18.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_18.b_0_moment1_0'], Moment2=['layer_norm_18.b_0_moment2_0'], Param=['layer_norm_18.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_119/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_19.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_19.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_19.w_0_moment1_0'], Moment2Out=['layer_norm_19.w_0_moment2_0'], ParamOut=['layer_norm_19.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_19.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_19.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_19.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_19.w_0_moment1_0'], Moment2=['layer_norm_19.w_0_moment2_0'], Param=['layer_norm_19.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_120/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_19.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_19.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_19.b_0_moment1_0'], Moment2Out=['layer_norm_19.b_0_moment2_0'], ParamOut=['layer_norm_19.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_19.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_19.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_19.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_19.b_0_moment1_0'], Moment2=['layer_norm_19.b_0_moment2_0'], Param=['layer_norm_19.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_121/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_40.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_40.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_40.w_0_fp32_master_0'], Moment1Out=['linear_40.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_40.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_40.w_0']} = adamw(inputs={Beta1Pow=['linear_40.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_40.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_40.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_40.w_0_fp32_master_0'], Moment1=['linear_40.w_0_fp32_master_0_moment1_0'], Moment2=['linear_40.w_0_fp32_master_0_moment2_0'], Param=['linear_40.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_122/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_40.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_40.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_40.b_0_fp32_master_0'], Moment1Out=['linear_40.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_40.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_40.b_0']} = adamw(inputs={Beta1Pow=['linear_40.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_40.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_40.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_40.b_0_fp32_master_0'], Moment1=['linear_40.b_0_fp32_master_0_moment1_0'], Moment2=['linear_40.b_0_fp32_master_0_moment2_0'], Param=['linear_40.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_123/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_41.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_41.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_41.w_0_fp32_master_0'], Moment1Out=['linear_41.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_41.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_41.w_0']} = adamw(inputs={Beta1Pow=['linear_41.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_41.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_41.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_41.w_0_fp32_master_0'], Moment1=['linear_41.w_0_fp32_master_0_moment1_0'], Moment2=['linear_41.w_0_fp32_master_0_moment2_0'], Param=['linear_41.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_124/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_41.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_41.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_41.b_0_fp32_master_0'], Moment1Out=['linear_41.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_41.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_41.b_0']} = adamw(inputs={Beta1Pow=['linear_41.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_41.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_41.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_41.b_0_fp32_master_0'], Moment1=['linear_41.b_0_fp32_master_0_moment1_0'], Moment2=['linear_41.b_0_fp32_master_0_moment2_0'], Param=['linear_41.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_125/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_42.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_42.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_42.w_0_fp32_master_0'], Moment1Out=['linear_42.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_42.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_42.w_0']} = adamw(inputs={Beta1Pow=['linear_42.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_42.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_42.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_42.w_0_fp32_master_0'], Moment1=['linear_42.w_0_fp32_master_0_moment1_0'], Moment2=['linear_42.w_0_fp32_master_0_moment2_0'], Param=['linear_42.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_126/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_42.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_42.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_42.b_0_fp32_master_0'], Moment1Out=['linear_42.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_42.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_42.b_0']} = adamw(inputs={Beta1Pow=['linear_42.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_42.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_42.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_42.b_0_fp32_master_0'], Moment1=['linear_42.b_0_fp32_master_0_moment1_0'], Moment2=['linear_42.b_0_fp32_master_0_moment2_0'], Param=['linear_42.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_127/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_43.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_43.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_43.w_0_fp32_master_0'], Moment1Out=['linear_43.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_43.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_43.w_0']} = adamw(inputs={Beta1Pow=['linear_43.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_43.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_43.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_43.w_0_fp32_master_0'], Moment1=['linear_43.w_0_fp32_master_0_moment1_0'], Moment2=['linear_43.w_0_fp32_master_0_moment2_0'], Param=['linear_43.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_128/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_43.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_43.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_43.b_0_fp32_master_0'], Moment1Out=['linear_43.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_43.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_43.b_0']} = adamw(inputs={Beta1Pow=['linear_43.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_43.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_43.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_43.b_0_fp32_master_0'], Moment1=['linear_43.b_0_fp32_master_0_moment1_0'], Moment2=['linear_43.b_0_fp32_master_0_moment2_0'], Param=['linear_43.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_129/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_20.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_20.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_20.w_0_moment1_0'], Moment2Out=['layer_norm_20.w_0_moment2_0'], ParamOut=['layer_norm_20.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_20.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_20.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_20.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_20.w_0_moment1_0'], Moment2=['layer_norm_20.w_0_moment2_0'], Param=['layer_norm_20.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_130/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_20.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_20.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_20.b_0_moment1_0'], Moment2Out=['layer_norm_20.b_0_moment2_0'], ParamOut=['layer_norm_20.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_20.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_20.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_20.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_20.b_0_moment1_0'], Moment2=['layer_norm_20.b_0_moment2_0'], Param=['layer_norm_20.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_131/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_21.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_21.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_21.w_0_moment1_0'], Moment2Out=['layer_norm_21.w_0_moment2_0'], ParamOut=['layer_norm_21.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_21.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_21.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_21.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_21.w_0_moment1_0'], Moment2=['layer_norm_21.w_0_moment2_0'], Param=['layer_norm_21.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_132/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_21.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_21.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_21.b_0_moment1_0'], Moment2Out=['layer_norm_21.b_0_moment2_0'], ParamOut=['layer_norm_21.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_21.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_21.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_21.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_21.b_0_moment1_0'], Moment2=['layer_norm_21.b_0_moment2_0'], Param=['layer_norm_21.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_133/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_44.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_44.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_44.w_0_fp32_master_0'], Moment1Out=['linear_44.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_44.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_44.w_0']} = adamw(inputs={Beta1Pow=['linear_44.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_44.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_44.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_44.w_0_fp32_master_0'], Moment1=['linear_44.w_0_fp32_master_0_moment1_0'], Moment2=['linear_44.w_0_fp32_master_0_moment2_0'], Param=['linear_44.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_134/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_44.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_44.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_44.b_0_fp32_master_0'], Moment1Out=['linear_44.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_44.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_44.b_0']} = adamw(inputs={Beta1Pow=['linear_44.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_44.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_44.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_44.b_0_fp32_master_0'], Moment1=['linear_44.b_0_fp32_master_0_moment1_0'], Moment2=['linear_44.b_0_fp32_master_0_moment2_0'], Param=['linear_44.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_135/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_45.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_45.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_45.w_0_fp32_master_0'], Moment1Out=['linear_45.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_45.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_45.w_0']} = adamw(inputs={Beta1Pow=['linear_45.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_45.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_45.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_45.w_0_fp32_master_0'], Moment1=['linear_45.w_0_fp32_master_0_moment1_0'], Moment2=['linear_45.w_0_fp32_master_0_moment2_0'], Param=['linear_45.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_136/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_45.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_45.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_45.b_0_fp32_master_0'], Moment1Out=['linear_45.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_45.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_45.b_0']} = adamw(inputs={Beta1Pow=['linear_45.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_45.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_45.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_45.b_0_fp32_master_0'], Moment1=['linear_45.b_0_fp32_master_0_moment1_0'], Moment2=['linear_45.b_0_fp32_master_0_moment2_0'], Param=['linear_45.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_137/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_46.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_46.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_46.w_0_fp32_master_0'], Moment1Out=['linear_46.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_46.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_46.w_0']} = adamw(inputs={Beta1Pow=['linear_46.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_46.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_46.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_46.w_0_fp32_master_0'], Moment1=['linear_46.w_0_fp32_master_0_moment1_0'], Moment2=['linear_46.w_0_fp32_master_0_moment2_0'], Param=['linear_46.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_138/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_46.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_46.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_46.b_0_fp32_master_0'], Moment1Out=['linear_46.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_46.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_46.b_0']} = adamw(inputs={Beta1Pow=['linear_46.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_46.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_46.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_46.b_0_fp32_master_0'], Moment1=['linear_46.b_0_fp32_master_0_moment1_0'], Moment2=['linear_46.b_0_fp32_master_0_moment2_0'], Param=['linear_46.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_139/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_47.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_47.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_47.w_0_fp32_master_0'], Moment1Out=['linear_47.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_47.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_47.w_0']} = adamw(inputs={Beta1Pow=['linear_47.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_47.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_47.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_47.w_0_fp32_master_0'], Moment1=['linear_47.w_0_fp32_master_0_moment1_0'], Moment2=['linear_47.w_0_fp32_master_0_moment2_0'], Param=['linear_47.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_140/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_47.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_47.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_47.b_0_fp32_master_0'], Moment1Out=['linear_47.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_47.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_47.b_0']} = adamw(inputs={Beta1Pow=['linear_47.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_47.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_47.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_47.b_0_fp32_master_0'], Moment1=['linear_47.b_0_fp32_master_0_moment1_0'], Moment2=['linear_47.b_0_fp32_master_0_moment2_0'], Param=['linear_47.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_141/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_22.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_22.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_22.w_0_moment1_0'], Moment2Out=['layer_norm_22.w_0_moment2_0'], ParamOut=['layer_norm_22.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_22.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_22.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_22.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_22.w_0_moment1_0'], Moment2=['layer_norm_22.w_0_moment2_0'], Param=['layer_norm_22.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_142/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_22.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_22.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_22.b_0_moment1_0'], Moment2Out=['layer_norm_22.b_0_moment2_0'], ParamOut=['layer_norm_22.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_22.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_22.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_22.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_22.b_0_moment1_0'], Moment2=['layer_norm_22.b_0_moment2_0'], Param=['layer_norm_22.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_143/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_23.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_23.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_23.w_0_moment1_0'], Moment2Out=['layer_norm_23.w_0_moment2_0'], ParamOut=['layer_norm_23.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_23.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_23.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_23.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_23.w_0_moment1_0'], Moment2=['layer_norm_23.w_0_moment2_0'], Param=['layer_norm_23.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_144/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_23.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_23.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_23.b_0_moment1_0'], Moment2Out=['layer_norm_23.b_0_moment2_0'], ParamOut=['layer_norm_23.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_23.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_23.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_23.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_23.b_0_moment1_0'], Moment2=['layer_norm_23.b_0_moment2_0'], Param=['layer_norm_23.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_145/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['embedding_0.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['embedding_1.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['embedding_1.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_0.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_0.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_0.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_0.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_0.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_0.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_0.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_1.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_1.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_1.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_1.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_1.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_1.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_1.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_2.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_2.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_2.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_3.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_3.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_3.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_2.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_2.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_2.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_2.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_4.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_4.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_4.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_5.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_5.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_5.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_3.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_3.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_3.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_3.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_6.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_6.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_6.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_7.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_7.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_7.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_4.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_4.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_4.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_4.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_8.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_8.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_8.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_9.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_9.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_9.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_5.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_5.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_5.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_5.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_10.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_10.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_10.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_11.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_11.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_11.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_6.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_6.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_6.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_6.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_12.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_12.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_12.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_13.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_13.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_13.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_7.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_7.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_7.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_7.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_14.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_14.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_14.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_15.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_15.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_15.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_8.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_8.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_8.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_8.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_16.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_16.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_16.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_17.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_17.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_17.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_9.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_9.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_9.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_9.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_18.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_18.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_18.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_19.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_19.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_19.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_10.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_10.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_10.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_10.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_20.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_20.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_20.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_21.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_21.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_21.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_11.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_11.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_11.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_11.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_22.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_22.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_22.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_23.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_23.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_23.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_12.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_12.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_12.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_12.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_24.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_24.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_24.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_25.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_25.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_25.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_13.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_13.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_13.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_13.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_26.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_26.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_26.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_27.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_27.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_27.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_14.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_14.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_14.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_14.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_28.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_28.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_28.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_28.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_29.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_29.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_29.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_29.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_15.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_15.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_15.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_15.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_30.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_30.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_30.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_30.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_31.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_31.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_31.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_31.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_16.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_16.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_16.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_16.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_32.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_32.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_32.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_32.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_33.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_33.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_33.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_33.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_17.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_17.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_17.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_17.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_34.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_34.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_34.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_34.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_35.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_35.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_35.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_35.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_18.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_18.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_18.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_18.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_36.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_36.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_36.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_36.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_37.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_37.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_37.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_37.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_19.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_19.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_19.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_19.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_38.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_38.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_38.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_38.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_39.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_39.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_39.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_39.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_20.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_20.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_20.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_20.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_40.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_40.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_40.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_40.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_41.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_41.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_41.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_41.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_21.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_21.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_21.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_21.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_42.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_42.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_42.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_42.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_43.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_43.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_43.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_43.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_22.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_22.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_22.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_22.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_44.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_44.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_44.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_44.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_45.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_45.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_45.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_45.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_23.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_23.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_23.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_23.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_46.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_46.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_46.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_46.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_47.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_47.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_47.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_47.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
}

[2024-03-01 03:14:33,663] [    INFO] process_group.py:150 - group_id: 0, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:58856
[2024-03-01 03:14:34,821] [    INFO] process_group.py:150 - group_id: 30, ranks: [0, 2], nranks: 2, trainer_endpoints: 172.17.0.3:58856
[2024-03-01 03:14:35,165] [    INFO] process_group.py:150 - group_id: 32, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:58856
[2024-03-01 03:14:35,492] [    INFO] process_group.py:150 - group_id: 33, ranks: [0, 1], nranks: 2, trainer_endpoints: 172.17.0.3:58856
/usr/local/lib/python3.9/dist-packages/paddle/distributed/auto_parallel/static/process_group.py:255: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-03-01 03:14:35,844] [    INFO] process_group.py:150 - group_id: 35, ranks: [1, 0], nranks: 2, trainer_endpoints: 172.17.0.3:58856
I0301 03:14:38.053397 30601 program_interpreter.cc:220] New Executor is Running.
/home/workspace/PaddleNLP/model_zoo/gpt-3/ppfleetx/utils/device.py:50: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
I0301 03:14:51.147982 30601 interpreter_util.cc:652] Standalone Executor is Used.
I0301 03:14:52.539858 31045 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
[32m[2024-03-01 03:15:02,064] [INFO][0m - [train] epoch: [0/1], batch: [9/30], avg_batch_cost: 2.34857 sec, speed: 0.43 step/s, ips_total: 1744 tokens/s, ips: 872 tokens/s, loss_scale: 32768.000000000, learning rate: 1.38889e-07, found_inf: 0, max_memory_allocated: 3329.2 MB, max_memory_reserved: 3479.2 MB, memory_allocated: 3108.7 MB, memory_reserved: 3479.2 MB[0m
[32m[2024-03-01 03:15:11,293] [INFO][0m - [train] epoch: [0/1], batch: [19/30], avg_batch_cost: 0.92286 sec, speed: 1.08 step/s, ips_total: 4438 tokens/s, ips: 2219 tokens/s, loss_scale: 32768.000000000, learning rate: 2.77778e-07, found_inf: 0, max_memory_allocated: 3329.2 MB, max_memory_reserved: 3479.2 MB, memory_allocated: 3108.7 MB, memory_reserved: 3479.2 MB[0m
[32m[2024-03-01 03:15:20,670] [INFO][0m - [train] epoch: [0/1], batch: [29/30], avg_batch_cost: 0.93751 sec, speed: 1.07 step/s, ips_total: 4369 tokens/s, ips: 2185 tokens/s, loss_scale: 32768.000000000, learning rate: 4.16667e-07, found_inf: 0, max_memory_allocated: 3329.2 MB, max_memory_reserved: 3479.2 MB, memory_allocated: 3108.7 MB, memory_reserved: 3479.2 MB[0m
[32m[2024-03-01 03:15:21,633] [INFO][0m - [Training] epoch: 0, total time: 43.07368 sec[0m
[32m[2024-03-01 03:15:21,633] [INFO][0m - The training process is complete and total cost of time for training is : 0:01:12[0m
I0301 03:15:23.476352 30601 process_group_nccl.cc:133] ProcessGroupNCCL destruct 
I0301 03:15:23.476531 30601 process_group_nccl.cc:133] ProcessGroupNCCL destruct 
I0301 03:15:23.476559 30601 process_group_nccl.cc:133] ProcessGroupNCCL destruct 
I0301 03:15:25.153625 30769 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
