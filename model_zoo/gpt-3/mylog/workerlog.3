grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
A new Series field (pipeline) detected!
A new field (schedule_mode) detected!
A new Series field (Profiler_auto) detected!
A new field (memory_stats) detected!
[2024-03-01 03:13:57,242] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0301 03:13:57.244073 30609 tcp_utils.cc:130] Successfully connected to 172.17.0.3:58855
I0301 03:13:57.284381 30609 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 03:13:57,288] [    INFO] topology.py:359 - Total 4 pipe comm group(s) create successfully!
W0301 03:13:57.298914 30609 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.8
W0301 03:13:57.345746 30609 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/usr/local/lib/python3.9/dist-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
I0301 03:14:05.459744 30609 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 03:14:05,460] [    INFO] topology.py:359 - Total 1 data comm group(s) create successfully!
[2024-03-01 03:14:05,460] [    INFO] topology.py:359 - Total 4 model comm group(s) create successfully!
[2024-03-01 03:14:05,460] [    INFO] topology.py:359 - Total 4 sharding comm group(s) create successfully!
I0301 03:14:05.460680 30609 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 03:14:05,460] [    INFO] topology.py:289 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 4, sep_degree: 1, mp_group: [3],  sharding_group: [3], pp_group: [3], dp_group: [0, 1, 2, 3], sep:group: None, check/clip group: [3]
[32m[2024-03-01 03:14:05,461] [INFO][0m - The global seed is set to 2055, local seed is set to 2059 and random seed is set to 1124.[0m
[32m[2024-03-01 03:14:05,463] [INFO][0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:05,463] [INFO][0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:05,463] [INFO][0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json[0m
[32m[2024-03-01 03:14:05,463] [INFO][0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt[0m
[2024-03-01 03:14:05,560] [ WARNING] language_module.py:66 -  > padded vocab (size: 50304) with 0 dummy tokens (new size: 50304)
[32m[2024-03-01 03:14:05,560] [INFO][0m - Model Size: 0.35 B[0m
[32m[2024-03-01 03:14:05,649] [INFO][0m - 
===========================================================
==       PaddleFleetX is powered by PaddlePaddle !       ==
===========================================================
==                                                       ==
==   For more info please go to the following website.   ==
==                                                       ==
==      https://github.com/PaddlePaddle/PaddleFleetX     ==
===========================================================
[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m - Data : [0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -     Eval : [0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -         collate_fn : gpt_collate_fn[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -         dataset : [0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -             input_dir : ./data/[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -             max_seq_len : 1024[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -             mode : Eval[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -             name : GPTDataset[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -             num_samples : 40[0m
[32m[2024-03-01 03:14:05,650] [INFO][0m -             seed : 1024[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             split : [969, 30, 1][0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -         sample_split : 2[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -     Train : [0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -         collate_fn : gpt_collate_fn[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -         dataset : [0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             input_dir : ./data/[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             max_seq_len : 1024[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             mode : Train[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             name : GPTDataset[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             num_samples : 120[0m
[32m[2024-03-01 03:14:05,651] [INFO][0m -             seed : 1024[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -             split : [969, 30, 1][0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -         sample_split : 2[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m - Distributed : [0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -     dp_degree : 2[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -     mp_degree : 1[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -     pipeline : [0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -         schedule_mode : 1F1B[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -     pp_degree : 2[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -     sharding : [0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -         broadcast_overlap : False[0m
[32m[2024-03-01 03:14:05,652] [INFO][0m -         reduce_overlap : False[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -         sharding_degree : 1[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -         sharding_stage : 1[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m - Engine : [0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -     accumulate_steps : 2[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -     eval_freq : 100000[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -     eval_iters : 10[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -     logging_freq : 10[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -     max_steps : 30[0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -     mix_precision : [0m
[32m[2024-03-01 03:14:05,653] [INFO][0m -         custom_black_list : ['reduce_sum', 'c_softmax_with_cross_entropy', 'elementwise_div'][0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         custom_white_list : ['lookup_table', 'lookup_table_v2'][0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         dtype : float16[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         enable : True[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         level : o2[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         scale_loss : 32768.0[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -     num_train_epochs : 1[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -     save_load : [0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         ckpt_dir : None[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         output_dir : ./output[0m
[32m[2024-03-01 03:14:05,654] [INFO][0m -         save_epoch : 1[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -         save_steps : 9223372036854775807[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -     strategy : {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 32768.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': ['lookup_table', 'lookup_table_v2'], 'custom_black_list': ['reduce_sum', 'c_softmax_with_cross_entropy', 'elementwise_div'], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': False}","sharding":"{'enable': False, 'stage': 1, 'degree': 1, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': False, 'k_steps': 1, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': '1F1B', 'vpp_degree': 1, 'vpp_seg_method': '', 'micro_batch_size': 1, 'accumulate_steps': 2, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': True}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': False}","sp_optimization":"{'enable': False}",}[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -     test_iters : 100[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -     verbose : 2[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m - FusedPasses : [0m
[32m[2024-03-01 03:14:05,655] [INFO][0m - Global : [0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -     device : gpu[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -     global_batch_size : 4[0m
[32m[2024-03-01 03:14:05,655] [INFO][0m -     local_batch_size : 2[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     micro_batch_size : 1[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     seed : 1024[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m - Model : [0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     attention_probs_dropout_prob : 0[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     ffn_hidden_size : 4096[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     fuse_attn_qkv : True[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     fused_softmax_with_triangular : True[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     hidden_dropout_prob : 0[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     hidden_size : 1024[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     initializer_range : 0.02[0m
[32m[2024-03-01 03:14:05,656] [INFO][0m -     max_position_embeddings : 1024[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     module : GPTModuleAuto[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     name : GPT[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     num_attention_heads : 16[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     num_layers : 24[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     recompute_granularity : full[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     scale_qk_by_layer_num : True[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     sequence_parallel : False[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     type_vocab_size : 16[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     use_flash_attn : False[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     use_recompute : True[0m
[32m[2024-03-01 03:14:05,657] [INFO][0m -     vocab_size : 50304[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -     vocab_size_divisible_unit : 128[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m - Optimizer : [0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -     beta1 : 0.9[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -     beta2 : 0.999[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -     epsilon : 1e-08[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -     grad_clip : [0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -         clip_norm : 1.0[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -         name : ClipGradByGlobalNorm[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -     lr : [0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -         decay_steps : 1440000[0m
[32m[2024-03-01 03:14:05,658] [INFO][0m -         max_lr : 5e-05[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -         min_lr : 1e-05[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -         name : CosineAnnealingWithWarmupDecay[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -         use_increments : True[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -         warmup_rate : 0.01[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -     name : AdamW[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -     weight_decay : 0.01[0m
[32m[2024-03-01 03:14:05,659] [INFO][0m - Profiler_auto : [0m
[32m[2024-03-01 03:14:05,659] [INFO][0m -     memory_stats : True[0m
[32m[2024-03-01 03:14:06,190] [INFO][0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:06,191] [INFO][0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:06,191] [INFO][0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json[0m
[32m[2024-03-01 03:14:06,191] [INFO][0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt[0m
[35m[2024-03-01 03:14:08,855] [DEBUG][0m - build dataset(<ppfleetx.data.dataset.gpt_dataset.GPTDataset object at 0x7fd7cff31b80>) success...[0m
[32m[2024-03-01 03:14:08,858] [INFO][0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:08,859] [INFO][0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.[0m
[32m[2024-03-01 03:14:08,859] [INFO][0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json[0m
[32m[2024-03-01 03:14:08,859] [INFO][0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt[0m
[35m[2024-03-01 03:14:09,026] [DEBUG][0m - build dataset(<ppfleetx.data.dataset.gpt_dataset.GPTDataset object at 0x7fd7f07e9490>) success...[0m
[32m[2024-03-01 03:14:09,027] [INFO][0m - run with paddle 0.0.0, commit id ab0f50d8[0m
[35m[2024-03-01 03:14:09,029] [DEBUG][0m - build optimizer (Weight Decay, params: ) success..[0m
[2024-03-01 03:14:09,031] [    INFO] cluster.py:914 - Node Count: 1, Local Device Size: 4, GPU Model: NVIDIA GeForce GTX 1080 Ti, GPU Memory: 11GB, World size: 4, EndPoint: 172.17.0.3:58859.
[2024-03-01 03:14:09,032] [    INFO] engine.py:202 - Distribute training by paddle.distributed.launch
[2024-03-01 03:14:09,032] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
[2024-03-01 03:14:09,032] [ WARNING] fleet.py:272 - The dygraph parallel environment has been initialized.
[2024-03-01 03:14:09,032] [ WARNING] fleet.py:295 - The dygraph hybrid parallel environment has been initialized.
/home/workspace/PaddleNLP/model_zoo/gpt-3/ppfleetx/utils/device.py:50: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-03-01 03:14:09,051] [    INFO] engine.py:655 - Building model with 'to_static' method.
INFO 2024-03-01 03:14:09,051 helper.py:245] start to build program for mode = train.
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/paddle/utils/inplace_utils.py:31: UserWarning: In static graph mode, reshape_() is the same as reshape() and does not perform inplace operation.
  warnings.warn(
[2024-03-01 03:14:24,126] [    INFO] parallelizer_v2.py:283 - Applying AMP-float16-o2 ...
[2024-03-01 03:14:24,973] [    INFO] auto_parallel_recompute.py:392 - The excluded ops in recompute segments are:
[[], []]
[_remove_and_get_optimizer_op] op type:  check_finite_and_unscale
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  c_allreduce_max
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  check_finite_and_unscale
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  c_allreduce_max
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  concat
[_remove_and_get_optimizer_op] op type:  reduce_any
[_remove_and_get_optimizer_op] op type:  memcpy_d2h
[_remove_and_get_optimizer_op] op type:  update_loss_scaling
[_remove_and_get_optimizer_op] op type:  update_loss_scaling
[_remove_and_get_optimizer_op] op type:  fill_constant
[_remove_and_get_optimizer_op] op type:  c_allreduce_sum
[_remove_and_get_optimizer_op] op type:  sqrt
[_remove_and_get_optimizer_op] op type:  fill_constant
[_remove_and_get_optimizer_op] op type:  elementwise_max
[_remove_and_get_optimizer_op] op type:  elementwise_div
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
params_grads [(persist trainable param linear_48.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_48.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_48.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_48.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_49.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_49.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_49.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_49.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_50.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_50.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_50.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_50.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_51.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_51.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_51.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_51.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_24.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_24.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_24.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_24.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_25.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_25.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_25.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_25.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_52.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_52.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_52.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_52.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_53.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_53.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_53.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_53.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_54.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_54.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_54.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_54.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_55.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_55.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_55.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_55.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_26.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_26.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_26.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_26.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_27.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_27.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_27.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_27.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_56.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_56.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_56.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_56.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_57.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_57.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_57.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_57.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_58.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_58.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_58.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_58.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_59.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_59.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_59.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_59.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_28.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_28.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_28.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_28.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_29.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_29.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_29.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_29.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_60.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_60.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_60.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_60.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_61.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_61.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_61.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_61.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_62.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_62.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_62.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_62.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_63.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_63.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_63.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_63.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_30.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_30.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_30.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_30.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_31.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_31.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_31.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_31.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_64.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_64.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_64.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_64.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_65.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_65.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_65.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_65.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_66.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_66.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_66.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_66.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_67.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_67.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_67.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_67.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_32.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_32.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_32.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_32.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_33.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_33.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_33.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_33.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_68.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_68.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_68.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_68.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_69.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_69.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_69.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_69.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_70.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_70.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_70.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_70.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_71.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_71.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_71.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_71.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_34.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_34.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_34.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_34.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_35.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_35.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_35.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_35.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_72.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_72.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_72.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_72.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_73.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_73.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_73.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_73.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_74.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_74.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_74.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_74.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_75.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_75.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_75.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_75.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_36.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_36.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_36.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_36.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_37.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_37.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_37.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_37.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_76.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_76.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_76.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_76.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_77.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_77.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_77.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_77.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_78.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_78.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_78.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_78.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_79.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_79.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_79.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_79.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_38.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_38.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_38.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_38.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_39.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_39.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_39.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_39.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_80.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_80.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_80.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_80.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_81.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_81.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_81.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_81.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_82.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_82.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_82.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_82.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_83.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_83.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_83.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_83.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_40.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_40.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_40.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_40.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_41.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_41.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_41.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_41.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_84.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_84.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_84.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_84.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_85.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_85.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_85.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_85.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_86.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_86.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_86.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_86.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_87.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_87.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_87.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_87.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_42.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_42.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_42.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_42.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_43.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_43.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_43.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_43.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_88.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_88.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_88.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_88.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_89.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_89.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_89.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_89.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_90.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_90.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_90.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_90.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_91.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_91.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_91.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_91.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_44.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_44.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_44.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_44.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_45.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_45.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_45.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_45.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param linear_92.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), var linear_92.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)), (persist trainable param linear_92.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), var linear_92.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)), (persist trainable param linear_93.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), var linear_93.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_93.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_93.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param linear_94.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), var linear_94.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_94.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var linear_94.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_95.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), var linear_95.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)), (persist trainable param linear_95.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), var linear_95.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)), (persist trainable param layer_norm_46.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_46.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_46.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_46.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_47.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_47.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_47.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_47.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_48.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_48.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)), (persist trainable param layer_norm_48.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), var layer_norm_48.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False))]
new_params_to_grads [[persist trainable param layer_norm_24.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_24.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_24.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_24.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_48.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_48.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_48.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_48.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_49.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_49.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_49.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_49.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_25.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_25.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_25.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_25.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_50.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_50.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_50.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_50.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_51.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_51.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_51.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_51.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_26.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_26.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_26.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_26.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_52.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_52.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_52.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_52.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_53.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_53.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_53.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_53.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_27.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_27.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_27.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_27.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_54.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_54.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_54.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_54.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_55.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_55.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_55.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_55.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_28.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_28.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_28.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_28.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_56.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_56.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_56.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_56.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_57.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_57.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_57.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_57.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_29.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_29.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_29.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_29.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_58.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_58.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_58.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_58.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_59.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_59.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_59.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_59.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_30.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_30.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_30.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_30.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_60.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_60.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_60.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_60.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_61.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_61.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_61.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_61.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_31.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_31.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_31.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_31.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_62.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_62.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_62.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_62.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_63.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_63.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_63.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_63.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_32.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_32.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_32.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_32.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_64.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_64.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_64.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_64.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_65.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_65.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_65.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_65.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_33.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_33.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_33.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_33.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_66.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_66.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_66.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_66.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_67.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_67.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_67.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_67.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_34.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_34.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_34.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_34.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_68.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_68.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_68.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_68.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_69.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_69.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_69.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_69.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_35.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_35.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_35.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_35.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_70.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_70.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_70.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_70.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_71.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_71.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_71.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_71.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_36.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_36.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_36.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_36.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_72.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_72.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_72.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_72.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_73.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_73.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_73.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_73.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_37.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_37.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_37.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_37.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_74.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_74.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_74.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_74.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_75.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_75.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_75.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_75.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_38.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_38.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_38.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_38.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_76.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_76.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_76.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_76.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_77.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_77.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_77.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_77.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_39.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_39.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_39.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_39.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_78.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_78.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_78.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_78.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_79.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_79.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_79.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_79.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_40.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_40.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_40.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_40.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_80.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_80.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_80.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_80.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_81.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_81.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_81.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_81.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_41.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_41.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_41.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_41.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_82.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_82.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_82.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_82.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_83.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_83.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_83.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_83.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_42.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_42.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_42.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_42.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_84.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_84.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_84.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_84.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_85.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_85.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_85.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_85.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_43.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_43.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_43.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_43.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_86.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_86.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_86.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_86.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_87.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_87.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_87.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_87.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_44.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_44.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_44.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_44.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_88.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_88.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_88.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_88.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_89.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_89.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_89.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_89.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_45.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_45.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_45.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_45.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_90.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_90.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_90.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_90.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_91.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_91.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_91.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_91.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_46.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_46.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_46.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_46.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_92.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False), persist var linear_92.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)], [persist trainable param linear_92.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False), persist var linear_92.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)], [persist trainable param linear_93.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False), persist var linear_93.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_93.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_93.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_47.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_47.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_47.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_47.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param linear_94.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False), persist var linear_94.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_94.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var linear_94.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_95.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False), persist var linear_95.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)], [persist trainable param linear_95.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False), persist var linear_95.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)], [persist trainable param layer_norm_48.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_48.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)], [persist trainable param layer_norm_48.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False), persist var layer_norm_48.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)]]
['linear_48.w_0@GRAD', 'linear_48.b_0@GRAD', 'linear_49.w_0@GRAD', 'linear_49.b_0@GRAD', 'linear_50.w_0@GRAD', 'linear_50.b_0@GRAD', 'linear_51.w_0@GRAD', 'linear_51.b_0@GRAD', 'layer_norm_24.w_0@GRAD', 'layer_norm_24.b_0@GRAD', 'layer_norm_25.w_0@GRAD', 'layer_norm_25.b_0@GRAD', 'linear_52.w_0@GRAD', 'linear_52.b_0@GRAD', 'linear_53.w_0@GRAD', 'linear_53.b_0@GRAD', 'linear_54.w_0@GRAD', 'linear_54.b_0@GRAD', 'linear_55.w_0@GRAD', 'linear_55.b_0@GRAD', 'layer_norm_26.w_0@GRAD', 'layer_norm_26.b_0@GRAD', 'layer_norm_27.w_0@GRAD', 'layer_norm_27.b_0@GRAD', 'linear_56.w_0@GRAD', 'linear_56.b_0@GRAD', 'linear_57.w_0@GRAD', 'linear_57.b_0@GRAD', 'linear_58.w_0@GRAD', 'linear_58.b_0@GRAD', 'linear_59.w_0@GRAD', 'linear_59.b_0@GRAD', 'layer_norm_28.w_0@GRAD', 'layer_norm_28.b_0@GRAD', 'layer_norm_29.w_0@GRAD', 'layer_norm_29.b_0@GRAD', 'linear_60.w_0@GRAD', 'linear_60.b_0@GRAD', 'linear_61.w_0@GRAD', 'linear_61.b_0@GRAD', 'linear_62.w_0@GRAD', 'linear_62.b_0@GRAD', 'linear_63.w_0@GRAD', 'linear_63.b_0@GRAD', 'layer_norm_30.w_0@GRAD', 'layer_norm_30.b_0@GRAD', 'layer_norm_31.w_0@GRAD', 'layer_norm_31.b_0@GRAD', 'linear_64.w_0@GRAD', 'linear_64.b_0@GRAD', 'linear_65.w_0@GRAD', 'linear_65.b_0@GRAD', 'linear_66.w_0@GRAD', 'linear_66.b_0@GRAD', 'linear_67.w_0@GRAD', 'linear_67.b_0@GRAD', 'layer_norm_32.w_0@GRAD', 'layer_norm_32.b_0@GRAD', 'layer_norm_33.w_0@GRAD', 'layer_norm_33.b_0@GRAD', 'linear_68.w_0@GRAD', 'linear_68.b_0@GRAD', 'linear_69.w_0@GRAD', 'linear_69.b_0@GRAD', 'linear_70.w_0@GRAD', 'linear_70.b_0@GRAD', 'linear_71.w_0@GRAD', 'linear_71.b_0@GRAD', 'layer_norm_34.w_0@GRAD', 'layer_norm_34.b_0@GRAD', 'layer_norm_35.w_0@GRAD', 'layer_norm_35.b_0@GRAD', 'linear_72.w_0@GRAD', 'linear_72.b_0@GRAD', 'linear_73.w_0@GRAD', 'linear_73.b_0@GRAD', 'linear_74.w_0@GRAD', 'linear_74.b_0@GRAD', 'linear_75.w_0@GRAD', 'linear_75.b_0@GRAD', 'layer_norm_36.w_0@GRAD', 'layer_norm_36.b_0@GRAD', 'layer_norm_37.w_0@GRAD', 'layer_norm_37.b_0@GRAD', 'linear_76.w_0@GRAD', 'linear_76.b_0@GRAD', 'linear_77.w_0@GRAD', 'linear_77.b_0@GRAD', 'linear_78.w_0@GRAD', 'linear_78.b_0@GRAD', 'linear_79.w_0@GRAD', 'linear_79.b_0@GRAD', 'layer_norm_38.w_0@GRAD', 'layer_norm_38.b_0@GRAD', 'layer_norm_39.w_0@GRAD', 'layer_norm_39.b_0@GRAD', 'linear_80.w_0@GRAD', 'linear_80.b_0@GRAD', 'linear_81.w_0@GRAD', 'linear_81.b_0@GRAD', 'linear_82.w_0@GRAD', 'linear_82.b_0@GRAD', 'linear_83.w_0@GRAD', 'linear_83.b_0@GRAD', 'layer_norm_40.w_0@GRAD', 'layer_norm_40.b_0@GRAD', 'layer_norm_41.w_0@GRAD', 'layer_norm_41.b_0@GRAD', 'linear_84.w_0@GRAD', 'linear_84.b_0@GRAD', 'linear_85.w_0@GRAD', 'linear_85.b_0@GRAD', 'linear_86.w_0@GRAD', 'linear_86.b_0@GRAD', 'linear_87.w_0@GRAD', 'linear_87.b_0@GRAD', 'layer_norm_42.w_0@GRAD', 'layer_norm_42.b_0@GRAD', 'layer_norm_43.w_0@GRAD', 'layer_norm_43.b_0@GRAD', 'linear_88.w_0@GRAD', 'linear_88.b_0@GRAD', 'linear_89.w_0@GRAD', 'linear_89.b_0@GRAD', 'linear_90.w_0@GRAD', 'linear_90.b_0@GRAD', 'linear_91.w_0@GRAD', 'linear_91.b_0@GRAD', 'layer_norm_44.w_0@GRAD', 'layer_norm_44.b_0@GRAD', 'layer_norm_45.w_0@GRAD', 'layer_norm_45.b_0@GRAD', 'linear_92.w_0@GRAD', 'linear_92.b_0@GRAD', 'linear_93.w_0@GRAD', 'linear_93.b_0@GRAD', 'linear_94.w_0@GRAD', 'linear_94.b_0@GRAD', 'linear_95.w_0@GRAD', 'linear_95.b_0@GRAD', 'layer_norm_46.w_0@GRAD', 'layer_norm_46.b_0@GRAD', 'layer_norm_47.w_0@GRAD', 'layer_norm_47.b_0@GRAD', 'layer_norm_48.w_0@GRAD', 'layer_norm_48.b_0@GRAD'] 2233
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  c_allreduce_sum
op type:  scale
op type:  scale
op type:  check_finite_and_unscale
op type:  cast
op type:  c_allreduce_max
op type:  cast
op type:  check_finite_and_unscale
op type:  cast
op type:  c_allreduce_max
op type:  cast
op type:  concat
op type:  reduce_any
op type:  memcpy_d2h
op type:  update_loss_scaling
op type:  update_loss_scaling
op type:  fill_constant
op type:  c_allreduce_sum
op type:  sqrt
op type:  fill_constant
op type:  elementwise_max
op type:  elementwise_div
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  elementwise_mul
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var input0 : LOD_TENSOR.shape(1, 1024).dtype(int64).stop_gradient(True)
    var input1 : LOD_TENSOR.shape(1, 1024).dtype(int64).stop_gradient(True)
    var tmp_24 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_24.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_24.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_24.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_24.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_24.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_48.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_48.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_48.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_48.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_24.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_24.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_12.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_48.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_48.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_49.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_49.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_50.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_50.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_24.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_24.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_25.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_12.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_25.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_51.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_51.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_25.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_25.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_49.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_49.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_49.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_49.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_25.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_25.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_25.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_25.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_25.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_50.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_50.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_50.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_50.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_12.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_51.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_51.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_51.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_51.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_26.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_26.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_26.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_26.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_26.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_52.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_52.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_52.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_52.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_26.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_26.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_13.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_52.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_52.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_53.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_53.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_54.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_54.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_26.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_26.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_27.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_13.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_27.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_55.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_55.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_27.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_27.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_53.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_53.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_53.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_53.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_27.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_27.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_27.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_27.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_27.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_54.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_54.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_54.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_54.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_13.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_55.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_55.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_55.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_55.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_28 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_28.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_28.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_28.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_28.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_28.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_56.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_56.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_56.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_56.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_28.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_28.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_14.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_56.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_56.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_57.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_57.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_58.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_58.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_28.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_28.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_29.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_14.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_29.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_59.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_59.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_29.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_29.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_57.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_57.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_57.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_57.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_29.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_29.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_29.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_29.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_29.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_58.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_58.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_58.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_58.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_14.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_59.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_59.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_59.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_59.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_30.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_30.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_30.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_30.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_30.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_60.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_60.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_60.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_60.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_30.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_30.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_15.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_60.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_60.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_61.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_61.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_62.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_62.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_30.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_30.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_31.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_15.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_31.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_63.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_63.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_31.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_31.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_61.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_61.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_61.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_61.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_31.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_31.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_31.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_31.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_31.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_62.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_62.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_62.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_62.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_15.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_63.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_63.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_63.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_63.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_32.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_32.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_32.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_32.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_32.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_64.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_64.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_64.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_64.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_32.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_32.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_16.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_64.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_64.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_65.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_65.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_66.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_66.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_32.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_32.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_33.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_16.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_33.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_67.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_67.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_33.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_33.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_65.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_65.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_65.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_65.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_33.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_33.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_33.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_33.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_33.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_66.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_66.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_66.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_66.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_16.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_67.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_67.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_67.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_67.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_34.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_34.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_34.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_34.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_34.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_68.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_68.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_68.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_68.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_34.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_34.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_17.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_68.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_68.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_69.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_69.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_70.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_70.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_34.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_34.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_35.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_17.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_35.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_71.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_71.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_35.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_35.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_69.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_69.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_69.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_69.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_35.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_35.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_35.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_35.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_35.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_70.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_70.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_70.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_70.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_17.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_71.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_71.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_71.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_71.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_36.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_36.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_36.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_36.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_36.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_72.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_72.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_72.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_72.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_36.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_36.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_18.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_72.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_72.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_73.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_73.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_74.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_74.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_36.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_36.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_37.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_18.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_37.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_75.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_75.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_37.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_37.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_73.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_73.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_73.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_73.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_37 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_37.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_37.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_37.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_37.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_37.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_74.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_74.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_74.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_74.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_18.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_75.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_75.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_75.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_75.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_38.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_38.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_38.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_38.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_38.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_76.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_76.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_76.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_76.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_38.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_38.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_19.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_76.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_76.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_77.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_77.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_78.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_78.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_38.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_38.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_39.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_19.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_39.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_79.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_79.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_39.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_39.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_77.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_77.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_77.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_77.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_39.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_39.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_39.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_39.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_39.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_78.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_78.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_78.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_78.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_19.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_79.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_79.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_79.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_79.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_40.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_40.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_40.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_40.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_40.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_80.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_80.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_80.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_80.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_40.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_40.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_20.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_80.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_80.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_81.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_81.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_82.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_82.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_40.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_40.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_41.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_20.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_41.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_83.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_83.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_41.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_41.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_81.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_81.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_81.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_81.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_41.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_41.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_41.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_41.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_41.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_82.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_82.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_82.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_82.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_20.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_83.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_83.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_83.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_83.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_42 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_42.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_42.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_42.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_42.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_42.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_84.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_84.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_84.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_84.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_42.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_42.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_21.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_84.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_84.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_85.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_85.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_86.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_86.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_42.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_42.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_43.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_21.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_43.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_87.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_87.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_43.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_43.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_85.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_85.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_85.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_85.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_43.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_43.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_43.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_43.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_43.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_86.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_86.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_86.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_86.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_21.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_87.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_87.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_87.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_87.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_44.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_44.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_44.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_44.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_44.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_88.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_88.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_88.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_88.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_44.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_44.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_22.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_88.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_88.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_89.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_89.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_90.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_90.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_44.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_44.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_45.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_22.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_45.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_91.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_91.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_45.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_45.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_89.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_89.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_89.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_89.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_45.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_45.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_45.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_45.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_45.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_90.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_90.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_90.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_90.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_22.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_91.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_91.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_91.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_91.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_46 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_46.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_46.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_46.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_46.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_46.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_92.w_0 : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var linear_92.tmp_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    persist trainable param linear_92.b_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var linear_92.tmp_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_46.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_46.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_23.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_92.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_92.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_93.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_93.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_94.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_94.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_46.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_46.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_47.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_23.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_47.tmp_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_95.tmp_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_95.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_47.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_47.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    persist trainable param linear_93.w_0 : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var linear_93.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_93.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_93.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_47 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_47.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_47.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_47.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_47.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_47.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_94.w_0 : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var linear_94.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_94.b_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_94.tmp_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_23.tmp_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_95.w_0 : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_95.tmp_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param linear_95.b_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var linear_95.tmp_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_48 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    persist trainable param layer_norm_48.b_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist trainable param layer_norm_48.w_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_48.tmp_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_48.tmp_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_48.tmp_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_48.tmp_0 : LOD_TENSOR.shape(1, 1024, 50304).dtype(float16).stop_gradient(False)
    var label0 : LOD_TENSOR.shape(1, 1024).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, 1, 1024).dtype(int64).stop_gradient(True)
    var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(1, 1024, 1).dtype(float16).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(1, 1024, 50304).dtype(float16).stop_gradient(False)
    var label1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var label1.cast_fp16 : LOD_TENSOR.shape(1, 1024).dtype(float16).stop_gradient(True)
    var reshape2_48.tmp_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(True)
    var reshape2_48.tmp_1 : LOD_TENSOR.shape(0, 1, 1024).dtype(float16).stop_gradient(True)
    var reshape2_49.tmp_0 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_49.tmp_1 : LOD_TENSOR.shape(0, 1, 1024, 1).dtype(float16).stop_gradient(False)
    var tmp_49 : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var tmp_49.cast_fp32 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var reshape2_48.tmp_0.cast_fp32 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    var sum_1.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    var tmp_50 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var scaled_loss_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var scaled_loss_1@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var tmp_50@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var sum_0.tmp_0@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var tmp_49.cast_fp32@GRAD_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_49@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_49.tmp_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1).dtype(float16).stop_gradient(False)
    var matmul_v2_48.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 50304).dtype(float16).stop_gradient(False)
    var layer_norm_48.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_48.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_48.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_48@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_46.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_46.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_46.tmp_2.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_92.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_92.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_46.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_46.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_23.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_2.subprog_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_92.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_92.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_93.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_93.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_94.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_94.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_46.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_46.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_47.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_23.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_47.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_95.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_95.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_47.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_47.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_93.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_93.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_47.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_47.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_47.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_47.tmp_2.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_94.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_94.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_23.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_95.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_95.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_47@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_95.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_95.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_95.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_95.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_94.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_94.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_94.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_47.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_94.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_47.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_47.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_47@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_47@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_46@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_93.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_93.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_93.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_47.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_93.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_95.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_47.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_94.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_47.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_46.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_46.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_93.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_92.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_46.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_92.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_92.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_92.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_46.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_92.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_46.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_46.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_46@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_44.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_44.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_44.tmp_2.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_88.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_88.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_44.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_44.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_22.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_2.subprog_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_88.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_88.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_89.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_89.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_90.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_90.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_44.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_44.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_45.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_22.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_45.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_91.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_91.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_45.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_45.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_89.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_89.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_45.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_45.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_45.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_45.tmp_2.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_90.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_90.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_22.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_91.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_91.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_46@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_91.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_91.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_91.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_91.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_90.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_90.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_90.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_45.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_90.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_45.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_45.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_44@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_89.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_89.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_89.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_45.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_89.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_91.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_45.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_90.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_45.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_44.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_44.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_89.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_88.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_44.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_88.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_88.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_88.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_44.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_88.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_44.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_44.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_44@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_42.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_42.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_42.tmp_2.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_84.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_84.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_42.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_42.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_21.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_2.subprog_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_84.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_84.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_85.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_85.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_86.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_86.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_42.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_42.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_43.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_21.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_43.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_87.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_87.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_43.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_43.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_85.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_85.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_43.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_43.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_43.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_43.tmp_2.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_86.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_86.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_21.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_87.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_87.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_44@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_87.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_87.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_87.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_87.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_86.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_86.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_86.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_43.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_86.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_43.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_43.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_42@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_85.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_85.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_85.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_43.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_85.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_87.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_43.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_86.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_43.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_42.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_42.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_85.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_84.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_42.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_84.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_84.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_84.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_42.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_84.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_42.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_42.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_42@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_40.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_40.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_40.tmp_2.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_80.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_80.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_40.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_40.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_20.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_2.subprog_3 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_80.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_80.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_81.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_81.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_82.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_82.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_40.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_40.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_41.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_20.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_41.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_83.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_83.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_41.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_41.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_81.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_81.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_41.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_41.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_41.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_41.tmp_2.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_82.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_82.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_20.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_83.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_83.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_42@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_41@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_83.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_83.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_83.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_83.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_82.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_82.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_82.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_41.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_82.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_41.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_41.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_41@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_40@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_81.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_81.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_81.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_41.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_81.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_83.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_41.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_82.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_41.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_40.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_40.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_81.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_80.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_40.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_80.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_80.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_80.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_40.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_80.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_40.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_40.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_40@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_38.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_38.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_38.tmp_2.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_76.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_76.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_38.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_38.tmp_1.subprog_4 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_19.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_2.subprog_4 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_76.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_76.tmp_1.subprog_4 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_77.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_77.tmp_1.subprog_4 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_78.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_78.tmp_1.subprog_4 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_38.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_38.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_39.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_19.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_39.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_79.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_79.tmp_1.subprog_4 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_39.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_39.tmp_1.subprog_4 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_77.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_77.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_39.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_39.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_39.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_39.tmp_2.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_78.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_78.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_19.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_79.tmp_0.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_79.tmp_1.subprog_4 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_79.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_79.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_79.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_79.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_78.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_78.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_78.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_39.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_78.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_39.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_39.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_38@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_77.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_77.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_77.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_39.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_77.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_79.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_39.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_78.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_39.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_38.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_38.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_77.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_76.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_38.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_76.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_76.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_76.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_38.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_76.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_38.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_38.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_38@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_36.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_36.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_36.tmp_2.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_72.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_72.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_36.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_36.tmp_1.subprog_5 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_18.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_2.subprog_5 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_72.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_72.tmp_1.subprog_5 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_73.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_73.tmp_1.subprog_5 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_74.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_74.tmp_1.subprog_5 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_36.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_36.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_37.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_18.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_37.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_75.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_75.tmp_1.subprog_5 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_37.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_37.tmp_1.subprog_5 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_73.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_73.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_37.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_37.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_37.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_37.tmp_2.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_74.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_74.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_18.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_75.tmp_0.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_75.tmp_1.subprog_5 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_37@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_75.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_75.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_75.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_75.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_74.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_74.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_74.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_37.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_74.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_37.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_37.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_37@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_36@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_73.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_73.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_73.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_37.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_73.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_75.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_37.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_74.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_37.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_36.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_36.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_73.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_72.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_36.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_72.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_72.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_72.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_36.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_72.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_36.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_36.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_36@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_34.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_34.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_34.tmp_2.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_68.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_68.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_34.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_34.tmp_1.subprog_6 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_17.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_2.subprog_6 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_68.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_68.tmp_1.subprog_6 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_69.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_69.tmp_1.subprog_6 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_70.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_70.tmp_1.subprog_6 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_34.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_34.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_35.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_17.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_35.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_71.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_71.tmp_1.subprog_6 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_35.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_35.tmp_1.subprog_6 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_69.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_69.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_35.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_35.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_35.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_35.tmp_2.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_70.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_70.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_17.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_71.tmp_0.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_71.tmp_1.subprog_6 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_35@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_71.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_71.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_71.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_71.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_70.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_70.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_70.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_35.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_70.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_35.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_35.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_35@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_34@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_69.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_69.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_69.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_35.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_69.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_71.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_35.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_70.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_35.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_34.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_34.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_69.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_68.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_34.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_68.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_68.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_68.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_34.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_68.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_34.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_34.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_34@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_32.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_32.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_32.tmp_2.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_64.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_64.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_32.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_32.tmp_1.subprog_7 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_16.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_2.subprog_7 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_64.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_64.tmp_1.subprog_7 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_65.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_65.tmp_1.subprog_7 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_66.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_66.tmp_1.subprog_7 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_32.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_32.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_33.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_16.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_33.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_67.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_67.tmp_1.subprog_7 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_33.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_33.tmp_1.subprog_7 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_65.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_65.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_33.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_33.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_33.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_33.tmp_2.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_66.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_66.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_16.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_67.tmp_0.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_67.tmp_1.subprog_7 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_33@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_67.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_67.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_67.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_67.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_66.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_66.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_66.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_33.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_66.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_33.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_33.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_33@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_65.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_65.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_65.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_33.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_65.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_67.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_33.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_66.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_33.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_32.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_32.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_65.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_64.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_32.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_64.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_64.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_64.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_32.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_64.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_32.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_32.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_30.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_30.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_30.tmp_2.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_60.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_60.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_30.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_30.tmp_1.subprog_8 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_15.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_2.subprog_8 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_60.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_60.tmp_1.subprog_8 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_61.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_61.tmp_1.subprog_8 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_62.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_62.tmp_1.subprog_8 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_30.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_30.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_31.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_15.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_31.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_63.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_63.tmp_1.subprog_8 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_31.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_31.tmp_1.subprog_8 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_61.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_61.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_31.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_31.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_31.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_31.tmp_2.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_62.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_62.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_15.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_63.tmp_0.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_63.tmp_1.subprog_8 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_31@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_63.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_63.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_63.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_63.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_62.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_62.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_62.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_31.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_62.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_31.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_31.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_31@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_30@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_61.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_61.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_61.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_31.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_61.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_63.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_31.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_62.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_31.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_30.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_30.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_61.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_60.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_30.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_60.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_60.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_60.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_30.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_60.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_30.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_30.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_30@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_28.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_28.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_28.tmp_2.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_56.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_56.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_28.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_28.tmp_1.subprog_9 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_14.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_2.subprog_9 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_56.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_56.tmp_1.subprog_9 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_57.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_57.tmp_1.subprog_9 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_58.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_58.tmp_1.subprog_9 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_28.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_28.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_29.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_14.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_29.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_59.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_59.tmp_1.subprog_9 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_29.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_29.tmp_1.subprog_9 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_57.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_57.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_29.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_29.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_29.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_29.tmp_2.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_58.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_58.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_14.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_59.tmp_0.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_59.tmp_1.subprog_9 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_29@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_59.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_59.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_59.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_59.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_58.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_58.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_58.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_29.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_58.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_29.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_29.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_29@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_28@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_57.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_57.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_57.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_29.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_57.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_59.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_29.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_58.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_29.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_28.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_28.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_57.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_56.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_28.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_56.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_56.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_56.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_28.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_56.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_28.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_28.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_28@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_26.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_26.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_26.tmp_2.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_52.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_52.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_26.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_26.tmp_1.subprog_10 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_13.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_2.subprog_10 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_52.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_52.tmp_1.subprog_10 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_53.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_53.tmp_1.subprog_10 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_54.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_54.tmp_1.subprog_10 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_26.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_26.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_27.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_13.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_27.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_55.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_55.tmp_1.subprog_10 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_27.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_27.tmp_1.subprog_10 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_53.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_53.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_27.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_27.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_27.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_27.tmp_2.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_54.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_54.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_13.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_55.tmp_0.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_55.tmp_1.subprog_10 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_55.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_55.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_55.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_55.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_54.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_54.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_54.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_27.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_54.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_27.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_27.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_26@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_53.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_53.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_53.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_53.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_55.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_54.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_53.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_52.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_52.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_52.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_52.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_26.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_52.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_26.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_26.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_26@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_24.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_24.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_24.tmp_2.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_48.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_48.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var reshape2_24.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var reshape2_24.tmp_1.subprog_11 : LOD_TENSOR.shape(0, 1, 1024, 3072).dtype(float16).stop_gradient(False)
    var split_12.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_2.subprog_11 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_48.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_48.tmp_1.subprog_11 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_49.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_49.tmp_1.subprog_11 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_50.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_50.tmp_1.subprog_11 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var scale_24.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_24.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_25.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_12.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_25.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_51.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var transpose_51.tmp_1.subprog_11 : LOD_TENSOR.shape(0, 1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var reshape2_25.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var reshape2_25.tmp_1.subprog_11 : LOD_TENSOR.shape(0, 1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var linear_49.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_49.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_25.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var layer_norm_25.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_25.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024).dtype(float32).stop_gradient(True)
    var layer_norm_25.tmp_2.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_50.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_50.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var gelu_12.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_51.tmp_0.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_51.tmp_1.subprog_11 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_25@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_51.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_51.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_51.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var gelu_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_51.w_0@GRAD : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    var linear_50.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_50.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 4096).dtype(float16).stop_gradient(False)
    var linear_50.b_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var layer_norm_25.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_50.w_0@GRAD : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    var layer_norm_25.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_25.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_25@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_24@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_49.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_49.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_49.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    var reshape2_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_49.w_0@GRAD : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_51.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var fused_softmax_mask_upper_triangle_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var transpose_50.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var scale_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var matmul_v2_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 1024).dtype(float16).stop_gradient(False)
    var scale_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_49.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var transpose_48.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 1024, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var split_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 16, 192).dtype(float16).stop_gradient(False)
    var linear_48.tmp_1@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_48.tmp_0@GRAD : LOD_TENSOR.shape(1, 1024, 3072).dtype(float16).stop_gradient(False)
    var linear_48.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_24.tmp_2@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var linear_48.w_0@GRAD : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_24.b_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var layer_norm_24.w_0@GRAD : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    var tmp_24@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(1, 1024, 1024).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp32_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp32_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(2,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var opt_opt_squared_l2_norm_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_19.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_20.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_21.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_22.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_23.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_24.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_25.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_27.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_28.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_29.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_30.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_32.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_34.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_35.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_36.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_39.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_40.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_41.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_42.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_43.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_44.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_45.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_46.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_47.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_48.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_49.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_50.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_51.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_52.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_53.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_54.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_55.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_56.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_57.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_58.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_59.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_60.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_61.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_62.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_63.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_64.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_65.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_66.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_67.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_68.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_69.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_70.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_71.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_72.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_73.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_74.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_75.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_76.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_77.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_78.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_79.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_80.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_81.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_82.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_83.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_84.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_85.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_86.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_87.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_88.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_89.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_90.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_91.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_92.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_93.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_94.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_95.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_96.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_97.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_98.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_99.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_100.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_101.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_102.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_103.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_104.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_105.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_106.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_107.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_108.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_109.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_110.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_111.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_112.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_113.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_114.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_115.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_116.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_117.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_118.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_119.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_120.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_121.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_122.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_123.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_124.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_125.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_126.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_127.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_128.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_129.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_130.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_131.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_132.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_133.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_134.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_135.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_136.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_137.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_138.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_139.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_140.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_141.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_142.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_143.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_144.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_145.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_sum_2.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_tmp_99 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_100 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_101 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_102 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_103 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_104 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_105 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_106 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_107 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_108 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_109 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_110 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_111 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_112 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_113 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_114 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_115 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_116 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_117 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_118 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_119 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_120 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_121 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_122 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_123 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_124 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_125 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_126 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_127 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_128 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_129 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_130 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_131 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_132 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_133 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_134 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_135 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_136 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_137 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_138 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_139 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_140 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_141 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_142 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_143 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_144 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_145 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_146 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_147 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_148 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_149 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_150 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_151 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_152 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_153 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_154 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_155 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_156 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_157 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_158 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_159 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_160 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_161 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_162 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_163 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_164 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_165 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_166 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_167 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_168 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_169 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_170 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_171 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_172 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_173 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_174 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_175 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_176 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_177 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_178 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_179 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_180 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_181 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_182 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_183 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_184 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_185 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_186 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_187 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_188 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_189 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_190 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_191 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_192 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_193 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_194 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var linear_48.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_48.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_48.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_48.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_48.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_48.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_48.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_48.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_48.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_48.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_49.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_49.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_49.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_49.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_49.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_49.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_49.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_49.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_49.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_49.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_50.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_50.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_50.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_50.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_50.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_50.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_50.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_50.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_50.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_50.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_51.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_51.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_51.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_51.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_51.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_51.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_51.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_51.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_51.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_51.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_52.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_52.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_52.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_52.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_52.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_52.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_52.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_52.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_52.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_52.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_53.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_53.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_53.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_53.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_53.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_53.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_53.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_53.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_53.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_53.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_54.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_54.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_54.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_54.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_54.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_54.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_54.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_54.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_54.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_54.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_55.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_55.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_55.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_55.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_55.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_55.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_55.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_55.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_55.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_55.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_56.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_56.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_56.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_56.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_56.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_56.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_56.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_56.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_56.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_56.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_57.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_57.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_57.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_57.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_57.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_57.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_57.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_57.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_57.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_57.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_58.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_58.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_58.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_58.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_58.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_58.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_58.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_58.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_58.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_58.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_59.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_59.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_59.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_59.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_59.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_59.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_59.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_59.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_59.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_59.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_60.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_60.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_60.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_60.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_60.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_60.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_60.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_60.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_60.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_60.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_61.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_61.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_61.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_61.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_61.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_61.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_61.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_61.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_61.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_61.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_62.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_62.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_62.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_62.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_62.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_62.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_62.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_62.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_62.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_62.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_63.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_63.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_63.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_63.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_63.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_63.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_63.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_63.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_63.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_63.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_64.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_64.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_64.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_64.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_64.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_64.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_64.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_64.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_64.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_64.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_65.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_65.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_65.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_65.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_65.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_65.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_65.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_65.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_65.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_65.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_66.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_66.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_66.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_66.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_66.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_66.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_66.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_66.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_66.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_66.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_67.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_67.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_67.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_67.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_67.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_67.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_67.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_67.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_67.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_67.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_68.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_68.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_68.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_68.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_68.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_68.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_68.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_68.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_68.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_68.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_69.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_69.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_69.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_69.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_69.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_69.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_69.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_69.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_69.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_69.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_70.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_70.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_70.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_70.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_70.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_70.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_70.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_70.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_70.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_70.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_71.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_71.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_71.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_71.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_71.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_71.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_71.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_71.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_71.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_71.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_72.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_72.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_72.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_72.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_72.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_72.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_72.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_72.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_72.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_72.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_73.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_73.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_73.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_73.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_73.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_73.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_73.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_73.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_73.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_73.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_74.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_74.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_74.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_74.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_74.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_74.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_74.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_74.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_74.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_74.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_75.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_75.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_75.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_75.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_75.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_75.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_75.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_75.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_75.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_75.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_76.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_76.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_76.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_76.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_76.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_76.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_76.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_76.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_76.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_76.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_77.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_77.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_77.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_77.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_77.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_77.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_77.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_77.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_77.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_77.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_78.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_78.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_78.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_78.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_78.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_78.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_78.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_78.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_78.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_78.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_79.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_79.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_79.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_79.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_79.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_79.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_79.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_79.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_79.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_79.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_80.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_80.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_80.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_80.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_80.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_80.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_80.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_80.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_80.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_80.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_81.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_81.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_81.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_81.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_81.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_81.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_81.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_81.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_81.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_81.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_82.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_82.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_82.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_82.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_82.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_82.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_82.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_82.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_82.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_82.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_83.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_83.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_83.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_83.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_83.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_83.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_83.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_83.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_83.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_83.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_84.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_84.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_84.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_84.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_84.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_84.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_84.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_84.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_84.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_84.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_85.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_85.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_85.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_85.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_85.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_85.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_85.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_85.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_85.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_85.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_86.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_86.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_86.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_86.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_86.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_86.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_86.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_86.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_86.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_86.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_87.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_87.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_87.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_87.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_87.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_87.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_87.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_87.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_87.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_87.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_88.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_88.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_88.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_88.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_88.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_88.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_88.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_88.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_88.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_88.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_89.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_89.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_89.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_89.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_89.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_89.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_89.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_89.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_89.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_89.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_90.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_90.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_90.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_90.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_90.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_90.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_90.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_90.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_90.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_90.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_91.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_91.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_91.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_91.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_91.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_91.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_91.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_91.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_91.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_91.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_92.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(True)
    persist var linear_92.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_92.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 3072).dtype(float32).stop_gradient(False)
    persist var linear_92.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_92.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_92.b_0_fp32_master_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(True)
    persist var linear_92.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_92.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var linear_92.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_92.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_93.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(True)
    persist var linear_93.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_93.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 1024).dtype(float32).stop_gradient(False)
    persist var linear_93.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_93.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_93.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_93.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_93.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_93.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_93.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_94.w_0_fp32_master_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(True)
    persist var linear_94.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_94.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024, 4096).dtype(float32).stop_gradient(False)
    persist var linear_94.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_94.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_94.b_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var linear_94.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_94.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_94.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_94.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_95.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(True)
    persist var linear_95.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_95.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 1024).dtype(float32).stop_gradient(False)
    persist var linear_95.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_95.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_95.b_0_fp32_master_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(True)
    persist var linear_95.b_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_95.b_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_95.b_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_95.b_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.w_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.w_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.b_0_moment1_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.b_0_moment2_0 : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var embedding_0.w_0@recv_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@RESHARD_0 : LOD_TENSOR.shape(50304, 1024).dtype(float16).stop_gradient(False)
    persist var layer_norm_24.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_24.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_48.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_48.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_49.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_49.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_25.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_25.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_50.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_50.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_51.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_51.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_26.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_26.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_52.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_52.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_53.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_53.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_27.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_27.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_54.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_54.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_55.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_55.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_28.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_28.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_56.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_56.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_57.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_57.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_29.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_29.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_58.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_58.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_59.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_59.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_30.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_30.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_60.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_60.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_61.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_61.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_31.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_31.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_62.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_62.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_63.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_63.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_32.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_32.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_64.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_64.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_65.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_65.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_33.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_33.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_66.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_66.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_67.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_67.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_34.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_34.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_68.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_68.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_69.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_69.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_35.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_35.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_70.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_70.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_71.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_71.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_36.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_36.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_72.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_72.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_73.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_73.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_37.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_37.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_74.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_74.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_75.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_75.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_38.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_38.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_76.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_76.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_77.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_77.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_39.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_39.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_78.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_78.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_79.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_79.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_40.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_40.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_80.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_80.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_81.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_81.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_41.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_41.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_82.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_82.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_83.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_83.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_42.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_42.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_84.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_84.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_85.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_85.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_43.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_43.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_86.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_86.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_87.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_87.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_44.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_44.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_88.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_88.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_89.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_89.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_45.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_45.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_90.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_90.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_91.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_91.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_46.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_46.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_92.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 3072).dtype(float16).stop_gradient(False)
    persist var linear_92.b_0@GRAD@MERGE : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    persist var linear_93.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 1024).dtype(float16).stop_gradient(False)
    persist var linear_93.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_47.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_47.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var linear_94.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024, 4096).dtype(float16).stop_gradient(False)
    persist var linear_94.b_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_95.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 1024).dtype(float16).stop_gradient(False)
    persist var linear_95.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float16).stop_gradient(False)
    persist var layer_norm_48.w_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var layer_norm_48.b_0@GRAD@MERGE : LOD_TENSOR.shape(1024,).dtype(float32).stop_gradient(False)
    persist var gradient_merge_k : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_zero : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_step : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_cond : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(True)
    var _generated_var_0 : STEP_SCOPES)

    {Out=['tmp_24']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], out_shape = [1, 1024, 1024], peer = 0, ring_id = 34, use_calc_stream = True, with_quant_attr = False)
    {Mean=['layer_norm_24.tmp_0'], Variance=['layer_norm_24.tmp_1'], Y=['layer_norm_24.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_24.b_0'], Scale=['layer_norm_24.w_0'], X=['tmp_24']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_48.tmp_0']} = matmul_v2(inputs={X=['layer_norm_24.tmp_2'], Y=['linear_48.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_48.tmp_1']} = elementwise_add(inputs={X=['linear_48.tmp_0'], Y=['linear_48.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_24.tmp_0'], XShape=['reshape2_24.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_48.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_12.tmp_0', 'split_12.tmp_1', 'split_12.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_24.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_48.tmp_0'], XShape=['transpose_48.tmp_1']} = transpose2(inputs={X=['split_12.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_49.tmp_0'], XShape=['transpose_49.tmp_1']} = transpose2(inputs={X=['split_12.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_50.tmp_0'], XShape=['transpose_50.tmp_1']} = transpose2(inputs={X=['split_12.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_24.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_48.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_24.tmp_0']} = matmul_v2(inputs={X=['scale_24.tmp_0'], Y=['transpose_49.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_25.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_24.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_12.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_25.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_25.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_12.tmp_0'], Y=['transpose_50.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_51.tmp_0'], XShape=['transpose_51.tmp_1']} = transpose2(inputs={X=['matmul_v2_25.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_25.tmp_0'], XShape=['reshape2_25.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_51.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_49.tmp_0']} = matmul_v2(inputs={X=['reshape2_25.tmp_0'], Y=['linear_49.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_49.tmp_1']} = elementwise_add(inputs={X=['linear_49.tmp_0'], Y=['linear_49.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_25']} = elementwise_add(inputs={X=['tmp_24'], Y=['linear_49.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_25.tmp_0'], Variance=['layer_norm_25.tmp_1'], Y=['layer_norm_25.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_25.b_0'], Scale=['layer_norm_25.w_0'], X=['tmp_25']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_50.tmp_0']} = matmul_v2(inputs={X=['layer_norm_25.tmp_2'], Y=['linear_50.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_50.tmp_1']} = elementwise_add(inputs={X=['linear_50.tmp_0'], Y=['linear_50.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_12.tmp_0']} = gelu(inputs={X=['linear_50.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_51.tmp_0']} = matmul_v2(inputs={X=['gelu_12.tmp_0'], Y=['linear_51.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_51.tmp_1']} = elementwise_add(inputs={X=['linear_51.tmp_0'], Y=['linear_51.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_26']} = elementwise_add(inputs={X=['tmp_25'], Y=['linear_51.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_26.tmp_0'], Variance=['layer_norm_26.tmp_1'], Y=['layer_norm_26.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_26.b_0'], Scale=['layer_norm_26.w_0'], X=['tmp_26']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_52.tmp_0']} = matmul_v2(inputs={X=['layer_norm_26.tmp_2'], Y=['linear_52.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_52.tmp_1']} = elementwise_add(inputs={X=['linear_52.tmp_0'], Y=['linear_52.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_26.tmp_0'], XShape=['reshape2_26.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_52.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_13.tmp_0', 'split_13.tmp_1', 'split_13.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_26.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_52.tmp_0'], XShape=['transpose_52.tmp_1']} = transpose2(inputs={X=['split_13.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_53.tmp_0'], XShape=['transpose_53.tmp_1']} = transpose2(inputs={X=['split_13.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_54.tmp_0'], XShape=['transpose_54.tmp_1']} = transpose2(inputs={X=['split_13.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_26.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_52.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_26.tmp_0']} = matmul_v2(inputs={X=['scale_26.tmp_0'], Y=['transpose_53.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_27.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_26.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_13.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_27.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_27.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_13.tmp_0'], Y=['transpose_54.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_55.tmp_0'], XShape=['transpose_55.tmp_1']} = transpose2(inputs={X=['matmul_v2_27.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_27.tmp_0'], XShape=['reshape2_27.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_55.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_53.tmp_0']} = matmul_v2(inputs={X=['reshape2_27.tmp_0'], Y=['linear_53.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_53.tmp_1']} = elementwise_add(inputs={X=['linear_53.tmp_0'], Y=['linear_53.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_27']} = elementwise_add(inputs={X=['tmp_26'], Y=['linear_53.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_27.tmp_0'], Variance=['layer_norm_27.tmp_1'], Y=['layer_norm_27.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_27.b_0'], Scale=['layer_norm_27.w_0'], X=['tmp_27']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_54.tmp_0']} = matmul_v2(inputs={X=['layer_norm_27.tmp_2'], Y=['linear_54.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_54.tmp_1']} = elementwise_add(inputs={X=['linear_54.tmp_0'], Y=['linear_54.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_13.tmp_0']} = gelu(inputs={X=['linear_54.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_55.tmp_0']} = matmul_v2(inputs={X=['gelu_13.tmp_0'], Y=['linear_55.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_55.tmp_1']} = elementwise_add(inputs={X=['linear_55.tmp_0'], Y=['linear_55.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_28']} = elementwise_add(inputs={X=['tmp_27'], Y=['linear_55.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_28.tmp_0'], Variance=['layer_norm_28.tmp_1'], Y=['layer_norm_28.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_28.b_0'], Scale=['layer_norm_28.w_0'], X=['tmp_28']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_56.tmp_0']} = matmul_v2(inputs={X=['layer_norm_28.tmp_2'], Y=['linear_56.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_56.tmp_1']} = elementwise_add(inputs={X=['linear_56.tmp_0'], Y=['linear_56.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_28.tmp_0'], XShape=['reshape2_28.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_56.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_14.tmp_0', 'split_14.tmp_1', 'split_14.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_28.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_56.tmp_0'], XShape=['transpose_56.tmp_1']} = transpose2(inputs={X=['split_14.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_57.tmp_0'], XShape=['transpose_57.tmp_1']} = transpose2(inputs={X=['split_14.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_58.tmp_0'], XShape=['transpose_58.tmp_1']} = transpose2(inputs={X=['split_14.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_28.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_56.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_28.tmp_0']} = matmul_v2(inputs={X=['scale_28.tmp_0'], Y=['transpose_57.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_29.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_28.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_14.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_29.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_29.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_14.tmp_0'], Y=['transpose_58.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_59.tmp_0'], XShape=['transpose_59.tmp_1']} = transpose2(inputs={X=['matmul_v2_29.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_29.tmp_0'], XShape=['reshape2_29.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_59.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_57.tmp_0']} = matmul_v2(inputs={X=['reshape2_29.tmp_0'], Y=['linear_57.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_57.tmp_1']} = elementwise_add(inputs={X=['linear_57.tmp_0'], Y=['linear_57.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_29']} = elementwise_add(inputs={X=['tmp_28'], Y=['linear_57.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_29.tmp_0'], Variance=['layer_norm_29.tmp_1'], Y=['layer_norm_29.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_29.b_0'], Scale=['layer_norm_29.w_0'], X=['tmp_29']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_58.tmp_0']} = matmul_v2(inputs={X=['layer_norm_29.tmp_2'], Y=['linear_58.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_58.tmp_1']} = elementwise_add(inputs={X=['linear_58.tmp_0'], Y=['linear_58.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_14.tmp_0']} = gelu(inputs={X=['linear_58.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_59.tmp_0']} = matmul_v2(inputs={X=['gelu_14.tmp_0'], Y=['linear_59.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_59.tmp_1']} = elementwise_add(inputs={X=['linear_59.tmp_0'], Y=['linear_59.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_30']} = elementwise_add(inputs={X=['tmp_29'], Y=['linear_59.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_30.tmp_0'], Variance=['layer_norm_30.tmp_1'], Y=['layer_norm_30.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_30.b_0'], Scale=['layer_norm_30.w_0'], X=['tmp_30']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_60.tmp_0']} = matmul_v2(inputs={X=['layer_norm_30.tmp_2'], Y=['linear_60.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_60.tmp_1']} = elementwise_add(inputs={X=['linear_60.tmp_0'], Y=['linear_60.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_30.tmp_0'], XShape=['reshape2_30.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_60.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_15.tmp_0', 'split_15.tmp_1', 'split_15.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_30.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_60.tmp_0'], XShape=['transpose_60.tmp_1']} = transpose2(inputs={X=['split_15.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_61.tmp_0'], XShape=['transpose_61.tmp_1']} = transpose2(inputs={X=['split_15.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_62.tmp_0'], XShape=['transpose_62.tmp_1']} = transpose2(inputs={X=['split_15.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_30.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_60.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_30.tmp_0']} = matmul_v2(inputs={X=['scale_30.tmp_0'], Y=['transpose_61.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_31.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_30.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_15.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_31.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_31.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_15.tmp_0'], Y=['transpose_62.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_63.tmp_0'], XShape=['transpose_63.tmp_1']} = transpose2(inputs={X=['matmul_v2_31.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_31.tmp_0'], XShape=['reshape2_31.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_63.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_61.tmp_0']} = matmul_v2(inputs={X=['reshape2_31.tmp_0'], Y=['linear_61.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_61.tmp_1']} = elementwise_add(inputs={X=['linear_61.tmp_0'], Y=['linear_61.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_31']} = elementwise_add(inputs={X=['tmp_30'], Y=['linear_61.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_31.tmp_0'], Variance=['layer_norm_31.tmp_1'], Y=['layer_norm_31.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_31.b_0'], Scale=['layer_norm_31.w_0'], X=['tmp_31']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_62.tmp_0']} = matmul_v2(inputs={X=['layer_norm_31.tmp_2'], Y=['linear_62.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_62.tmp_1']} = elementwise_add(inputs={X=['linear_62.tmp_0'], Y=['linear_62.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_15.tmp_0']} = gelu(inputs={X=['linear_62.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_63.tmp_0']} = matmul_v2(inputs={X=['gelu_15.tmp_0'], Y=['linear_63.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_63.tmp_1']} = elementwise_add(inputs={X=['linear_63.tmp_0'], Y=['linear_63.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_32']} = elementwise_add(inputs={X=['tmp_31'], Y=['linear_63.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_32.tmp_0'], Variance=['layer_norm_32.tmp_1'], Y=['layer_norm_32.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_32.b_0'], Scale=['layer_norm_32.w_0'], X=['tmp_32']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_64.tmp_0']} = matmul_v2(inputs={X=['layer_norm_32.tmp_2'], Y=['linear_64.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_64.tmp_1']} = elementwise_add(inputs={X=['linear_64.tmp_0'], Y=['linear_64.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_32.tmp_0'], XShape=['reshape2_32.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_64.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_16.tmp_0', 'split_16.tmp_1', 'split_16.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_32.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_64.tmp_0'], XShape=['transpose_64.tmp_1']} = transpose2(inputs={X=['split_16.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_65.tmp_0'], XShape=['transpose_65.tmp_1']} = transpose2(inputs={X=['split_16.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_66.tmp_0'], XShape=['transpose_66.tmp_1']} = transpose2(inputs={X=['split_16.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_32.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_64.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_32.tmp_0']} = matmul_v2(inputs={X=['scale_32.tmp_0'], Y=['transpose_65.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_33.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_32.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_16.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_33.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_33.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_16.tmp_0'], Y=['transpose_66.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_67.tmp_0'], XShape=['transpose_67.tmp_1']} = transpose2(inputs={X=['matmul_v2_33.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_33.tmp_0'], XShape=['reshape2_33.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_67.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_65.tmp_0']} = matmul_v2(inputs={X=['reshape2_33.tmp_0'], Y=['linear_65.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_65.tmp_1']} = elementwise_add(inputs={X=['linear_65.tmp_0'], Y=['linear_65.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_33']} = elementwise_add(inputs={X=['tmp_32'], Y=['linear_65.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_33.tmp_0'], Variance=['layer_norm_33.tmp_1'], Y=['layer_norm_33.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_33.b_0'], Scale=['layer_norm_33.w_0'], X=['tmp_33']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_66.tmp_0']} = matmul_v2(inputs={X=['layer_norm_33.tmp_2'], Y=['linear_66.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_66.tmp_1']} = elementwise_add(inputs={X=['linear_66.tmp_0'], Y=['linear_66.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_16.tmp_0']} = gelu(inputs={X=['linear_66.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_67.tmp_0']} = matmul_v2(inputs={X=['gelu_16.tmp_0'], Y=['linear_67.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_67.tmp_1']} = elementwise_add(inputs={X=['linear_67.tmp_0'], Y=['linear_67.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_34']} = elementwise_add(inputs={X=['tmp_33'], Y=['linear_67.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_34.tmp_0'], Variance=['layer_norm_34.tmp_1'], Y=['layer_norm_34.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_34.b_0'], Scale=['layer_norm_34.w_0'], X=['tmp_34']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_68.tmp_0']} = matmul_v2(inputs={X=['layer_norm_34.tmp_2'], Y=['linear_68.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_68.tmp_1']} = elementwise_add(inputs={X=['linear_68.tmp_0'], Y=['linear_68.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_34.tmp_0'], XShape=['reshape2_34.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_68.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_17.tmp_0', 'split_17.tmp_1', 'split_17.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_34.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_68.tmp_0'], XShape=['transpose_68.tmp_1']} = transpose2(inputs={X=['split_17.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_69.tmp_0'], XShape=['transpose_69.tmp_1']} = transpose2(inputs={X=['split_17.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_70.tmp_0'], XShape=['transpose_70.tmp_1']} = transpose2(inputs={X=['split_17.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_34.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_68.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_34.tmp_0']} = matmul_v2(inputs={X=['scale_34.tmp_0'], Y=['transpose_69.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_35.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_34.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_17.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_35.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_35.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_17.tmp_0'], Y=['transpose_70.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_71.tmp_0'], XShape=['transpose_71.tmp_1']} = transpose2(inputs={X=['matmul_v2_35.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_35.tmp_0'], XShape=['reshape2_35.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_71.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_69.tmp_0']} = matmul_v2(inputs={X=['reshape2_35.tmp_0'], Y=['linear_69.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_69.tmp_1']} = elementwise_add(inputs={X=['linear_69.tmp_0'], Y=['linear_69.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_35']} = elementwise_add(inputs={X=['tmp_34'], Y=['linear_69.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_35.tmp_0'], Variance=['layer_norm_35.tmp_1'], Y=['layer_norm_35.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_35.b_0'], Scale=['layer_norm_35.w_0'], X=['tmp_35']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_70.tmp_0']} = matmul_v2(inputs={X=['layer_norm_35.tmp_2'], Y=['linear_70.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_70.tmp_1']} = elementwise_add(inputs={X=['linear_70.tmp_0'], Y=['linear_70.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_17.tmp_0']} = gelu(inputs={X=['linear_70.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_71.tmp_0']} = matmul_v2(inputs={X=['gelu_17.tmp_0'], Y=['linear_71.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_71.tmp_1']} = elementwise_add(inputs={X=['linear_71.tmp_0'], Y=['linear_71.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_36']} = elementwise_add(inputs={X=['tmp_35'], Y=['linear_71.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_36.tmp_0'], Variance=['layer_norm_36.tmp_1'], Y=['layer_norm_36.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_36.b_0'], Scale=['layer_norm_36.w_0'], X=['tmp_36']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_72.tmp_0']} = matmul_v2(inputs={X=['layer_norm_36.tmp_2'], Y=['linear_72.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_72.tmp_1']} = elementwise_add(inputs={X=['linear_72.tmp_0'], Y=['linear_72.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_36.tmp_0'], XShape=['reshape2_36.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_72.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_18.tmp_0', 'split_18.tmp_1', 'split_18.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_36.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_72.tmp_0'], XShape=['transpose_72.tmp_1']} = transpose2(inputs={X=['split_18.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_73.tmp_0'], XShape=['transpose_73.tmp_1']} = transpose2(inputs={X=['split_18.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_74.tmp_0'], XShape=['transpose_74.tmp_1']} = transpose2(inputs={X=['split_18.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_36.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_72.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_36.tmp_0']} = matmul_v2(inputs={X=['scale_36.tmp_0'], Y=['transpose_73.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_37.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_36.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_18.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_37.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_37.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_18.tmp_0'], Y=['transpose_74.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_75.tmp_0'], XShape=['transpose_75.tmp_1']} = transpose2(inputs={X=['matmul_v2_37.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_37.tmp_0'], XShape=['reshape2_37.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_75.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_73.tmp_0']} = matmul_v2(inputs={X=['reshape2_37.tmp_0'], Y=['linear_73.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_73.tmp_1']} = elementwise_add(inputs={X=['linear_73.tmp_0'], Y=['linear_73.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_37']} = elementwise_add(inputs={X=['tmp_36'], Y=['linear_73.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_37.tmp_0'], Variance=['layer_norm_37.tmp_1'], Y=['layer_norm_37.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_37.b_0'], Scale=['layer_norm_37.w_0'], X=['tmp_37']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_74.tmp_0']} = matmul_v2(inputs={X=['layer_norm_37.tmp_2'], Y=['linear_74.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_74.tmp_1']} = elementwise_add(inputs={X=['linear_74.tmp_0'], Y=['linear_74.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_18.tmp_0']} = gelu(inputs={X=['linear_74.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_75.tmp_0']} = matmul_v2(inputs={X=['gelu_18.tmp_0'], Y=['linear_75.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_75.tmp_1']} = elementwise_add(inputs={X=['linear_75.tmp_0'], Y=['linear_75.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_38']} = elementwise_add(inputs={X=['tmp_37'], Y=['linear_75.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_38.tmp_0'], Variance=['layer_norm_38.tmp_1'], Y=['layer_norm_38.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_38.b_0'], Scale=['layer_norm_38.w_0'], X=['tmp_38']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_76.tmp_0']} = matmul_v2(inputs={X=['layer_norm_38.tmp_2'], Y=['linear_76.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_76.tmp_1']} = elementwise_add(inputs={X=['linear_76.tmp_0'], Y=['linear_76.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_38.tmp_0'], XShape=['reshape2_38.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_76.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_19.tmp_0', 'split_19.tmp_1', 'split_19.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_38.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_76.tmp_0'], XShape=['transpose_76.tmp_1']} = transpose2(inputs={X=['split_19.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_77.tmp_0'], XShape=['transpose_77.tmp_1']} = transpose2(inputs={X=['split_19.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_78.tmp_0'], XShape=['transpose_78.tmp_1']} = transpose2(inputs={X=['split_19.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_38.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_76.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_38.tmp_0']} = matmul_v2(inputs={X=['scale_38.tmp_0'], Y=['transpose_77.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_39.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_38.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_19.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_39.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_39.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_19.tmp_0'], Y=['transpose_78.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_79.tmp_0'], XShape=['transpose_79.tmp_1']} = transpose2(inputs={X=['matmul_v2_39.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_39.tmp_0'], XShape=['reshape2_39.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_79.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_77.tmp_0']} = matmul_v2(inputs={X=['reshape2_39.tmp_0'], Y=['linear_77.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_77.tmp_1']} = elementwise_add(inputs={X=['linear_77.tmp_0'], Y=['linear_77.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_39']} = elementwise_add(inputs={X=['tmp_38'], Y=['linear_77.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_39.tmp_0'], Variance=['layer_norm_39.tmp_1'], Y=['layer_norm_39.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_39.b_0'], Scale=['layer_norm_39.w_0'], X=['tmp_39']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_78.tmp_0']} = matmul_v2(inputs={X=['layer_norm_39.tmp_2'], Y=['linear_78.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_78.tmp_1']} = elementwise_add(inputs={X=['linear_78.tmp_0'], Y=['linear_78.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_19.tmp_0']} = gelu(inputs={X=['linear_78.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_79.tmp_0']} = matmul_v2(inputs={X=['gelu_19.tmp_0'], Y=['linear_79.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_79.tmp_1']} = elementwise_add(inputs={X=['linear_79.tmp_0'], Y=['linear_79.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_40']} = elementwise_add(inputs={X=['tmp_39'], Y=['linear_79.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_40.tmp_0'], Variance=['layer_norm_40.tmp_1'], Y=['layer_norm_40.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_40.b_0'], Scale=['layer_norm_40.w_0'], X=['tmp_40']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_80.tmp_0']} = matmul_v2(inputs={X=['layer_norm_40.tmp_2'], Y=['linear_80.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_80.tmp_1']} = elementwise_add(inputs={X=['linear_80.tmp_0'], Y=['linear_80.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_40.tmp_0'], XShape=['reshape2_40.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_80.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_20.tmp_0', 'split_20.tmp_1', 'split_20.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_40.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_80.tmp_0'], XShape=['transpose_80.tmp_1']} = transpose2(inputs={X=['split_20.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_81.tmp_0'], XShape=['transpose_81.tmp_1']} = transpose2(inputs={X=['split_20.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_82.tmp_0'], XShape=['transpose_82.tmp_1']} = transpose2(inputs={X=['split_20.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_40.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_80.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_40.tmp_0']} = matmul_v2(inputs={X=['scale_40.tmp_0'], Y=['transpose_81.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_41.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_40.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_20.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_41.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_41.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_20.tmp_0'], Y=['transpose_82.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_83.tmp_0'], XShape=['transpose_83.tmp_1']} = transpose2(inputs={X=['matmul_v2_41.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_41.tmp_0'], XShape=['reshape2_41.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_83.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_81.tmp_0']} = matmul_v2(inputs={X=['reshape2_41.tmp_0'], Y=['linear_81.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_81.tmp_1']} = elementwise_add(inputs={X=['linear_81.tmp_0'], Y=['linear_81.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_41']} = elementwise_add(inputs={X=['tmp_40'], Y=['linear_81.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_41.tmp_0'], Variance=['layer_norm_41.tmp_1'], Y=['layer_norm_41.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_41.b_0'], Scale=['layer_norm_41.w_0'], X=['tmp_41']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_82.tmp_0']} = matmul_v2(inputs={X=['layer_norm_41.tmp_2'], Y=['linear_82.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_82.tmp_1']} = elementwise_add(inputs={X=['linear_82.tmp_0'], Y=['linear_82.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_20.tmp_0']} = gelu(inputs={X=['linear_82.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_83.tmp_0']} = matmul_v2(inputs={X=['gelu_20.tmp_0'], Y=['linear_83.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_83.tmp_1']} = elementwise_add(inputs={X=['linear_83.tmp_0'], Y=['linear_83.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_42']} = elementwise_add(inputs={X=['tmp_41'], Y=['linear_83.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_42.tmp_0'], Variance=['layer_norm_42.tmp_1'], Y=['layer_norm_42.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_42.b_0'], Scale=['layer_norm_42.w_0'], X=['tmp_42']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_84.tmp_0']} = matmul_v2(inputs={X=['layer_norm_42.tmp_2'], Y=['linear_84.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_84.tmp_1']} = elementwise_add(inputs={X=['linear_84.tmp_0'], Y=['linear_84.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_42.tmp_0'], XShape=['reshape2_42.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_84.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_21.tmp_0', 'split_21.tmp_1', 'split_21.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_42.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_84.tmp_0'], XShape=['transpose_84.tmp_1']} = transpose2(inputs={X=['split_21.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_85.tmp_0'], XShape=['transpose_85.tmp_1']} = transpose2(inputs={X=['split_21.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_86.tmp_0'], XShape=['transpose_86.tmp_1']} = transpose2(inputs={X=['split_21.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_42.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_84.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_42.tmp_0']} = matmul_v2(inputs={X=['scale_42.tmp_0'], Y=['transpose_85.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_43.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_42.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_21.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_43.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_43.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_21.tmp_0'], Y=['transpose_86.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_87.tmp_0'], XShape=['transpose_87.tmp_1']} = transpose2(inputs={X=['matmul_v2_43.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_43.tmp_0'], XShape=['reshape2_43.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_87.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_85.tmp_0']} = matmul_v2(inputs={X=['reshape2_43.tmp_0'], Y=['linear_85.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_85.tmp_1']} = elementwise_add(inputs={X=['linear_85.tmp_0'], Y=['linear_85.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43']} = elementwise_add(inputs={X=['tmp_42'], Y=['linear_85.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_43.tmp_0'], Variance=['layer_norm_43.tmp_1'], Y=['layer_norm_43.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_43.b_0'], Scale=['layer_norm_43.w_0'], X=['tmp_43']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_86.tmp_0']} = matmul_v2(inputs={X=['layer_norm_43.tmp_2'], Y=['linear_86.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_86.tmp_1']} = elementwise_add(inputs={X=['linear_86.tmp_0'], Y=['linear_86.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_21.tmp_0']} = gelu(inputs={X=['linear_86.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_87.tmp_0']} = matmul_v2(inputs={X=['gelu_21.tmp_0'], Y=['linear_87.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_87.tmp_1']} = elementwise_add(inputs={X=['linear_87.tmp_0'], Y=['linear_87.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_44']} = elementwise_add(inputs={X=['tmp_43'], Y=['linear_87.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_44.tmp_0'], Variance=['layer_norm_44.tmp_1'], Y=['layer_norm_44.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_44.b_0'], Scale=['layer_norm_44.w_0'], X=['tmp_44']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_88.tmp_0']} = matmul_v2(inputs={X=['layer_norm_44.tmp_2'], Y=['linear_88.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_88.tmp_1']} = elementwise_add(inputs={X=['linear_88.tmp_0'], Y=['linear_88.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_44.tmp_0'], XShape=['reshape2_44.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_88.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_22.tmp_0', 'split_22.tmp_1', 'split_22.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_44.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_88.tmp_0'], XShape=['transpose_88.tmp_1']} = transpose2(inputs={X=['split_22.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_89.tmp_0'], XShape=['transpose_89.tmp_1']} = transpose2(inputs={X=['split_22.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_90.tmp_0'], XShape=['transpose_90.tmp_1']} = transpose2(inputs={X=['split_22.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_44.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_88.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_44.tmp_0']} = matmul_v2(inputs={X=['scale_44.tmp_0'], Y=['transpose_89.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_45.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_44.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_22.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_45.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_45.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_22.tmp_0'], Y=['transpose_90.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_91.tmp_0'], XShape=['transpose_91.tmp_1']} = transpose2(inputs={X=['matmul_v2_45.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_45.tmp_0'], XShape=['reshape2_45.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_91.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_89.tmp_0']} = matmul_v2(inputs={X=['reshape2_45.tmp_0'], Y=['linear_89.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_89.tmp_1']} = elementwise_add(inputs={X=['linear_89.tmp_0'], Y=['linear_89.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_45']} = elementwise_add(inputs={X=['tmp_44'], Y=['linear_89.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_45.tmp_0'], Variance=['layer_norm_45.tmp_1'], Y=['layer_norm_45.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_45.b_0'], Scale=['layer_norm_45.w_0'], X=['tmp_45']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_90.tmp_0']} = matmul_v2(inputs={X=['layer_norm_45.tmp_2'], Y=['linear_90.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_90.tmp_1']} = elementwise_add(inputs={X=['linear_90.tmp_0'], Y=['linear_90.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_22.tmp_0']} = gelu(inputs={X=['linear_90.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_91.tmp_0']} = matmul_v2(inputs={X=['gelu_22.tmp_0'], Y=['linear_91.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_91.tmp_1']} = elementwise_add(inputs={X=['linear_91.tmp_0'], Y=['linear_91.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_46']} = elementwise_add(inputs={X=['tmp_45'], Y=['linear_91.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_46.tmp_0'], Variance=['layer_norm_46.tmp_1'], Y=['layer_norm_46.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_46.b_0'], Scale=['layer_norm_46.w_0'], X=['tmp_46']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_92.tmp_0']} = matmul_v2(inputs={X=['layer_norm_46.tmp_2'], Y=['linear_92.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_92.tmp_1']} = elementwise_add(inputs={X=['linear_92.tmp_0'], Y=['linear_92.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_46.tmp_0'], XShape=['reshape2_46.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_92.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_23.tmp_0', 'split_23.tmp_1', 'split_23.tmp_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_46.tmp_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_92.tmp_0'], XShape=['transpose_92.tmp_1']} = transpose2(inputs={X=['split_23.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_93.tmp_0'], XShape=['transpose_93.tmp_1']} = transpose2(inputs={X=['split_23.tmp_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_94.tmp_0'], XShape=['transpose_94.tmp_1']} = transpose2(inputs={X=['split_23.tmp_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scale_46.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_92.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_46.tmp_0']} = matmul_v2(inputs={X=['scale_46.tmp_0'], Y=['transpose_93.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_47.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_46.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_23.tmp_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_47.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_47.tmp_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_23.tmp_0'], Y=['transpose_94.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_95.tmp_0'], XShape=['transpose_95.tmp_1']} = transpose2(inputs={X=['matmul_v2_47.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_47.tmp_0'], XShape=['reshape2_47.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_95.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_93.tmp_0']} = matmul_v2(inputs={X=['reshape2_47.tmp_0'], Y=['linear_93.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_93.tmp_1']} = elementwise_add(inputs={X=['linear_93.tmp_0'], Y=['linear_93.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_47']} = elementwise_add(inputs={X=['tmp_46'], Y=['linear_93.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_47.tmp_0'], Variance=['layer_norm_47.tmp_1'], Y=['layer_norm_47.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_47.b_0'], Scale=['layer_norm_47.w_0'], X=['tmp_47']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_94.tmp_0']} = matmul_v2(inputs={X=['layer_norm_47.tmp_2'], Y=['linear_94.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_94.tmp_1']} = elementwise_add(inputs={X=['linear_94.tmp_0'], Y=['linear_94.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_23.tmp_0']} = gelu(inputs={X=['linear_94.tmp_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['linear_95.tmp_0']} = matmul_v2(inputs={X=['gelu_23.tmp_0'], Y=['linear_95.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_95.tmp_1']} = elementwise_add(inputs={X=['linear_95.tmp_0'], Y=['linear_95.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_48']} = elementwise_add(inputs={X=['tmp_47'], Y=['linear_95.tmp_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_48.tmp_0'], Variance=['layer_norm_48.tmp_1'], Y=['layer_norm_48.tmp_2']} = layer_norm(inputs={Bias=['layer_norm_48.b_0'], Scale=['layer_norm_48.w_0'], X=['tmp_48']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['embedding_0.w_0@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], out_shape = [50304, 1024], peer = 0, ring_id = 34, use_calc_stream = True, with_quant_attr = False)
    {Out=['embedding_0.w_0@RESHARD_0']} = assign(inputs={X=['embedding_0.w_0@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_48.tmp_0']} = matmul_v2(inputs={X=['layer_norm_48.tmp_2'], Y=['embedding_0.w_0@RESHARD_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 605, op original_id: 605, process_mesh (annotated): {shape: [2], process_ids: [1,3], dim_names: [dp]}; layer_norm_48.tmp_2's dims_mapping (input, annotated, non-parameter): [0, -1, -1], partial on dims: []; embedding_0.w_0's dims_mapping (input, annotated, parameter): [-1, -1], partial on dims: []; matmul_v2_48.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['label0']}, axes = [2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['unsqueeze2_0.tmp_0'], Logits=['matmul_v2_48.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {Out=['label1.cast_fp16']} = cast(inputs={X=['label1']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['reshape2_48.tmp_0'], XShape=['reshape2_48.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['label1.cast_fp16']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [-1], use_quantizer = False, with_quant_attr = False)
    {Out=['reshape2_49.tmp_0'], XShape=['reshape2_49.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['softmax_with_cross_entropy_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [-1], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_49']} = elementwise_mul(inputs={X=['reshape2_49.tmp_0'], Y=['reshape2_48.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_49.cast_fp32']} = cast(inputs={X=['tmp_49']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['sum_0.tmp_0']} = reduce_sum(inputs={X=['tmp_49.cast_fp32']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['reshape2_48.tmp_0.cast_fp32']} = cast(inputs={X=['reshape2_48.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['sum_1.tmp_0']} = reduce_sum(inputs={X=['reshape2_48.tmp_0.cast_fp32']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_50']} = elementwise_div(inputs={X=['sum_0.tmp_0'], Y=['sum_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['scaled_loss_0']} = elementwise_mul(inputs={X=['tmp_50'], Y=['loss_scaling_0']}, axis = -1, op_device = , op_namescope = /, op_role = 256, op_role_var = [], with_quant_attr = False)
    {Out=['scaled_loss_1@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = , op_role = 257, op_role_var = [], place_type = -1, shape = [], str_value = , value = 1.0, with_quant_attr = False)
    {X@GRAD=['tmp_50@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['scaled_loss_1@GRAD'], X=['tmp_50'], Y=['loss_scaling_0']}, axis = -1, op_role = 1)
    {X@GRAD=['sum_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_div_grad(inputs={Out=['tmp_50'], Out@GRAD=['tmp_50@GRAD'], X=['sum_0.tmp_0'], Y=['sum_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_49.cast_fp32@GRAD_0']} = reduce_sum_grad(inputs={Out@GRAD=['sum_0.tmp_0@GRAD'], X=['tmp_49.cast_fp32']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_49@GRAD']} = cast(inputs={X=['tmp_49.cast_fp32@GRAD_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['reshape2_49.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_49@GRAD'], X=['reshape2_49.tmp_0'], Y=['reshape2_48.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_49.tmp_0@GRAD'], XShape=['reshape2_49.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [-1], use_quantizer = False, with_quant_attr = False)
    {Logits@GRAD=['matmul_v2_48.tmp_0@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['unsqueeze2_0.tmp_0'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {X@GRAD=['layer_norm_48.tmp_2@GRAD'], Y@GRAD=['embedding_0.w_0@GRAD@RENAME@block0@0']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_48.tmp_0@GRAD'], X=['layer_norm_48.tmp_2'], Y=['embedding_0.w_0@RESHARD_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['embedding_0.w_0@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@RENAME@block0@0']} = scale(inputs={ScaleTensor=[], X=['embedding_0.w_0@GRAD@RENAME@block0@0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_48.b_0@GRAD'], Scale@GRAD=['layer_norm_48.w_0@GRAD'], X@GRAD=['tmp_48@GRAD']} = layer_norm_grad(inputs={Bias=['layer_norm_48.b_0'], Mean=['layer_norm_48.tmp_0'], Scale=['layer_norm_48.w_0'], Variance=['layer_norm_48.tmp_1'], X=['tmp_48'], Y@GRAD=['layer_norm_48.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['layer_norm_48.b_0', 'layer_norm_48.b_0@GRAD', 'layer_norm_48.w_0', 'layer_norm_48.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_46.tmp_0.subprog_0'], Variance=['layer_norm_46.tmp_1.subprog_0'], Y=['layer_norm_46.tmp_2.subprog_0']} = layer_norm(inputs={Bias=['layer_norm_46.b_0'], Scale=['layer_norm_46.w_0'], X=['tmp_46']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_92.tmp_0.subprog_0']} = matmul_v2(inputs={X=['layer_norm_46.tmp_2.subprog_0'], Y=['linear_92.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_92.tmp_1.subprog_0']} = elementwise_add(inputs={X=['linear_92.tmp_0.subprog_0'], Y=['linear_92.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_46.tmp_0.subprog_0'], XShape=['reshape2_46.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_92.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_23.tmp_0.subprog_0', 'split_23.tmp_1.subprog_0', 'split_23.tmp_2.subprog_0']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_46.tmp_0.subprog_0']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_92.tmp_0.subprog_0'], XShape=['transpose_92.tmp_1.subprog_0']} = transpose2(inputs={X=['split_23.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_93.tmp_0.subprog_0'], XShape=['transpose_93.tmp_1.subprog_0']} = transpose2(inputs={X=['split_23.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_94.tmp_0.subprog_0'], XShape=['transpose_94.tmp_1.subprog_0']} = transpose2(inputs={X=['split_23.tmp_2.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_46.tmp_0.subprog_0']} = scale(inputs={ScaleTensor=[], X=['transpose_92.tmp_0.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_46.tmp_0.subprog_0']} = matmul_v2(inputs={X=['scale_46.tmp_0.subprog_0'], Y=['transpose_93.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_47.tmp_0.subprog_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_46.tmp_0.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_23.tmp_0.subprog_0']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_47.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_47.tmp_0.subprog_0']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_23.tmp_0.subprog_0'], Y=['transpose_94.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_95.tmp_0.subprog_0'], XShape=['transpose_95.tmp_1.subprog_0']} = transpose2(inputs={X=['matmul_v2_47.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_47.tmp_0.subprog_0'], XShape=['reshape2_47.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_95.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_93.tmp_0.subprog_0']} = matmul_v2(inputs={X=['reshape2_47.tmp_0.subprog_0'], Y=['linear_93.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_93.tmp_1.subprog_0']} = elementwise_add(inputs={X=['linear_93.tmp_0.subprog_0'], Y=['linear_93.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_47.subprog_0']} = elementwise_add(inputs={X=['tmp_46'], Y=['linear_93.tmp_1.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_47.tmp_0.subprog_0'], Variance=['layer_norm_47.tmp_1.subprog_0'], Y=['layer_norm_47.tmp_2.subprog_0']} = layer_norm(inputs={Bias=['layer_norm_47.b_0'], Scale=['layer_norm_47.w_0'], X=['tmp_47.subprog_0']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_94.tmp_0.subprog_0']} = matmul_v2(inputs={X=['layer_norm_47.tmp_2.subprog_0'], Y=['linear_94.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_94.tmp_1.subprog_0']} = elementwise_add(inputs={X=['linear_94.tmp_0.subprog_0'], Y=['linear_94.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_23.tmp_0.subprog_0']} = gelu(inputs={X=['linear_94.tmp_1.subprog_0']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_95.tmp_0.subprog_0']} = matmul_v2(inputs={X=['gelu_23.tmp_0.subprog_0'], Y=['linear_95.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_95.tmp_1.subprog_0']} = elementwise_add(inputs={X=['linear_95.tmp_0.subprog_0'], Y=['linear_95.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_47@GRAD@RENAME@block0@0'], Y@GRAD=['linear_95.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_48@GRAD'], X=['tmp_47.subprog_0'], Y=['linear_95.tmp_1.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_95.tmp_0@GRAD'], Y@GRAD=['linear_95.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_95.tmp_1@GRAD'], X=['linear_95.tmp_0.subprog_0'], Y=['linear_95.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_95.b_0', 'linear_95.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_23.tmp_0@GRAD'], Y@GRAD=['linear_95.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_95.tmp_0@GRAD'], X=['gelu_23.tmp_0.subprog_0'], Y=['linear_95.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_95.w_0', 'linear_95.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_94.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_23.tmp_0@GRAD'], X=['linear_94.tmp_1.subprog_0']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_94.tmp_0@GRAD'], Y@GRAD=['linear_94.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_94.tmp_1@GRAD'], X=['linear_94.tmp_0.subprog_0'], Y=['linear_94.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_94.b_0', 'linear_94.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_47.tmp_2@GRAD'], Y@GRAD=['linear_94.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_94.tmp_0@GRAD'], X=['layer_norm_47.tmp_2.subprog_0'], Y=['linear_94.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_94.w_0', 'linear_94.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_47.b_0@GRAD'], Scale@GRAD=['layer_norm_47.w_0@GRAD'], X@GRAD=['tmp_47@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_47.b_0'], Mean=['layer_norm_47.tmp_0.subprog_0'], Scale=['layer_norm_47.w_0'], Variance=['layer_norm_47.tmp_1.subprog_0'], X=['tmp_47.subprog_0'], Y@GRAD=['layer_norm_47.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['layer_norm_47.b_0', 'layer_norm_47.b_0@GRAD', 'layer_norm_47.w_0', 'layer_norm_47.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_47@GRAD']} = sum(inputs={X=['tmp_47@GRAD@RENAME@block0@0', 'tmp_47@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_46@GRAD@RENAME@block0@0'], Y@GRAD=['linear_93.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_47@GRAD'], X=['tmp_46'], Y=['linear_93.tmp_1.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_93.tmp_0@GRAD'], Y@GRAD=['linear_93.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_93.tmp_1@GRAD'], X=['linear_93.tmp_0.subprog_0'], Y=['linear_93.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_93.b_0', 'linear_93.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_47.tmp_0@GRAD'], Y@GRAD=['linear_93.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_93.tmp_0@GRAD'], X=['reshape2_47.tmp_0.subprog_0'], Y=['linear_93.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_93.w_0', 'linear_93.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_95.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_47.tmp_0@GRAD'], XShape=['reshape2_47.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_47.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_95.tmp_0@GRAD'], XShape=['transpose_95.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_23.tmp_0@GRAD'], Y@GRAD=['transpose_94.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_47.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_23.tmp_0.subprog_0'], Y=['transpose_94.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_47.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_23.tmp_0.subprog_0'], Out@GRAD=['fused_softmax_mask_upper_triangle_23.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_46.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_47.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_46.tmp_0@GRAD'], Y@GRAD=['transpose_93.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_46.tmp_0@GRAD'], X=['scale_46.tmp_0.subprog_0'], Y=['transpose_93.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_92.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_46.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_23.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_94.tmp_0@GRAD'], XShape=['transpose_94.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_23.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_93.tmp_0@GRAD'], XShape=['transpose_93.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_23.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_92.tmp_0@GRAD'], XShape=['transpose_92.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_46.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_23.tmp_0@GRAD', 'split_23.tmp_1@GRAD', 'split_23.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_92.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_46.tmp_0@GRAD'], XShape=['reshape2_46.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_92.tmp_0@GRAD'], Y@GRAD=['linear_92.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_92.tmp_1@GRAD'], X=['linear_92.tmp_0.subprog_0'], Y=['linear_92.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_92.b_0', 'linear_92.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_46.tmp_2@GRAD'], Y@GRAD=['linear_92.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_92.tmp_0@GRAD'], X=['layer_norm_46.tmp_2.subprog_0'], Y=['linear_92.w_0']}, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['linear_92.w_0', 'linear_92.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_46.b_0@GRAD'], Scale@GRAD=['layer_norm_46.w_0@GRAD'], X@GRAD=['tmp_46@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_46.b_0'], Mean=['layer_norm_46.tmp_0.subprog_0'], Scale=['layer_norm_46.w_0'], Variance=['layer_norm_46.tmp_1.subprog_0'], X=['tmp_46'], Y@GRAD=['layer_norm_46.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_23/, op_role = 1, op_role_var = ['layer_norm_46.b_0', 'layer_norm_46.b_0@GRAD', 'layer_norm_46.w_0', 'layer_norm_46.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_44.tmp_0.subprog_1'], Variance=['layer_norm_44.tmp_1.subprog_1'], Y=['layer_norm_44.tmp_2.subprog_1']} = layer_norm(inputs={Bias=['layer_norm_44.b_0'], Scale=['layer_norm_44.w_0'], X=['tmp_44']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_88.tmp_0.subprog_1']} = matmul_v2(inputs={X=['layer_norm_44.tmp_2.subprog_1'], Y=['linear_88.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_88.tmp_1.subprog_1']} = elementwise_add(inputs={X=['linear_88.tmp_0.subprog_1'], Y=['linear_88.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_44.tmp_0.subprog_1'], XShape=['reshape2_44.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_88.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_22.tmp_0.subprog_1', 'split_22.tmp_1.subprog_1', 'split_22.tmp_2.subprog_1']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_44.tmp_0.subprog_1']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_88.tmp_0.subprog_1'], XShape=['transpose_88.tmp_1.subprog_1']} = transpose2(inputs={X=['split_22.tmp_0.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_89.tmp_0.subprog_1'], XShape=['transpose_89.tmp_1.subprog_1']} = transpose2(inputs={X=['split_22.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_90.tmp_0.subprog_1'], XShape=['transpose_90.tmp_1.subprog_1']} = transpose2(inputs={X=['split_22.tmp_2.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_44.tmp_0.subprog_1']} = scale(inputs={ScaleTensor=[], X=['transpose_88.tmp_0.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_44.tmp_0.subprog_1']} = matmul_v2(inputs={X=['scale_44.tmp_0.subprog_1'], Y=['transpose_89.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_45.tmp_0.subprog_1']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_44.tmp_0.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_22.tmp_0.subprog_1']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_45.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_45.tmp_0.subprog_1']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_22.tmp_0.subprog_1'], Y=['transpose_90.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_91.tmp_0.subprog_1'], XShape=['transpose_91.tmp_1.subprog_1']} = transpose2(inputs={X=['matmul_v2_45.tmp_0.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_45.tmp_0.subprog_1'], XShape=['reshape2_45.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_91.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_89.tmp_0.subprog_1']} = matmul_v2(inputs={X=['reshape2_45.tmp_0.subprog_1'], Y=['linear_89.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_89.tmp_1.subprog_1']} = elementwise_add(inputs={X=['linear_89.tmp_0.subprog_1'], Y=['linear_89.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_45.subprog_1']} = elementwise_add(inputs={X=['tmp_44'], Y=['linear_89.tmp_1.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_45.tmp_0.subprog_1'], Variance=['layer_norm_45.tmp_1.subprog_1'], Y=['layer_norm_45.tmp_2.subprog_1']} = layer_norm(inputs={Bias=['layer_norm_45.b_0'], Scale=['layer_norm_45.w_0'], X=['tmp_45.subprog_1']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_90.tmp_0.subprog_1']} = matmul_v2(inputs={X=['layer_norm_45.tmp_2.subprog_1'], Y=['linear_90.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_90.tmp_1.subprog_1']} = elementwise_add(inputs={X=['linear_90.tmp_0.subprog_1'], Y=['linear_90.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_22.tmp_0.subprog_1']} = gelu(inputs={X=['linear_90.tmp_1.subprog_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_91.tmp_0.subprog_1']} = matmul_v2(inputs={X=['gelu_22.tmp_0.subprog_1'], Y=['linear_91.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_91.tmp_1.subprog_1']} = elementwise_add(inputs={X=['linear_91.tmp_0.subprog_1'], Y=['linear_91.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_46@GRAD']} = sum(inputs={X=['tmp_46@GRAD@RENAME@block0@0', 'tmp_46@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_45@GRAD@RENAME@block0@0'], Y@GRAD=['linear_91.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_46@GRAD'], X=['tmp_45.subprog_1'], Y=['linear_91.tmp_1.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_91.tmp_0@GRAD'], Y@GRAD=['linear_91.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_91.tmp_1@GRAD'], X=['linear_91.tmp_0.subprog_1'], Y=['linear_91.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_91.b_0', 'linear_91.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_22.tmp_0@GRAD'], Y@GRAD=['linear_91.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_91.tmp_0@GRAD'], X=['gelu_22.tmp_0.subprog_1'], Y=['linear_91.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_91.w_0', 'linear_91.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_90.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_22.tmp_0@GRAD'], X=['linear_90.tmp_1.subprog_1']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_90.tmp_0@GRAD'], Y@GRAD=['linear_90.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_90.tmp_1@GRAD'], X=['linear_90.tmp_0.subprog_1'], Y=['linear_90.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_90.b_0', 'linear_90.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_45.tmp_2@GRAD'], Y@GRAD=['linear_90.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_90.tmp_0@GRAD'], X=['layer_norm_45.tmp_2.subprog_1'], Y=['linear_90.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_90.w_0', 'linear_90.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_45.b_0@GRAD'], Scale@GRAD=['layer_norm_45.w_0@GRAD'], X@GRAD=['tmp_45@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_45.b_0'], Mean=['layer_norm_45.tmp_0.subprog_1'], Scale=['layer_norm_45.w_0'], Variance=['layer_norm_45.tmp_1.subprog_1'], X=['tmp_45.subprog_1'], Y@GRAD=['layer_norm_45.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['layer_norm_45.b_0', 'layer_norm_45.b_0@GRAD', 'layer_norm_45.w_0', 'layer_norm_45.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_45@GRAD']} = sum(inputs={X=['tmp_45@GRAD@RENAME@block0@0', 'tmp_45@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_44@GRAD@RENAME@block0@0'], Y@GRAD=['linear_89.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_45@GRAD'], X=['tmp_44'], Y=['linear_89.tmp_1.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_89.tmp_0@GRAD'], Y@GRAD=['linear_89.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_89.tmp_1@GRAD'], X=['linear_89.tmp_0.subprog_1'], Y=['linear_89.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_89.b_0', 'linear_89.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_45.tmp_0@GRAD'], Y@GRAD=['linear_89.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_89.tmp_0@GRAD'], X=['reshape2_45.tmp_0.subprog_1'], Y=['linear_89.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_89.w_0', 'linear_89.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_91.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_45.tmp_0@GRAD'], XShape=['reshape2_45.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_45.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_91.tmp_0@GRAD'], XShape=['transpose_91.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_22.tmp_0@GRAD'], Y@GRAD=['transpose_90.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_45.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_22.tmp_0.subprog_1'], Y=['transpose_90.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_45.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_22.tmp_0.subprog_1'], Out@GRAD=['fused_softmax_mask_upper_triangle_22.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_44.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_45.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_44.tmp_0@GRAD'], Y@GRAD=['transpose_89.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_44.tmp_0@GRAD'], X=['scale_44.tmp_0.subprog_1'], Y=['transpose_89.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_88.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_44.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_22.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_90.tmp_0@GRAD'], XShape=['transpose_90.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_22.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_89.tmp_0@GRAD'], XShape=['transpose_89.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_22.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_88.tmp_0@GRAD'], XShape=['transpose_88.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_44.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_22.tmp_0@GRAD', 'split_22.tmp_1@GRAD', 'split_22.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_88.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_44.tmp_0@GRAD'], XShape=['reshape2_44.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_88.tmp_0@GRAD'], Y@GRAD=['linear_88.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_88.tmp_1@GRAD'], X=['linear_88.tmp_0.subprog_1'], Y=['linear_88.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_88.b_0', 'linear_88.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_44.tmp_2@GRAD'], Y@GRAD=['linear_88.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_88.tmp_0@GRAD'], X=['layer_norm_44.tmp_2.subprog_1'], Y=['linear_88.w_0']}, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['linear_88.w_0', 'linear_88.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_44.b_0@GRAD'], Scale@GRAD=['layer_norm_44.w_0@GRAD'], X@GRAD=['tmp_44@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_44.b_0'], Mean=['layer_norm_44.tmp_0.subprog_1'], Scale=['layer_norm_44.w_0'], Variance=['layer_norm_44.tmp_1.subprog_1'], X=['tmp_44'], Y@GRAD=['layer_norm_44.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_22/, op_role = 1, op_role_var = ['layer_norm_44.b_0', 'layer_norm_44.b_0@GRAD', 'layer_norm_44.w_0', 'layer_norm_44.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_42.tmp_0.subprog_2'], Variance=['layer_norm_42.tmp_1.subprog_2'], Y=['layer_norm_42.tmp_2.subprog_2']} = layer_norm(inputs={Bias=['layer_norm_42.b_0'], Scale=['layer_norm_42.w_0'], X=['tmp_42']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_84.tmp_0.subprog_2']} = matmul_v2(inputs={X=['layer_norm_42.tmp_2.subprog_2'], Y=['linear_84.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_84.tmp_1.subprog_2']} = elementwise_add(inputs={X=['linear_84.tmp_0.subprog_2'], Y=['linear_84.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_42.tmp_0.subprog_2'], XShape=['reshape2_42.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_84.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_21.tmp_0.subprog_2', 'split_21.tmp_1.subprog_2', 'split_21.tmp_2.subprog_2']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_42.tmp_0.subprog_2']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_84.tmp_0.subprog_2'], XShape=['transpose_84.tmp_1.subprog_2']} = transpose2(inputs={X=['split_21.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_85.tmp_0.subprog_2'], XShape=['transpose_85.tmp_1.subprog_2']} = transpose2(inputs={X=['split_21.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_86.tmp_0.subprog_2'], XShape=['transpose_86.tmp_1.subprog_2']} = transpose2(inputs={X=['split_21.tmp_2.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_42.tmp_0.subprog_2']} = scale(inputs={ScaleTensor=[], X=['transpose_84.tmp_0.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_42.tmp_0.subprog_2']} = matmul_v2(inputs={X=['scale_42.tmp_0.subprog_2'], Y=['transpose_85.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_43.tmp_0.subprog_2']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_42.tmp_0.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_21.tmp_0.subprog_2']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_43.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_43.tmp_0.subprog_2']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_21.tmp_0.subprog_2'], Y=['transpose_86.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_87.tmp_0.subprog_2'], XShape=['transpose_87.tmp_1.subprog_2']} = transpose2(inputs={X=['matmul_v2_43.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_43.tmp_0.subprog_2'], XShape=['reshape2_43.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_87.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_85.tmp_0.subprog_2']} = matmul_v2(inputs={X=['reshape2_43.tmp_0.subprog_2'], Y=['linear_85.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_85.tmp_1.subprog_2']} = elementwise_add(inputs={X=['linear_85.tmp_0.subprog_2'], Y=['linear_85.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43.subprog_2']} = elementwise_add(inputs={X=['tmp_42'], Y=['linear_85.tmp_1.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_43.tmp_0.subprog_2'], Variance=['layer_norm_43.tmp_1.subprog_2'], Y=['layer_norm_43.tmp_2.subprog_2']} = layer_norm(inputs={Bias=['layer_norm_43.b_0'], Scale=['layer_norm_43.w_0'], X=['tmp_43.subprog_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_86.tmp_0.subprog_2']} = matmul_v2(inputs={X=['layer_norm_43.tmp_2.subprog_2'], Y=['linear_86.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_86.tmp_1.subprog_2']} = elementwise_add(inputs={X=['linear_86.tmp_0.subprog_2'], Y=['linear_86.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_21.tmp_0.subprog_2']} = gelu(inputs={X=['linear_86.tmp_1.subprog_2']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_87.tmp_0.subprog_2']} = matmul_v2(inputs={X=['gelu_21.tmp_0.subprog_2'], Y=['linear_87.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_87.tmp_1.subprog_2']} = elementwise_add(inputs={X=['linear_87.tmp_0.subprog_2'], Y=['linear_87.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_44@GRAD']} = sum(inputs={X=['tmp_44@GRAD@RENAME@block0@0', 'tmp_44@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_43@GRAD@RENAME@block0@0'], Y@GRAD=['linear_87.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_44@GRAD'], X=['tmp_43.subprog_2'], Y=['linear_87.tmp_1.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_87.tmp_0@GRAD'], Y@GRAD=['linear_87.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_87.tmp_1@GRAD'], X=['linear_87.tmp_0.subprog_2'], Y=['linear_87.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_87.b_0', 'linear_87.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_21.tmp_0@GRAD'], Y@GRAD=['linear_87.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_87.tmp_0@GRAD'], X=['gelu_21.tmp_0.subprog_2'], Y=['linear_87.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_87.w_0', 'linear_87.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_86.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_21.tmp_0@GRAD'], X=['linear_86.tmp_1.subprog_2']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_86.tmp_0@GRAD'], Y@GRAD=['linear_86.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_86.tmp_1@GRAD'], X=['linear_86.tmp_0.subprog_2'], Y=['linear_86.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_86.b_0', 'linear_86.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_43.tmp_2@GRAD'], Y@GRAD=['linear_86.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_86.tmp_0@GRAD'], X=['layer_norm_43.tmp_2.subprog_2'], Y=['linear_86.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_86.w_0', 'linear_86.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_43.b_0@GRAD'], Scale@GRAD=['layer_norm_43.w_0@GRAD'], X@GRAD=['tmp_43@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_43.b_0'], Mean=['layer_norm_43.tmp_0.subprog_2'], Scale=['layer_norm_43.w_0'], Variance=['layer_norm_43.tmp_1.subprog_2'], X=['tmp_43.subprog_2'], Y@GRAD=['layer_norm_43.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['layer_norm_43.b_0', 'layer_norm_43.b_0@GRAD', 'layer_norm_43.w_0', 'layer_norm_43.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_43@GRAD']} = sum(inputs={X=['tmp_43@GRAD@RENAME@block0@0', 'tmp_43@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_42@GRAD@RENAME@block0@0'], Y@GRAD=['linear_85.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_43@GRAD'], X=['tmp_42'], Y=['linear_85.tmp_1.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_85.tmp_0@GRAD'], Y@GRAD=['linear_85.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_85.tmp_1@GRAD'], X=['linear_85.tmp_0.subprog_2'], Y=['linear_85.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_85.b_0', 'linear_85.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_43.tmp_0@GRAD'], Y@GRAD=['linear_85.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_85.tmp_0@GRAD'], X=['reshape2_43.tmp_0.subprog_2'], Y=['linear_85.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_85.w_0', 'linear_85.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_87.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_43.tmp_0@GRAD'], XShape=['reshape2_43.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_43.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_87.tmp_0@GRAD'], XShape=['transpose_87.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_21.tmp_0@GRAD'], Y@GRAD=['transpose_86.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_43.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_21.tmp_0.subprog_2'], Y=['transpose_86.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_43.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_21.tmp_0.subprog_2'], Out@GRAD=['fused_softmax_mask_upper_triangle_21.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_42.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_43.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_42.tmp_0@GRAD'], Y@GRAD=['transpose_85.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_42.tmp_0@GRAD'], X=['scale_42.tmp_0.subprog_2'], Y=['transpose_85.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_84.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_42.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_21.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_86.tmp_0@GRAD'], XShape=['transpose_86.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_21.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_85.tmp_0@GRAD'], XShape=['transpose_85.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_21.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_84.tmp_0@GRAD'], XShape=['transpose_84.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_42.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_21.tmp_0@GRAD', 'split_21.tmp_1@GRAD', 'split_21.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_84.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_42.tmp_0@GRAD'], XShape=['reshape2_42.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_84.tmp_0@GRAD'], Y@GRAD=['linear_84.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_84.tmp_1@GRAD'], X=['linear_84.tmp_0.subprog_2'], Y=['linear_84.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_84.b_0', 'linear_84.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_42.tmp_2@GRAD'], Y@GRAD=['linear_84.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_84.tmp_0@GRAD'], X=['layer_norm_42.tmp_2.subprog_2'], Y=['linear_84.w_0']}, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['linear_84.w_0', 'linear_84.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_42.b_0@GRAD'], Scale@GRAD=['layer_norm_42.w_0@GRAD'], X@GRAD=['tmp_42@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_42.b_0'], Mean=['layer_norm_42.tmp_0.subprog_2'], Scale=['layer_norm_42.w_0'], Variance=['layer_norm_42.tmp_1.subprog_2'], X=['tmp_42'], Y@GRAD=['layer_norm_42.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_21/, op_role = 1, op_role_var = ['layer_norm_42.b_0', 'layer_norm_42.b_0@GRAD', 'layer_norm_42.w_0', 'layer_norm_42.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_40.tmp_0.subprog_3'], Variance=['layer_norm_40.tmp_1.subprog_3'], Y=['layer_norm_40.tmp_2.subprog_3']} = layer_norm(inputs={Bias=['layer_norm_40.b_0'], Scale=['layer_norm_40.w_0'], X=['tmp_40']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_80.tmp_0.subprog_3']} = matmul_v2(inputs={X=['layer_norm_40.tmp_2.subprog_3'], Y=['linear_80.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_80.tmp_1.subprog_3']} = elementwise_add(inputs={X=['linear_80.tmp_0.subprog_3'], Y=['linear_80.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_40.tmp_0.subprog_3'], XShape=['reshape2_40.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_80.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_20.tmp_0.subprog_3', 'split_20.tmp_1.subprog_3', 'split_20.tmp_2.subprog_3']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_40.tmp_0.subprog_3']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_80.tmp_0.subprog_3'], XShape=['transpose_80.tmp_1.subprog_3']} = transpose2(inputs={X=['split_20.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_81.tmp_0.subprog_3'], XShape=['transpose_81.tmp_1.subprog_3']} = transpose2(inputs={X=['split_20.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_82.tmp_0.subprog_3'], XShape=['transpose_82.tmp_1.subprog_3']} = transpose2(inputs={X=['split_20.tmp_2.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_40.tmp_0.subprog_3']} = scale(inputs={ScaleTensor=[], X=['transpose_80.tmp_0.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_40.tmp_0.subprog_3']} = matmul_v2(inputs={X=['scale_40.tmp_0.subprog_3'], Y=['transpose_81.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_41.tmp_0.subprog_3']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_40.tmp_0.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_20.tmp_0.subprog_3']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_41.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_41.tmp_0.subprog_3']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_20.tmp_0.subprog_3'], Y=['transpose_82.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_83.tmp_0.subprog_3'], XShape=['transpose_83.tmp_1.subprog_3']} = transpose2(inputs={X=['matmul_v2_41.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_41.tmp_0.subprog_3'], XShape=['reshape2_41.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_83.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_81.tmp_0.subprog_3']} = matmul_v2(inputs={X=['reshape2_41.tmp_0.subprog_3'], Y=['linear_81.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_81.tmp_1.subprog_3']} = elementwise_add(inputs={X=['linear_81.tmp_0.subprog_3'], Y=['linear_81.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_41.subprog_3']} = elementwise_add(inputs={X=['tmp_40'], Y=['linear_81.tmp_1.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_41.tmp_0.subprog_3'], Variance=['layer_norm_41.tmp_1.subprog_3'], Y=['layer_norm_41.tmp_2.subprog_3']} = layer_norm(inputs={Bias=['layer_norm_41.b_0'], Scale=['layer_norm_41.w_0'], X=['tmp_41.subprog_3']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_82.tmp_0.subprog_3']} = matmul_v2(inputs={X=['layer_norm_41.tmp_2.subprog_3'], Y=['linear_82.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_82.tmp_1.subprog_3']} = elementwise_add(inputs={X=['linear_82.tmp_0.subprog_3'], Y=['linear_82.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_20.tmp_0.subprog_3']} = gelu(inputs={X=['linear_82.tmp_1.subprog_3']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_83.tmp_0.subprog_3']} = matmul_v2(inputs={X=['gelu_20.tmp_0.subprog_3'], Y=['linear_83.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_83.tmp_1.subprog_3']} = elementwise_add(inputs={X=['linear_83.tmp_0.subprog_3'], Y=['linear_83.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_42@GRAD']} = sum(inputs={X=['tmp_42@GRAD@RENAME@block0@0', 'tmp_42@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_41@GRAD@RENAME@block0@0'], Y@GRAD=['linear_83.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_42@GRAD'], X=['tmp_41.subprog_3'], Y=['linear_83.tmp_1.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_83.tmp_0@GRAD'], Y@GRAD=['linear_83.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_83.tmp_1@GRAD'], X=['linear_83.tmp_0.subprog_3'], Y=['linear_83.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_83.b_0', 'linear_83.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_20.tmp_0@GRAD'], Y@GRAD=['linear_83.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_83.tmp_0@GRAD'], X=['gelu_20.tmp_0.subprog_3'], Y=['linear_83.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_83.w_0', 'linear_83.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_82.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_20.tmp_0@GRAD'], X=['linear_82.tmp_1.subprog_3']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_82.tmp_0@GRAD'], Y@GRAD=['linear_82.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_82.tmp_1@GRAD'], X=['linear_82.tmp_0.subprog_3'], Y=['linear_82.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_82.b_0', 'linear_82.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_41.tmp_2@GRAD'], Y@GRAD=['linear_82.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_82.tmp_0@GRAD'], X=['layer_norm_41.tmp_2.subprog_3'], Y=['linear_82.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_82.w_0', 'linear_82.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_41.b_0@GRAD'], Scale@GRAD=['layer_norm_41.w_0@GRAD'], X@GRAD=['tmp_41@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_41.b_0'], Mean=['layer_norm_41.tmp_0.subprog_3'], Scale=['layer_norm_41.w_0'], Variance=['layer_norm_41.tmp_1.subprog_3'], X=['tmp_41.subprog_3'], Y@GRAD=['layer_norm_41.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['layer_norm_41.b_0', 'layer_norm_41.b_0@GRAD', 'layer_norm_41.w_0', 'layer_norm_41.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_41@GRAD']} = sum(inputs={X=['tmp_41@GRAD@RENAME@block0@0', 'tmp_41@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_40@GRAD@RENAME@block0@0'], Y@GRAD=['linear_81.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_41@GRAD'], X=['tmp_40'], Y=['linear_81.tmp_1.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_81.tmp_0@GRAD'], Y@GRAD=['linear_81.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_81.tmp_1@GRAD'], X=['linear_81.tmp_0.subprog_3'], Y=['linear_81.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_81.b_0', 'linear_81.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_41.tmp_0@GRAD'], Y@GRAD=['linear_81.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_81.tmp_0@GRAD'], X=['reshape2_41.tmp_0.subprog_3'], Y=['linear_81.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_81.w_0', 'linear_81.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_83.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_41.tmp_0@GRAD'], XShape=['reshape2_41.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_41.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_83.tmp_0@GRAD'], XShape=['transpose_83.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_20.tmp_0@GRAD'], Y@GRAD=['transpose_82.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_41.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_20.tmp_0.subprog_3'], Y=['transpose_82.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_41.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_20.tmp_0.subprog_3'], Out@GRAD=['fused_softmax_mask_upper_triangle_20.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_40.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_41.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_40.tmp_0@GRAD'], Y@GRAD=['transpose_81.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_40.tmp_0@GRAD'], X=['scale_40.tmp_0.subprog_3'], Y=['transpose_81.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_80.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_40.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_20.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_82.tmp_0@GRAD'], XShape=['transpose_82.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_20.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_81.tmp_0@GRAD'], XShape=['transpose_81.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_20.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_80.tmp_0@GRAD'], XShape=['transpose_80.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_40.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_20.tmp_0@GRAD', 'split_20.tmp_1@GRAD', 'split_20.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_80.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_40.tmp_0@GRAD'], XShape=['reshape2_40.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_80.tmp_0@GRAD'], Y@GRAD=['linear_80.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_80.tmp_1@GRAD'], X=['linear_80.tmp_0.subprog_3'], Y=['linear_80.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_80.b_0', 'linear_80.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_40.tmp_2@GRAD'], Y@GRAD=['linear_80.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_80.tmp_0@GRAD'], X=['layer_norm_40.tmp_2.subprog_3'], Y=['linear_80.w_0']}, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['linear_80.w_0', 'linear_80.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_40.b_0@GRAD'], Scale@GRAD=['layer_norm_40.w_0@GRAD'], X@GRAD=['tmp_40@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_40.b_0'], Mean=['layer_norm_40.tmp_0.subprog_3'], Scale=['layer_norm_40.w_0'], Variance=['layer_norm_40.tmp_1.subprog_3'], X=['tmp_40'], Y@GRAD=['layer_norm_40.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_20/, op_role = 1, op_role_var = ['layer_norm_40.b_0', 'layer_norm_40.b_0@GRAD', 'layer_norm_40.w_0', 'layer_norm_40.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_38.tmp_0.subprog_4'], Variance=['layer_norm_38.tmp_1.subprog_4'], Y=['layer_norm_38.tmp_2.subprog_4']} = layer_norm(inputs={Bias=['layer_norm_38.b_0'], Scale=['layer_norm_38.w_0'], X=['tmp_38']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_76.tmp_0.subprog_4']} = matmul_v2(inputs={X=['layer_norm_38.tmp_2.subprog_4'], Y=['linear_76.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_76.tmp_1.subprog_4']} = elementwise_add(inputs={X=['linear_76.tmp_0.subprog_4'], Y=['linear_76.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_38.tmp_0.subprog_4'], XShape=['reshape2_38.tmp_1.subprog_4']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_76.tmp_1.subprog_4']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_19.tmp_0.subprog_4', 'split_19.tmp_1.subprog_4', 'split_19.tmp_2.subprog_4']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_38.tmp_0.subprog_4']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_76.tmp_0.subprog_4'], XShape=['transpose_76.tmp_1.subprog_4']} = transpose2(inputs={X=['split_19.tmp_0.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_77.tmp_0.subprog_4'], XShape=['transpose_77.tmp_1.subprog_4']} = transpose2(inputs={X=['split_19.tmp_1.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_78.tmp_0.subprog_4'], XShape=['transpose_78.tmp_1.subprog_4']} = transpose2(inputs={X=['split_19.tmp_2.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_38.tmp_0.subprog_4']} = scale(inputs={ScaleTensor=[], X=['transpose_76.tmp_0.subprog_4']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_38.tmp_0.subprog_4']} = matmul_v2(inputs={X=['scale_38.tmp_0.subprog_4'], Y=['transpose_77.tmp_0.subprog_4']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_39.tmp_0.subprog_4']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_38.tmp_0.subprog_4']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_19.tmp_0.subprog_4']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_39.tmp_0.subprog_4']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_39.tmp_0.subprog_4']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_19.tmp_0.subprog_4'], Y=['transpose_78.tmp_0.subprog_4']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_79.tmp_0.subprog_4'], XShape=['transpose_79.tmp_1.subprog_4']} = transpose2(inputs={X=['matmul_v2_39.tmp_0.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_39.tmp_0.subprog_4'], XShape=['reshape2_39.tmp_1.subprog_4']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_79.tmp_0.subprog_4']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_77.tmp_0.subprog_4']} = matmul_v2(inputs={X=['reshape2_39.tmp_0.subprog_4'], Y=['linear_77.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_77.tmp_1.subprog_4']} = elementwise_add(inputs={X=['linear_77.tmp_0.subprog_4'], Y=['linear_77.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_39.subprog_4']} = elementwise_add(inputs={X=['tmp_38'], Y=['linear_77.tmp_1.subprog_4']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_39.tmp_0.subprog_4'], Variance=['layer_norm_39.tmp_1.subprog_4'], Y=['layer_norm_39.tmp_2.subprog_4']} = layer_norm(inputs={Bias=['layer_norm_39.b_0'], Scale=['layer_norm_39.w_0'], X=['tmp_39.subprog_4']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_78.tmp_0.subprog_4']} = matmul_v2(inputs={X=['layer_norm_39.tmp_2.subprog_4'], Y=['linear_78.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_78.tmp_1.subprog_4']} = elementwise_add(inputs={X=['linear_78.tmp_0.subprog_4'], Y=['linear_78.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_19.tmp_0.subprog_4']} = gelu(inputs={X=['linear_78.tmp_1.subprog_4']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_79.tmp_0.subprog_4']} = matmul_v2(inputs={X=['gelu_19.tmp_0.subprog_4'], Y=['linear_79.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_79.tmp_1.subprog_4']} = elementwise_add(inputs={X=['linear_79.tmp_0.subprog_4'], Y=['linear_79.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_40@GRAD']} = sum(inputs={X=['tmp_40@GRAD@RENAME@block0@0', 'tmp_40@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_39@GRAD@RENAME@block0@0'], Y@GRAD=['linear_79.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_40@GRAD'], X=['tmp_39.subprog_4'], Y=['linear_79.tmp_1.subprog_4']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_79.tmp_0@GRAD'], Y@GRAD=['linear_79.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_79.tmp_1@GRAD'], X=['linear_79.tmp_0.subprog_4'], Y=['linear_79.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_79.b_0', 'linear_79.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_19.tmp_0@GRAD'], Y@GRAD=['linear_79.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_79.tmp_0@GRAD'], X=['gelu_19.tmp_0.subprog_4'], Y=['linear_79.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_79.w_0', 'linear_79.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_78.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_19.tmp_0@GRAD'], X=['linear_78.tmp_1.subprog_4']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_78.tmp_0@GRAD'], Y@GRAD=['linear_78.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_78.tmp_1@GRAD'], X=['linear_78.tmp_0.subprog_4'], Y=['linear_78.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_78.b_0', 'linear_78.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_39.tmp_2@GRAD'], Y@GRAD=['linear_78.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_78.tmp_0@GRAD'], X=['layer_norm_39.tmp_2.subprog_4'], Y=['linear_78.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_78.w_0', 'linear_78.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_39.b_0@GRAD'], Scale@GRAD=['layer_norm_39.w_0@GRAD'], X@GRAD=['tmp_39@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_39.b_0'], Mean=['layer_norm_39.tmp_0.subprog_4'], Scale=['layer_norm_39.w_0'], Variance=['layer_norm_39.tmp_1.subprog_4'], X=['tmp_39.subprog_4'], Y@GRAD=['layer_norm_39.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['layer_norm_39.b_0', 'layer_norm_39.b_0@GRAD', 'layer_norm_39.w_0', 'layer_norm_39.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_39@GRAD']} = sum(inputs={X=['tmp_39@GRAD@RENAME@block0@0', 'tmp_39@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_38@GRAD@RENAME@block0@0'], Y@GRAD=['linear_77.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['tmp_38'], Y=['linear_77.tmp_1.subprog_4']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_77.tmp_0@GRAD'], Y@GRAD=['linear_77.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_77.tmp_1@GRAD'], X=['linear_77.tmp_0.subprog_4'], Y=['linear_77.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_77.b_0', 'linear_77.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_39.tmp_0@GRAD'], Y@GRAD=['linear_77.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_77.tmp_0@GRAD'], X=['reshape2_39.tmp_0.subprog_4'], Y=['linear_77.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_77.w_0', 'linear_77.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_79.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_39.tmp_0@GRAD'], XShape=['reshape2_39.tmp_1.subprog_4']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_39.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_79.tmp_0@GRAD'], XShape=['transpose_79.tmp_1.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_19.tmp_0@GRAD'], Y@GRAD=['transpose_78.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_39.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_19.tmp_0.subprog_4'], Y=['transpose_78.tmp_0.subprog_4']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_39.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_19.tmp_0.subprog_4'], Out@GRAD=['fused_softmax_mask_upper_triangle_19.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_38.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_39.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_38.tmp_0@GRAD'], Y@GRAD=['transpose_77.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_38.tmp_0@GRAD'], X=['scale_38.tmp_0.subprog_4'], Y=['transpose_77.tmp_0.subprog_4']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_76.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_38.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_19.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_78.tmp_0@GRAD'], XShape=['transpose_78.tmp_1.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_19.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_77.tmp_0@GRAD'], XShape=['transpose_77.tmp_1.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_19.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_76.tmp_0@GRAD'], XShape=['transpose_76.tmp_1.subprog_4']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_38.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_19.tmp_0@GRAD', 'split_19.tmp_1@GRAD', 'split_19.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_76.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_38.tmp_0@GRAD'], XShape=['reshape2_38.tmp_1.subprog_4']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_76.tmp_0@GRAD'], Y@GRAD=['linear_76.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_76.tmp_1@GRAD'], X=['linear_76.tmp_0.subprog_4'], Y=['linear_76.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_76.b_0', 'linear_76.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_38.tmp_2@GRAD'], Y@GRAD=['linear_76.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_76.tmp_0@GRAD'], X=['layer_norm_38.tmp_2.subprog_4'], Y=['linear_76.w_0']}, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['linear_76.w_0', 'linear_76.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_38.b_0@GRAD'], Scale@GRAD=['layer_norm_38.w_0@GRAD'], X@GRAD=['tmp_38@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_38.b_0'], Mean=['layer_norm_38.tmp_0.subprog_4'], Scale=['layer_norm_38.w_0'], Variance=['layer_norm_38.tmp_1.subprog_4'], X=['tmp_38'], Y@GRAD=['layer_norm_38.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_19/, op_role = 1, op_role_var = ['layer_norm_38.b_0', 'layer_norm_38.b_0@GRAD', 'layer_norm_38.w_0', 'layer_norm_38.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_36.tmp_0.subprog_5'], Variance=['layer_norm_36.tmp_1.subprog_5'], Y=['layer_norm_36.tmp_2.subprog_5']} = layer_norm(inputs={Bias=['layer_norm_36.b_0'], Scale=['layer_norm_36.w_0'], X=['tmp_36']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_72.tmp_0.subprog_5']} = matmul_v2(inputs={X=['layer_norm_36.tmp_2.subprog_5'], Y=['linear_72.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_72.tmp_1.subprog_5']} = elementwise_add(inputs={X=['linear_72.tmp_0.subprog_5'], Y=['linear_72.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_36.tmp_0.subprog_5'], XShape=['reshape2_36.tmp_1.subprog_5']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_72.tmp_1.subprog_5']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_18.tmp_0.subprog_5', 'split_18.tmp_1.subprog_5', 'split_18.tmp_2.subprog_5']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_36.tmp_0.subprog_5']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_72.tmp_0.subprog_5'], XShape=['transpose_72.tmp_1.subprog_5']} = transpose2(inputs={X=['split_18.tmp_0.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_73.tmp_0.subprog_5'], XShape=['transpose_73.tmp_1.subprog_5']} = transpose2(inputs={X=['split_18.tmp_1.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_74.tmp_0.subprog_5'], XShape=['transpose_74.tmp_1.subprog_5']} = transpose2(inputs={X=['split_18.tmp_2.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_36.tmp_0.subprog_5']} = scale(inputs={ScaleTensor=[], X=['transpose_72.tmp_0.subprog_5']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_36.tmp_0.subprog_5']} = matmul_v2(inputs={X=['scale_36.tmp_0.subprog_5'], Y=['transpose_73.tmp_0.subprog_5']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_37.tmp_0.subprog_5']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_36.tmp_0.subprog_5']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_18.tmp_0.subprog_5']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_37.tmp_0.subprog_5']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_37.tmp_0.subprog_5']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_18.tmp_0.subprog_5'], Y=['transpose_74.tmp_0.subprog_5']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_75.tmp_0.subprog_5'], XShape=['transpose_75.tmp_1.subprog_5']} = transpose2(inputs={X=['matmul_v2_37.tmp_0.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_37.tmp_0.subprog_5'], XShape=['reshape2_37.tmp_1.subprog_5']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_75.tmp_0.subprog_5']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_73.tmp_0.subprog_5']} = matmul_v2(inputs={X=['reshape2_37.tmp_0.subprog_5'], Y=['linear_73.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_73.tmp_1.subprog_5']} = elementwise_add(inputs={X=['linear_73.tmp_0.subprog_5'], Y=['linear_73.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_37.subprog_5']} = elementwise_add(inputs={X=['tmp_36'], Y=['linear_73.tmp_1.subprog_5']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_37.tmp_0.subprog_5'], Variance=['layer_norm_37.tmp_1.subprog_5'], Y=['layer_norm_37.tmp_2.subprog_5']} = layer_norm(inputs={Bias=['layer_norm_37.b_0'], Scale=['layer_norm_37.w_0'], X=['tmp_37.subprog_5']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_74.tmp_0.subprog_5']} = matmul_v2(inputs={X=['layer_norm_37.tmp_2.subprog_5'], Y=['linear_74.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_74.tmp_1.subprog_5']} = elementwise_add(inputs={X=['linear_74.tmp_0.subprog_5'], Y=['linear_74.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_18.tmp_0.subprog_5']} = gelu(inputs={X=['linear_74.tmp_1.subprog_5']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_75.tmp_0.subprog_5']} = matmul_v2(inputs={X=['gelu_18.tmp_0.subprog_5'], Y=['linear_75.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_75.tmp_1.subprog_5']} = elementwise_add(inputs={X=['linear_75.tmp_0.subprog_5'], Y=['linear_75.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_38@GRAD']} = sum(inputs={X=['tmp_38@GRAD@RENAME@block0@0', 'tmp_38@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_37@GRAD@RENAME@block0@0'], Y@GRAD=['linear_75.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_38@GRAD'], X=['tmp_37.subprog_5'], Y=['linear_75.tmp_1.subprog_5']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_75.tmp_0@GRAD'], Y@GRAD=['linear_75.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_75.tmp_1@GRAD'], X=['linear_75.tmp_0.subprog_5'], Y=['linear_75.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_75.b_0', 'linear_75.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_18.tmp_0@GRAD'], Y@GRAD=['linear_75.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_75.tmp_0@GRAD'], X=['gelu_18.tmp_0.subprog_5'], Y=['linear_75.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_75.w_0', 'linear_75.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_74.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_18.tmp_0@GRAD'], X=['linear_74.tmp_1.subprog_5']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_74.tmp_0@GRAD'], Y@GRAD=['linear_74.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_74.tmp_1@GRAD'], X=['linear_74.tmp_0.subprog_5'], Y=['linear_74.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_74.b_0', 'linear_74.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_37.tmp_2@GRAD'], Y@GRAD=['linear_74.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_74.tmp_0@GRAD'], X=['layer_norm_37.tmp_2.subprog_5'], Y=['linear_74.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_74.w_0', 'linear_74.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_37.b_0@GRAD'], Scale@GRAD=['layer_norm_37.w_0@GRAD'], X@GRAD=['tmp_37@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_37.b_0'], Mean=['layer_norm_37.tmp_0.subprog_5'], Scale=['layer_norm_37.w_0'], Variance=['layer_norm_37.tmp_1.subprog_5'], X=['tmp_37.subprog_5'], Y@GRAD=['layer_norm_37.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['layer_norm_37.b_0', 'layer_norm_37.b_0@GRAD', 'layer_norm_37.w_0', 'layer_norm_37.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_37@GRAD']} = sum(inputs={X=['tmp_37@GRAD@RENAME@block0@0', 'tmp_37@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_36@GRAD@RENAME@block0@0'], Y@GRAD=['linear_73.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['tmp_36'], Y=['linear_73.tmp_1.subprog_5']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_73.tmp_0@GRAD'], Y@GRAD=['linear_73.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_73.tmp_1@GRAD'], X=['linear_73.tmp_0.subprog_5'], Y=['linear_73.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_73.b_0', 'linear_73.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_37.tmp_0@GRAD'], Y@GRAD=['linear_73.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_73.tmp_0@GRAD'], X=['reshape2_37.tmp_0.subprog_5'], Y=['linear_73.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_73.w_0', 'linear_73.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_75.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_37.tmp_0@GRAD'], XShape=['reshape2_37.tmp_1.subprog_5']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_37.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_75.tmp_0@GRAD'], XShape=['transpose_75.tmp_1.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_18.tmp_0@GRAD'], Y@GRAD=['transpose_74.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_37.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_18.tmp_0.subprog_5'], Y=['transpose_74.tmp_0.subprog_5']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_37.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_18.tmp_0.subprog_5'], Out@GRAD=['fused_softmax_mask_upper_triangle_18.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_36.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_37.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_36.tmp_0@GRAD'], Y@GRAD=['transpose_73.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_36.tmp_0@GRAD'], X=['scale_36.tmp_0.subprog_5'], Y=['transpose_73.tmp_0.subprog_5']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_72.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_36.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_18.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_74.tmp_0@GRAD'], XShape=['transpose_74.tmp_1.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_18.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_73.tmp_0@GRAD'], XShape=['transpose_73.tmp_1.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_18.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_72.tmp_0@GRAD'], XShape=['transpose_72.tmp_1.subprog_5']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_36.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_18.tmp_0@GRAD', 'split_18.tmp_1@GRAD', 'split_18.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_72.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_36.tmp_0@GRAD'], XShape=['reshape2_36.tmp_1.subprog_5']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_72.tmp_0@GRAD'], Y@GRAD=['linear_72.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_72.tmp_1@GRAD'], X=['linear_72.tmp_0.subprog_5'], Y=['linear_72.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_72.b_0', 'linear_72.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_36.tmp_2@GRAD'], Y@GRAD=['linear_72.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_72.tmp_0@GRAD'], X=['layer_norm_36.tmp_2.subprog_5'], Y=['linear_72.w_0']}, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['linear_72.w_0', 'linear_72.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_36.b_0@GRAD'], Scale@GRAD=['layer_norm_36.w_0@GRAD'], X@GRAD=['tmp_36@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_36.b_0'], Mean=['layer_norm_36.tmp_0.subprog_5'], Scale=['layer_norm_36.w_0'], Variance=['layer_norm_36.tmp_1.subprog_5'], X=['tmp_36'], Y@GRAD=['layer_norm_36.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_18/, op_role = 1, op_role_var = ['layer_norm_36.b_0', 'layer_norm_36.b_0@GRAD', 'layer_norm_36.w_0', 'layer_norm_36.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_34.tmp_0.subprog_6'], Variance=['layer_norm_34.tmp_1.subprog_6'], Y=['layer_norm_34.tmp_2.subprog_6']} = layer_norm(inputs={Bias=['layer_norm_34.b_0'], Scale=['layer_norm_34.w_0'], X=['tmp_34']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_68.tmp_0.subprog_6']} = matmul_v2(inputs={X=['layer_norm_34.tmp_2.subprog_6'], Y=['linear_68.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_68.tmp_1.subprog_6']} = elementwise_add(inputs={X=['linear_68.tmp_0.subprog_6'], Y=['linear_68.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_34.tmp_0.subprog_6'], XShape=['reshape2_34.tmp_1.subprog_6']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_68.tmp_1.subprog_6']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_17.tmp_0.subprog_6', 'split_17.tmp_1.subprog_6', 'split_17.tmp_2.subprog_6']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_34.tmp_0.subprog_6']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_68.tmp_0.subprog_6'], XShape=['transpose_68.tmp_1.subprog_6']} = transpose2(inputs={X=['split_17.tmp_0.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_69.tmp_0.subprog_6'], XShape=['transpose_69.tmp_1.subprog_6']} = transpose2(inputs={X=['split_17.tmp_1.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_70.tmp_0.subprog_6'], XShape=['transpose_70.tmp_1.subprog_6']} = transpose2(inputs={X=['split_17.tmp_2.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_34.tmp_0.subprog_6']} = scale(inputs={ScaleTensor=[], X=['transpose_68.tmp_0.subprog_6']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_34.tmp_0.subprog_6']} = matmul_v2(inputs={X=['scale_34.tmp_0.subprog_6'], Y=['transpose_69.tmp_0.subprog_6']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_35.tmp_0.subprog_6']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_34.tmp_0.subprog_6']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_17.tmp_0.subprog_6']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_35.tmp_0.subprog_6']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_35.tmp_0.subprog_6']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_17.tmp_0.subprog_6'], Y=['transpose_70.tmp_0.subprog_6']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_71.tmp_0.subprog_6'], XShape=['transpose_71.tmp_1.subprog_6']} = transpose2(inputs={X=['matmul_v2_35.tmp_0.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_35.tmp_0.subprog_6'], XShape=['reshape2_35.tmp_1.subprog_6']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_71.tmp_0.subprog_6']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_69.tmp_0.subprog_6']} = matmul_v2(inputs={X=['reshape2_35.tmp_0.subprog_6'], Y=['linear_69.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_69.tmp_1.subprog_6']} = elementwise_add(inputs={X=['linear_69.tmp_0.subprog_6'], Y=['linear_69.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_35.subprog_6']} = elementwise_add(inputs={X=['tmp_34'], Y=['linear_69.tmp_1.subprog_6']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_35.tmp_0.subprog_6'], Variance=['layer_norm_35.tmp_1.subprog_6'], Y=['layer_norm_35.tmp_2.subprog_6']} = layer_norm(inputs={Bias=['layer_norm_35.b_0'], Scale=['layer_norm_35.w_0'], X=['tmp_35.subprog_6']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_70.tmp_0.subprog_6']} = matmul_v2(inputs={X=['layer_norm_35.tmp_2.subprog_6'], Y=['linear_70.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_70.tmp_1.subprog_6']} = elementwise_add(inputs={X=['linear_70.tmp_0.subprog_6'], Y=['linear_70.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_17.tmp_0.subprog_6']} = gelu(inputs={X=['linear_70.tmp_1.subprog_6']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_71.tmp_0.subprog_6']} = matmul_v2(inputs={X=['gelu_17.tmp_0.subprog_6'], Y=['linear_71.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_71.tmp_1.subprog_6']} = elementwise_add(inputs={X=['linear_71.tmp_0.subprog_6'], Y=['linear_71.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_36@GRAD']} = sum(inputs={X=['tmp_36@GRAD@RENAME@block0@0', 'tmp_36@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_35@GRAD@RENAME@block0@0'], Y@GRAD=['linear_71.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_36@GRAD'], X=['tmp_35.subprog_6'], Y=['linear_71.tmp_1.subprog_6']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_71.tmp_0@GRAD'], Y@GRAD=['linear_71.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_71.tmp_1@GRAD'], X=['linear_71.tmp_0.subprog_6'], Y=['linear_71.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_71.b_0', 'linear_71.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_17.tmp_0@GRAD'], Y@GRAD=['linear_71.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_71.tmp_0@GRAD'], X=['gelu_17.tmp_0.subprog_6'], Y=['linear_71.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_71.w_0', 'linear_71.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_70.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_17.tmp_0@GRAD'], X=['linear_70.tmp_1.subprog_6']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_70.tmp_0@GRAD'], Y@GRAD=['linear_70.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_70.tmp_1@GRAD'], X=['linear_70.tmp_0.subprog_6'], Y=['linear_70.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_70.b_0', 'linear_70.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_35.tmp_2@GRAD'], Y@GRAD=['linear_70.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_70.tmp_0@GRAD'], X=['layer_norm_35.tmp_2.subprog_6'], Y=['linear_70.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_70.w_0', 'linear_70.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_35.b_0@GRAD'], Scale@GRAD=['layer_norm_35.w_0@GRAD'], X@GRAD=['tmp_35@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_35.b_0'], Mean=['layer_norm_35.tmp_0.subprog_6'], Scale=['layer_norm_35.w_0'], Variance=['layer_norm_35.tmp_1.subprog_6'], X=['tmp_35.subprog_6'], Y@GRAD=['layer_norm_35.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['layer_norm_35.b_0', 'layer_norm_35.b_0@GRAD', 'layer_norm_35.w_0', 'layer_norm_35.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_35@GRAD']} = sum(inputs={X=['tmp_35@GRAD@RENAME@block0@0', 'tmp_35@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_34@GRAD@RENAME@block0@0'], Y@GRAD=['linear_69.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_35@GRAD'], X=['tmp_34'], Y=['linear_69.tmp_1.subprog_6']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_69.tmp_0@GRAD'], Y@GRAD=['linear_69.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_69.tmp_1@GRAD'], X=['linear_69.tmp_0.subprog_6'], Y=['linear_69.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_69.b_0', 'linear_69.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_35.tmp_0@GRAD'], Y@GRAD=['linear_69.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_69.tmp_0@GRAD'], X=['reshape2_35.tmp_0.subprog_6'], Y=['linear_69.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_69.w_0', 'linear_69.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_71.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_35.tmp_0@GRAD'], XShape=['reshape2_35.tmp_1.subprog_6']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_35.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_71.tmp_0@GRAD'], XShape=['transpose_71.tmp_1.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_17.tmp_0@GRAD'], Y@GRAD=['transpose_70.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_35.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_17.tmp_0.subprog_6'], Y=['transpose_70.tmp_0.subprog_6']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_35.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_17.tmp_0.subprog_6'], Out@GRAD=['fused_softmax_mask_upper_triangle_17.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_34.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_35.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_34.tmp_0@GRAD'], Y@GRAD=['transpose_69.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_34.tmp_0@GRAD'], X=['scale_34.tmp_0.subprog_6'], Y=['transpose_69.tmp_0.subprog_6']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_68.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_34.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_17.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_70.tmp_0@GRAD'], XShape=['transpose_70.tmp_1.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_17.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_69.tmp_0@GRAD'], XShape=['transpose_69.tmp_1.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_17.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_68.tmp_0@GRAD'], XShape=['transpose_68.tmp_1.subprog_6']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_34.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_17.tmp_0@GRAD', 'split_17.tmp_1@GRAD', 'split_17.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_68.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_34.tmp_0@GRAD'], XShape=['reshape2_34.tmp_1.subprog_6']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_68.tmp_0@GRAD'], Y@GRAD=['linear_68.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_68.tmp_1@GRAD'], X=['linear_68.tmp_0.subprog_6'], Y=['linear_68.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_68.b_0', 'linear_68.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_34.tmp_2@GRAD'], Y@GRAD=['linear_68.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_68.tmp_0@GRAD'], X=['layer_norm_34.tmp_2.subprog_6'], Y=['linear_68.w_0']}, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['linear_68.w_0', 'linear_68.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_34.b_0@GRAD'], Scale@GRAD=['layer_norm_34.w_0@GRAD'], X@GRAD=['tmp_34@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_34.b_0'], Mean=['layer_norm_34.tmp_0.subprog_6'], Scale=['layer_norm_34.w_0'], Variance=['layer_norm_34.tmp_1.subprog_6'], X=['tmp_34'], Y@GRAD=['layer_norm_34.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_17/, op_role = 1, op_role_var = ['layer_norm_34.b_0', 'layer_norm_34.b_0@GRAD', 'layer_norm_34.w_0', 'layer_norm_34.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_32.tmp_0.subprog_7'], Variance=['layer_norm_32.tmp_1.subprog_7'], Y=['layer_norm_32.tmp_2.subprog_7']} = layer_norm(inputs={Bias=['layer_norm_32.b_0'], Scale=['layer_norm_32.w_0'], X=['tmp_32']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_64.tmp_0.subprog_7']} = matmul_v2(inputs={X=['layer_norm_32.tmp_2.subprog_7'], Y=['linear_64.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_64.tmp_1.subprog_7']} = elementwise_add(inputs={X=['linear_64.tmp_0.subprog_7'], Y=['linear_64.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_32.tmp_0.subprog_7'], XShape=['reshape2_32.tmp_1.subprog_7']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_64.tmp_1.subprog_7']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_16.tmp_0.subprog_7', 'split_16.tmp_1.subprog_7', 'split_16.tmp_2.subprog_7']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_32.tmp_0.subprog_7']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_64.tmp_0.subprog_7'], XShape=['transpose_64.tmp_1.subprog_7']} = transpose2(inputs={X=['split_16.tmp_0.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_65.tmp_0.subprog_7'], XShape=['transpose_65.tmp_1.subprog_7']} = transpose2(inputs={X=['split_16.tmp_1.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_66.tmp_0.subprog_7'], XShape=['transpose_66.tmp_1.subprog_7']} = transpose2(inputs={X=['split_16.tmp_2.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_32.tmp_0.subprog_7']} = scale(inputs={ScaleTensor=[], X=['transpose_64.tmp_0.subprog_7']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_32.tmp_0.subprog_7']} = matmul_v2(inputs={X=['scale_32.tmp_0.subprog_7'], Y=['transpose_65.tmp_0.subprog_7']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_33.tmp_0.subprog_7']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_32.tmp_0.subprog_7']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_16.tmp_0.subprog_7']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_33.tmp_0.subprog_7']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_33.tmp_0.subprog_7']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_16.tmp_0.subprog_7'], Y=['transpose_66.tmp_0.subprog_7']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_67.tmp_0.subprog_7'], XShape=['transpose_67.tmp_1.subprog_7']} = transpose2(inputs={X=['matmul_v2_33.tmp_0.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_33.tmp_0.subprog_7'], XShape=['reshape2_33.tmp_1.subprog_7']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_67.tmp_0.subprog_7']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_65.tmp_0.subprog_7']} = matmul_v2(inputs={X=['reshape2_33.tmp_0.subprog_7'], Y=['linear_65.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_65.tmp_1.subprog_7']} = elementwise_add(inputs={X=['linear_65.tmp_0.subprog_7'], Y=['linear_65.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_33.subprog_7']} = elementwise_add(inputs={X=['tmp_32'], Y=['linear_65.tmp_1.subprog_7']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_33.tmp_0.subprog_7'], Variance=['layer_norm_33.tmp_1.subprog_7'], Y=['layer_norm_33.tmp_2.subprog_7']} = layer_norm(inputs={Bias=['layer_norm_33.b_0'], Scale=['layer_norm_33.w_0'], X=['tmp_33.subprog_7']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_66.tmp_0.subprog_7']} = matmul_v2(inputs={X=['layer_norm_33.tmp_2.subprog_7'], Y=['linear_66.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_66.tmp_1.subprog_7']} = elementwise_add(inputs={X=['linear_66.tmp_0.subprog_7'], Y=['linear_66.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_16.tmp_0.subprog_7']} = gelu(inputs={X=['linear_66.tmp_1.subprog_7']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_67.tmp_0.subprog_7']} = matmul_v2(inputs={X=['gelu_16.tmp_0.subprog_7'], Y=['linear_67.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_67.tmp_1.subprog_7']} = elementwise_add(inputs={X=['linear_67.tmp_0.subprog_7'], Y=['linear_67.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_34@GRAD']} = sum(inputs={X=['tmp_34@GRAD@RENAME@block0@0', 'tmp_34@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_33@GRAD@RENAME@block0@0'], Y@GRAD=['linear_67.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_34@GRAD'], X=['tmp_33.subprog_7'], Y=['linear_67.tmp_1.subprog_7']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_67.tmp_0@GRAD'], Y@GRAD=['linear_67.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_67.tmp_1@GRAD'], X=['linear_67.tmp_0.subprog_7'], Y=['linear_67.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_67.b_0', 'linear_67.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_16.tmp_0@GRAD'], Y@GRAD=['linear_67.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_67.tmp_0@GRAD'], X=['gelu_16.tmp_0.subprog_7'], Y=['linear_67.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_67.w_0', 'linear_67.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_66.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_16.tmp_0@GRAD'], X=['linear_66.tmp_1.subprog_7']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_66.tmp_0@GRAD'], Y@GRAD=['linear_66.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_66.tmp_1@GRAD'], X=['linear_66.tmp_0.subprog_7'], Y=['linear_66.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_66.b_0', 'linear_66.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_33.tmp_2@GRAD'], Y@GRAD=['linear_66.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_66.tmp_0@GRAD'], X=['layer_norm_33.tmp_2.subprog_7'], Y=['linear_66.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_66.w_0', 'linear_66.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_33.b_0@GRAD'], Scale@GRAD=['layer_norm_33.w_0@GRAD'], X@GRAD=['tmp_33@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_33.b_0'], Mean=['layer_norm_33.tmp_0.subprog_7'], Scale=['layer_norm_33.w_0'], Variance=['layer_norm_33.tmp_1.subprog_7'], X=['tmp_33.subprog_7'], Y@GRAD=['layer_norm_33.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['layer_norm_33.b_0', 'layer_norm_33.b_0@GRAD', 'layer_norm_33.w_0', 'layer_norm_33.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_33@GRAD']} = sum(inputs={X=['tmp_33@GRAD@RENAME@block0@0', 'tmp_33@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_32@GRAD@RENAME@block0@0'], Y@GRAD=['linear_65.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_33@GRAD'], X=['tmp_32'], Y=['linear_65.tmp_1.subprog_7']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_65.tmp_0@GRAD'], Y@GRAD=['linear_65.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_65.tmp_1@GRAD'], X=['linear_65.tmp_0.subprog_7'], Y=['linear_65.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_65.b_0', 'linear_65.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_33.tmp_0@GRAD'], Y@GRAD=['linear_65.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_65.tmp_0@GRAD'], X=['reshape2_33.tmp_0.subprog_7'], Y=['linear_65.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_65.w_0', 'linear_65.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_67.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_33.tmp_0@GRAD'], XShape=['reshape2_33.tmp_1.subprog_7']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_33.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_67.tmp_0@GRAD'], XShape=['transpose_67.tmp_1.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_16.tmp_0@GRAD'], Y@GRAD=['transpose_66.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_33.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_16.tmp_0.subprog_7'], Y=['transpose_66.tmp_0.subprog_7']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_33.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_16.tmp_0.subprog_7'], Out@GRAD=['fused_softmax_mask_upper_triangle_16.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_32.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_33.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_32.tmp_0@GRAD'], Y@GRAD=['transpose_65.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_32.tmp_0@GRAD'], X=['scale_32.tmp_0.subprog_7'], Y=['transpose_65.tmp_0.subprog_7']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_64.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_32.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_16.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_66.tmp_0@GRAD'], XShape=['transpose_66.tmp_1.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_16.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_65.tmp_0@GRAD'], XShape=['transpose_65.tmp_1.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_16.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_64.tmp_0@GRAD'], XShape=['transpose_64.tmp_1.subprog_7']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_32.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_16.tmp_0@GRAD', 'split_16.tmp_1@GRAD', 'split_16.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_64.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_32.tmp_0@GRAD'], XShape=['reshape2_32.tmp_1.subprog_7']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_64.tmp_0@GRAD'], Y@GRAD=['linear_64.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_64.tmp_1@GRAD'], X=['linear_64.tmp_0.subprog_7'], Y=['linear_64.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_64.b_0', 'linear_64.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_32.tmp_2@GRAD'], Y@GRAD=['linear_64.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_64.tmp_0@GRAD'], X=['layer_norm_32.tmp_2.subprog_7'], Y=['linear_64.w_0']}, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['linear_64.w_0', 'linear_64.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_32.b_0@GRAD'], Scale@GRAD=['layer_norm_32.w_0@GRAD'], X@GRAD=['tmp_32@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_32.b_0'], Mean=['layer_norm_32.tmp_0.subprog_7'], Scale=['layer_norm_32.w_0'], Variance=['layer_norm_32.tmp_1.subprog_7'], X=['tmp_32'], Y@GRAD=['layer_norm_32.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_16/, op_role = 1, op_role_var = ['layer_norm_32.b_0', 'layer_norm_32.b_0@GRAD', 'layer_norm_32.w_0', 'layer_norm_32.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_30.tmp_0.subprog_8'], Variance=['layer_norm_30.tmp_1.subprog_8'], Y=['layer_norm_30.tmp_2.subprog_8']} = layer_norm(inputs={Bias=['layer_norm_30.b_0'], Scale=['layer_norm_30.w_0'], X=['tmp_30']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_60.tmp_0.subprog_8']} = matmul_v2(inputs={X=['layer_norm_30.tmp_2.subprog_8'], Y=['linear_60.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_60.tmp_1.subprog_8']} = elementwise_add(inputs={X=['linear_60.tmp_0.subprog_8'], Y=['linear_60.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_30.tmp_0.subprog_8'], XShape=['reshape2_30.tmp_1.subprog_8']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_60.tmp_1.subprog_8']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_15.tmp_0.subprog_8', 'split_15.tmp_1.subprog_8', 'split_15.tmp_2.subprog_8']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_30.tmp_0.subprog_8']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_60.tmp_0.subprog_8'], XShape=['transpose_60.tmp_1.subprog_8']} = transpose2(inputs={X=['split_15.tmp_0.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_61.tmp_0.subprog_8'], XShape=['transpose_61.tmp_1.subprog_8']} = transpose2(inputs={X=['split_15.tmp_1.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_62.tmp_0.subprog_8'], XShape=['transpose_62.tmp_1.subprog_8']} = transpose2(inputs={X=['split_15.tmp_2.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_30.tmp_0.subprog_8']} = scale(inputs={ScaleTensor=[], X=['transpose_60.tmp_0.subprog_8']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_30.tmp_0.subprog_8']} = matmul_v2(inputs={X=['scale_30.tmp_0.subprog_8'], Y=['transpose_61.tmp_0.subprog_8']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_31.tmp_0.subprog_8']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_30.tmp_0.subprog_8']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_15.tmp_0.subprog_8']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_31.tmp_0.subprog_8']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_31.tmp_0.subprog_8']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_15.tmp_0.subprog_8'], Y=['transpose_62.tmp_0.subprog_8']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_63.tmp_0.subprog_8'], XShape=['transpose_63.tmp_1.subprog_8']} = transpose2(inputs={X=['matmul_v2_31.tmp_0.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_31.tmp_0.subprog_8'], XShape=['reshape2_31.tmp_1.subprog_8']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_63.tmp_0.subprog_8']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_61.tmp_0.subprog_8']} = matmul_v2(inputs={X=['reshape2_31.tmp_0.subprog_8'], Y=['linear_61.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_61.tmp_1.subprog_8']} = elementwise_add(inputs={X=['linear_61.tmp_0.subprog_8'], Y=['linear_61.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_31.subprog_8']} = elementwise_add(inputs={X=['tmp_30'], Y=['linear_61.tmp_1.subprog_8']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_31.tmp_0.subprog_8'], Variance=['layer_norm_31.tmp_1.subprog_8'], Y=['layer_norm_31.tmp_2.subprog_8']} = layer_norm(inputs={Bias=['layer_norm_31.b_0'], Scale=['layer_norm_31.w_0'], X=['tmp_31.subprog_8']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_62.tmp_0.subprog_8']} = matmul_v2(inputs={X=['layer_norm_31.tmp_2.subprog_8'], Y=['linear_62.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_62.tmp_1.subprog_8']} = elementwise_add(inputs={X=['linear_62.tmp_0.subprog_8'], Y=['linear_62.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_15.tmp_0.subprog_8']} = gelu(inputs={X=['linear_62.tmp_1.subprog_8']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_63.tmp_0.subprog_8']} = matmul_v2(inputs={X=['gelu_15.tmp_0.subprog_8'], Y=['linear_63.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_63.tmp_1.subprog_8']} = elementwise_add(inputs={X=['linear_63.tmp_0.subprog_8'], Y=['linear_63.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_32@GRAD']} = sum(inputs={X=['tmp_32@GRAD@RENAME@block0@0', 'tmp_32@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_31@GRAD@RENAME@block0@0'], Y@GRAD=['linear_63.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['tmp_31.subprog_8'], Y=['linear_63.tmp_1.subprog_8']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_63.tmp_0@GRAD'], Y@GRAD=['linear_63.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_63.tmp_1@GRAD'], X=['linear_63.tmp_0.subprog_8'], Y=['linear_63.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_63.b_0', 'linear_63.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_15.tmp_0@GRAD'], Y@GRAD=['linear_63.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_63.tmp_0@GRAD'], X=['gelu_15.tmp_0.subprog_8'], Y=['linear_63.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_63.w_0', 'linear_63.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_62.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_15.tmp_0@GRAD'], X=['linear_62.tmp_1.subprog_8']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_62.tmp_0@GRAD'], Y@GRAD=['linear_62.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_62.tmp_1@GRAD'], X=['linear_62.tmp_0.subprog_8'], Y=['linear_62.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_62.b_0', 'linear_62.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_31.tmp_2@GRAD'], Y@GRAD=['linear_62.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_62.tmp_0@GRAD'], X=['layer_norm_31.tmp_2.subprog_8'], Y=['linear_62.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_62.w_0', 'linear_62.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_31.b_0@GRAD'], Scale@GRAD=['layer_norm_31.w_0@GRAD'], X@GRAD=['tmp_31@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_31.b_0'], Mean=['layer_norm_31.tmp_0.subprog_8'], Scale=['layer_norm_31.w_0'], Variance=['layer_norm_31.tmp_1.subprog_8'], X=['tmp_31.subprog_8'], Y@GRAD=['layer_norm_31.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['layer_norm_31.b_0', 'layer_norm_31.b_0@GRAD', 'layer_norm_31.w_0', 'layer_norm_31.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_31@GRAD']} = sum(inputs={X=['tmp_31@GRAD@RENAME@block0@0', 'tmp_31@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_30@GRAD@RENAME@block0@0'], Y@GRAD=['linear_61.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_31@GRAD'], X=['tmp_30'], Y=['linear_61.tmp_1.subprog_8']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_61.tmp_0@GRAD'], Y@GRAD=['linear_61.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_61.tmp_1@GRAD'], X=['linear_61.tmp_0.subprog_8'], Y=['linear_61.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_61.b_0', 'linear_61.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_31.tmp_0@GRAD'], Y@GRAD=['linear_61.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_61.tmp_0@GRAD'], X=['reshape2_31.tmp_0.subprog_8'], Y=['linear_61.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_61.w_0', 'linear_61.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_63.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_31.tmp_0@GRAD'], XShape=['reshape2_31.tmp_1.subprog_8']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_31.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_63.tmp_0@GRAD'], XShape=['transpose_63.tmp_1.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_15.tmp_0@GRAD'], Y@GRAD=['transpose_62.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_31.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_15.tmp_0.subprog_8'], Y=['transpose_62.tmp_0.subprog_8']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_31.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_15.tmp_0.subprog_8'], Out@GRAD=['fused_softmax_mask_upper_triangle_15.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_30.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_31.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_30.tmp_0@GRAD'], Y@GRAD=['transpose_61.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_30.tmp_0@GRAD'], X=['scale_30.tmp_0.subprog_8'], Y=['transpose_61.tmp_0.subprog_8']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_60.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_30.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_15.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_62.tmp_0@GRAD'], XShape=['transpose_62.tmp_1.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_15.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_61.tmp_0@GRAD'], XShape=['transpose_61.tmp_1.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_15.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_60.tmp_0@GRAD'], XShape=['transpose_60.tmp_1.subprog_8']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_30.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_15.tmp_0@GRAD', 'split_15.tmp_1@GRAD', 'split_15.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_60.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_30.tmp_0@GRAD'], XShape=['reshape2_30.tmp_1.subprog_8']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_60.tmp_0@GRAD'], Y@GRAD=['linear_60.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_60.tmp_1@GRAD'], X=['linear_60.tmp_0.subprog_8'], Y=['linear_60.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_60.b_0', 'linear_60.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_30.tmp_2@GRAD'], Y@GRAD=['linear_60.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_60.tmp_0@GRAD'], X=['layer_norm_30.tmp_2.subprog_8'], Y=['linear_60.w_0']}, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['linear_60.w_0', 'linear_60.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_30.b_0@GRAD'], Scale@GRAD=['layer_norm_30.w_0@GRAD'], X@GRAD=['tmp_30@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_30.b_0'], Mean=['layer_norm_30.tmp_0.subprog_8'], Scale=['layer_norm_30.w_0'], Variance=['layer_norm_30.tmp_1.subprog_8'], X=['tmp_30'], Y@GRAD=['layer_norm_30.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_15/, op_role = 1, op_role_var = ['layer_norm_30.b_0', 'layer_norm_30.b_0@GRAD', 'layer_norm_30.w_0', 'layer_norm_30.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_28.tmp_0.subprog_9'], Variance=['layer_norm_28.tmp_1.subprog_9'], Y=['layer_norm_28.tmp_2.subprog_9']} = layer_norm(inputs={Bias=['layer_norm_28.b_0'], Scale=['layer_norm_28.w_0'], X=['tmp_28']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_56.tmp_0.subprog_9']} = matmul_v2(inputs={X=['layer_norm_28.tmp_2.subprog_9'], Y=['linear_56.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_56.tmp_1.subprog_9']} = elementwise_add(inputs={X=['linear_56.tmp_0.subprog_9'], Y=['linear_56.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_28.tmp_0.subprog_9'], XShape=['reshape2_28.tmp_1.subprog_9']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_56.tmp_1.subprog_9']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_14.tmp_0.subprog_9', 'split_14.tmp_1.subprog_9', 'split_14.tmp_2.subprog_9']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_28.tmp_0.subprog_9']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_56.tmp_0.subprog_9'], XShape=['transpose_56.tmp_1.subprog_9']} = transpose2(inputs={X=['split_14.tmp_0.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_57.tmp_0.subprog_9'], XShape=['transpose_57.tmp_1.subprog_9']} = transpose2(inputs={X=['split_14.tmp_1.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_58.tmp_0.subprog_9'], XShape=['transpose_58.tmp_1.subprog_9']} = transpose2(inputs={X=['split_14.tmp_2.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_28.tmp_0.subprog_9']} = scale(inputs={ScaleTensor=[], X=['transpose_56.tmp_0.subprog_9']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_28.tmp_0.subprog_9']} = matmul_v2(inputs={X=['scale_28.tmp_0.subprog_9'], Y=['transpose_57.tmp_0.subprog_9']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_29.tmp_0.subprog_9']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_28.tmp_0.subprog_9']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_14.tmp_0.subprog_9']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_29.tmp_0.subprog_9']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_29.tmp_0.subprog_9']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_14.tmp_0.subprog_9'], Y=['transpose_58.tmp_0.subprog_9']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_59.tmp_0.subprog_9'], XShape=['transpose_59.tmp_1.subprog_9']} = transpose2(inputs={X=['matmul_v2_29.tmp_0.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_29.tmp_0.subprog_9'], XShape=['reshape2_29.tmp_1.subprog_9']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_59.tmp_0.subprog_9']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_57.tmp_0.subprog_9']} = matmul_v2(inputs={X=['reshape2_29.tmp_0.subprog_9'], Y=['linear_57.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_57.tmp_1.subprog_9']} = elementwise_add(inputs={X=['linear_57.tmp_0.subprog_9'], Y=['linear_57.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_29.subprog_9']} = elementwise_add(inputs={X=['tmp_28'], Y=['linear_57.tmp_1.subprog_9']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_29.tmp_0.subprog_9'], Variance=['layer_norm_29.tmp_1.subprog_9'], Y=['layer_norm_29.tmp_2.subprog_9']} = layer_norm(inputs={Bias=['layer_norm_29.b_0'], Scale=['layer_norm_29.w_0'], X=['tmp_29.subprog_9']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_58.tmp_0.subprog_9']} = matmul_v2(inputs={X=['layer_norm_29.tmp_2.subprog_9'], Y=['linear_58.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_58.tmp_1.subprog_9']} = elementwise_add(inputs={X=['linear_58.tmp_0.subprog_9'], Y=['linear_58.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_14.tmp_0.subprog_9']} = gelu(inputs={X=['linear_58.tmp_1.subprog_9']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_59.tmp_0.subprog_9']} = matmul_v2(inputs={X=['gelu_14.tmp_0.subprog_9'], Y=['linear_59.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_59.tmp_1.subprog_9']} = elementwise_add(inputs={X=['linear_59.tmp_0.subprog_9'], Y=['linear_59.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_30@GRAD']} = sum(inputs={X=['tmp_30@GRAD@RENAME@block0@0', 'tmp_30@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_29@GRAD@RENAME@block0@0'], Y@GRAD=['linear_59.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['tmp_29.subprog_9'], Y=['linear_59.tmp_1.subprog_9']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_59.tmp_0@GRAD'], Y@GRAD=['linear_59.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_59.tmp_1@GRAD'], X=['linear_59.tmp_0.subprog_9'], Y=['linear_59.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_59.b_0', 'linear_59.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_14.tmp_0@GRAD'], Y@GRAD=['linear_59.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_59.tmp_0@GRAD'], X=['gelu_14.tmp_0.subprog_9'], Y=['linear_59.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_59.w_0', 'linear_59.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_58.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_14.tmp_0@GRAD'], X=['linear_58.tmp_1.subprog_9']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_58.tmp_0@GRAD'], Y@GRAD=['linear_58.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_58.tmp_1@GRAD'], X=['linear_58.tmp_0.subprog_9'], Y=['linear_58.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_58.b_0', 'linear_58.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_29.tmp_2@GRAD'], Y@GRAD=['linear_58.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_58.tmp_0@GRAD'], X=['layer_norm_29.tmp_2.subprog_9'], Y=['linear_58.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_58.w_0', 'linear_58.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_29.b_0@GRAD'], Scale@GRAD=['layer_norm_29.w_0@GRAD'], X@GRAD=['tmp_29@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_29.b_0'], Mean=['layer_norm_29.tmp_0.subprog_9'], Scale=['layer_norm_29.w_0'], Variance=['layer_norm_29.tmp_1.subprog_9'], X=['tmp_29.subprog_9'], Y@GRAD=['layer_norm_29.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['layer_norm_29.b_0', 'layer_norm_29.b_0@GRAD', 'layer_norm_29.w_0', 'layer_norm_29.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_29@GRAD']} = sum(inputs={X=['tmp_29@GRAD@RENAME@block0@0', 'tmp_29@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_28@GRAD@RENAME@block0@0'], Y@GRAD=['linear_57.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_29@GRAD'], X=['tmp_28'], Y=['linear_57.tmp_1.subprog_9']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_57.tmp_0@GRAD'], Y@GRAD=['linear_57.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_57.tmp_1@GRAD'], X=['linear_57.tmp_0.subprog_9'], Y=['linear_57.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_57.b_0', 'linear_57.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_29.tmp_0@GRAD'], Y@GRAD=['linear_57.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_57.tmp_0@GRAD'], X=['reshape2_29.tmp_0.subprog_9'], Y=['linear_57.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_57.w_0', 'linear_57.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_59.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_29.tmp_0@GRAD'], XShape=['reshape2_29.tmp_1.subprog_9']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_29.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_59.tmp_0@GRAD'], XShape=['transpose_59.tmp_1.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_14.tmp_0@GRAD'], Y@GRAD=['transpose_58.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_29.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_14.tmp_0.subprog_9'], Y=['transpose_58.tmp_0.subprog_9']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_29.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_14.tmp_0.subprog_9'], Out@GRAD=['fused_softmax_mask_upper_triangle_14.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_28.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_29.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_28.tmp_0@GRAD'], Y@GRAD=['transpose_57.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_28.tmp_0@GRAD'], X=['scale_28.tmp_0.subprog_9'], Y=['transpose_57.tmp_0.subprog_9']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_56.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_28.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_14.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_58.tmp_0@GRAD'], XShape=['transpose_58.tmp_1.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_14.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_57.tmp_0@GRAD'], XShape=['transpose_57.tmp_1.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_14.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_56.tmp_0@GRAD'], XShape=['transpose_56.tmp_1.subprog_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_28.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_14.tmp_0@GRAD', 'split_14.tmp_1@GRAD', 'split_14.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_56.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_28.tmp_0@GRAD'], XShape=['reshape2_28.tmp_1.subprog_9']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_56.tmp_0@GRAD'], Y@GRAD=['linear_56.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_56.tmp_1@GRAD'], X=['linear_56.tmp_0.subprog_9'], Y=['linear_56.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_56.b_0', 'linear_56.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_28.tmp_2@GRAD'], Y@GRAD=['linear_56.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_56.tmp_0@GRAD'], X=['layer_norm_28.tmp_2.subprog_9'], Y=['linear_56.w_0']}, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['linear_56.w_0', 'linear_56.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_28.b_0@GRAD'], Scale@GRAD=['layer_norm_28.w_0@GRAD'], X@GRAD=['tmp_28@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_28.b_0'], Mean=['layer_norm_28.tmp_0.subprog_9'], Scale=['layer_norm_28.w_0'], Variance=['layer_norm_28.tmp_1.subprog_9'], X=['tmp_28'], Y@GRAD=['layer_norm_28.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_14/, op_role = 1, op_role_var = ['layer_norm_28.b_0', 'layer_norm_28.b_0@GRAD', 'layer_norm_28.w_0', 'layer_norm_28.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_26.tmp_0.subprog_10'], Variance=['layer_norm_26.tmp_1.subprog_10'], Y=['layer_norm_26.tmp_2.subprog_10']} = layer_norm(inputs={Bias=['layer_norm_26.b_0'], Scale=['layer_norm_26.w_0'], X=['tmp_26']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_52.tmp_0.subprog_10']} = matmul_v2(inputs={X=['layer_norm_26.tmp_2.subprog_10'], Y=['linear_52.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_52.tmp_1.subprog_10']} = elementwise_add(inputs={X=['linear_52.tmp_0.subprog_10'], Y=['linear_52.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_26.tmp_0.subprog_10'], XShape=['reshape2_26.tmp_1.subprog_10']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_52.tmp_1.subprog_10']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_13.tmp_0.subprog_10', 'split_13.tmp_1.subprog_10', 'split_13.tmp_2.subprog_10']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_26.tmp_0.subprog_10']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_52.tmp_0.subprog_10'], XShape=['transpose_52.tmp_1.subprog_10']} = transpose2(inputs={X=['split_13.tmp_0.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_53.tmp_0.subprog_10'], XShape=['transpose_53.tmp_1.subprog_10']} = transpose2(inputs={X=['split_13.tmp_1.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_54.tmp_0.subprog_10'], XShape=['transpose_54.tmp_1.subprog_10']} = transpose2(inputs={X=['split_13.tmp_2.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_26.tmp_0.subprog_10']} = scale(inputs={ScaleTensor=[], X=['transpose_52.tmp_0.subprog_10']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_26.tmp_0.subprog_10']} = matmul_v2(inputs={X=['scale_26.tmp_0.subprog_10'], Y=['transpose_53.tmp_0.subprog_10']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_27.tmp_0.subprog_10']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_26.tmp_0.subprog_10']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_13.tmp_0.subprog_10']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_27.tmp_0.subprog_10']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_27.tmp_0.subprog_10']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_13.tmp_0.subprog_10'], Y=['transpose_54.tmp_0.subprog_10']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_55.tmp_0.subprog_10'], XShape=['transpose_55.tmp_1.subprog_10']} = transpose2(inputs={X=['matmul_v2_27.tmp_0.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_27.tmp_0.subprog_10'], XShape=['reshape2_27.tmp_1.subprog_10']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_55.tmp_0.subprog_10']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_53.tmp_0.subprog_10']} = matmul_v2(inputs={X=['reshape2_27.tmp_0.subprog_10'], Y=['linear_53.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_53.tmp_1.subprog_10']} = elementwise_add(inputs={X=['linear_53.tmp_0.subprog_10'], Y=['linear_53.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_27.subprog_10']} = elementwise_add(inputs={X=['tmp_26'], Y=['linear_53.tmp_1.subprog_10']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_27.tmp_0.subprog_10'], Variance=['layer_norm_27.tmp_1.subprog_10'], Y=['layer_norm_27.tmp_2.subprog_10']} = layer_norm(inputs={Bias=['layer_norm_27.b_0'], Scale=['layer_norm_27.w_0'], X=['tmp_27.subprog_10']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_54.tmp_0.subprog_10']} = matmul_v2(inputs={X=['layer_norm_27.tmp_2.subprog_10'], Y=['linear_54.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_54.tmp_1.subprog_10']} = elementwise_add(inputs={X=['linear_54.tmp_0.subprog_10'], Y=['linear_54.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_13.tmp_0.subprog_10']} = gelu(inputs={X=['linear_54.tmp_1.subprog_10']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_55.tmp_0.subprog_10']} = matmul_v2(inputs={X=['gelu_13.tmp_0.subprog_10'], Y=['linear_55.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_55.tmp_1.subprog_10']} = elementwise_add(inputs={X=['linear_55.tmp_0.subprog_10'], Y=['linear_55.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_28@GRAD']} = sum(inputs={X=['tmp_28@GRAD@RENAME@block0@0', 'tmp_28@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@0'], Y@GRAD=['linear_55.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['tmp_27.subprog_10'], Y=['linear_55.tmp_1.subprog_10']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_55.tmp_0@GRAD'], Y@GRAD=['linear_55.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_55.tmp_1@GRAD'], X=['linear_55.tmp_0.subprog_10'], Y=['linear_55.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_55.b_0', 'linear_55.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_13.tmp_0@GRAD'], Y@GRAD=['linear_55.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_55.tmp_0@GRAD'], X=['gelu_13.tmp_0.subprog_10'], Y=['linear_55.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_55.w_0', 'linear_55.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_54.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_13.tmp_0@GRAD'], X=['linear_54.tmp_1.subprog_10']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_54.tmp_0@GRAD'], Y@GRAD=['linear_54.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_54.tmp_1@GRAD'], X=['linear_54.tmp_0.subprog_10'], Y=['linear_54.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_54.b_0', 'linear_54.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_27.tmp_2@GRAD'], Y@GRAD=['linear_54.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_54.tmp_0@GRAD'], X=['layer_norm_27.tmp_2.subprog_10'], Y=['linear_54.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_54.w_0', 'linear_54.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_27.b_0@GRAD'], Scale@GRAD=['layer_norm_27.w_0@GRAD'], X@GRAD=['tmp_27@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_27.b_0'], Mean=['layer_norm_27.tmp_0.subprog_10'], Scale=['layer_norm_27.w_0'], Variance=['layer_norm_27.tmp_1.subprog_10'], X=['tmp_27.subprog_10'], Y@GRAD=['layer_norm_27.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['layer_norm_27.b_0', 'layer_norm_27.b_0@GRAD', 'layer_norm_27.w_0', 'layer_norm_27.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_27@GRAD']} = sum(inputs={X=['tmp_27@GRAD@RENAME@block0@0', 'tmp_27@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_26@GRAD@RENAME@block0@0'], Y@GRAD=['linear_53.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['tmp_26'], Y=['linear_53.tmp_1.subprog_10']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_53.tmp_0@GRAD'], Y@GRAD=['linear_53.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_53.tmp_1@GRAD'], X=['linear_53.tmp_0.subprog_10'], Y=['linear_53.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_53.b_0', 'linear_53.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_27.tmp_0@GRAD'], Y@GRAD=['linear_53.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_53.tmp_0@GRAD'], X=['reshape2_27.tmp_0.subprog_10'], Y=['linear_53.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_53.w_0', 'linear_53.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_55.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_27.tmp_0@GRAD'], XShape=['reshape2_27.tmp_1.subprog_10']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_27.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_55.tmp_0@GRAD'], XShape=['transpose_55.tmp_1.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_13.tmp_0@GRAD'], Y@GRAD=['transpose_54.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_27.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_13.tmp_0.subprog_10'], Y=['transpose_54.tmp_0.subprog_10']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_27.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_13.tmp_0.subprog_10'], Out@GRAD=['fused_softmax_mask_upper_triangle_13.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_26.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_27.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_26.tmp_0@GRAD'], Y@GRAD=['transpose_53.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_26.tmp_0@GRAD'], X=['scale_26.tmp_0.subprog_10'], Y=['transpose_53.tmp_0.subprog_10']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_52.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_26.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_13.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_54.tmp_0@GRAD'], XShape=['transpose_54.tmp_1.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_13.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_53.tmp_0@GRAD'], XShape=['transpose_53.tmp_1.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_13.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_52.tmp_0@GRAD'], XShape=['transpose_52.tmp_1.subprog_10']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_26.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_13.tmp_0@GRAD', 'split_13.tmp_1@GRAD', 'split_13.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_52.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_26.tmp_0@GRAD'], XShape=['reshape2_26.tmp_1.subprog_10']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_52.tmp_0@GRAD'], Y@GRAD=['linear_52.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_52.tmp_1@GRAD'], X=['linear_52.tmp_0.subprog_10'], Y=['linear_52.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_52.b_0', 'linear_52.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_26.tmp_2@GRAD'], Y@GRAD=['linear_52.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_52.tmp_0@GRAD'], X=['layer_norm_26.tmp_2.subprog_10'], Y=['linear_52.w_0']}, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['linear_52.w_0', 'linear_52.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_26.b_0@GRAD'], Scale@GRAD=['layer_norm_26.w_0@GRAD'], X@GRAD=['tmp_26@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_26.b_0'], Mean=['layer_norm_26.tmp_0.subprog_10'], Scale=['layer_norm_26.w_0'], Variance=['layer_norm_26.tmp_1.subprog_10'], X=['tmp_26'], Y@GRAD=['layer_norm_26.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_13/, op_role = 1, op_role_var = ['layer_norm_26.b_0', 'layer_norm_26.b_0@GRAD', 'layer_norm_26.w_0', 'layer_norm_26.w_0@GRAD'], with_quant_attr = False)
    {Mean=['layer_norm_24.tmp_0.subprog_11'], Variance=['layer_norm_24.tmp_1.subprog_11'], Y=['layer_norm_24.tmp_2.subprog_11']} = layer_norm(inputs={Bias=['layer_norm_24.b_0'], Scale=['layer_norm_24.w_0'], X=['tmp_24']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_48.tmp_0.subprog_11']} = matmul_v2(inputs={X=['layer_norm_24.tmp_2.subprog_11'], Y=['linear_48.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_48.tmp_1.subprog_11']} = elementwise_add(inputs={X=['linear_48.tmp_0.subprog_11'], Y=['linear_48.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_24.tmp_0.subprog_11'], XShape=['reshape2_24.tmp_1.subprog_11']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_48.tmp_1.subprog_11']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {Out=['split_12.tmp_0.subprog_11', 'split_12.tmp_1.subprog_11', 'split_12.tmp_2.subprog_11']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['reshape2_24.tmp_0.subprog_11']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['transpose_48.tmp_0.subprog_11'], XShape=['transpose_48.tmp_1.subprog_11']} = transpose2(inputs={X=['split_12.tmp_0.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_49.tmp_0.subprog_11'], XShape=['transpose_49.tmp_1.subprog_11']} = transpose2(inputs={X=['split_12.tmp_1.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_50.tmp_0.subprog_11'], XShape=['transpose_50.tmp_1.subprog_11']} = transpose2(inputs={X=['split_12.tmp_2.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['scale_24.tmp_0.subprog_11']} = scale(inputs={ScaleTensor=[], X=['transpose_48.tmp_0.subprog_11']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {Out=['matmul_v2_24.tmp_0.subprog_11']} = matmul_v2(inputs={X=['scale_24.tmp_0.subprog_11'], Y=['transpose_49.tmp_0.subprog_11']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['scale_25.tmp_0.subprog_11']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_24.tmp_0.subprog_11']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {Out=['fused_softmax_mask_upper_triangle_12.tmp_0.subprog_11']} = fused_softmax_mask_upper_triangle(inputs={X=['scale_25.tmp_0.subprog_11']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_25.tmp_0.subprog_11']} = matmul_v2(inputs={X=['fused_softmax_mask_upper_triangle_12.tmp_0.subprog_11'], Y=['transpose_50.tmp_0.subprog_11']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_51.tmp_0.subprog_11'], XShape=['transpose_51.tmp_1.subprog_11']} = transpose2(inputs={X=['matmul_v2_25.tmp_0.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_25.tmp_0.subprog_11'], XShape=['reshape2_25.tmp_1.subprog_11']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_51.tmp_0.subprog_11']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_49.tmp_0.subprog_11']} = matmul_v2(inputs={X=['reshape2_25.tmp_0.subprog_11'], Y=['linear_49.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_49.tmp_1.subprog_11']} = elementwise_add(inputs={X=['linear_49.tmp_0.subprog_11'], Y=['linear_49.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_25.subprog_11']} = elementwise_add(inputs={X=['tmp_24'], Y=['linear_49.tmp_1.subprog_11']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Mean=['layer_norm_25.tmp_0.subprog_11'], Variance=['layer_norm_25.tmp_1.subprog_11'], Y=['layer_norm_25.tmp_2.subprog_11']} = layer_norm(inputs={Bias=['layer_norm_25.b_0'], Scale=['layer_norm_25.w_0'], X=['tmp_25.subprog_11']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_50.tmp_0.subprog_11']} = matmul_v2(inputs={X=['layer_norm_25.tmp_2.subprog_11'], Y=['linear_50.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_50.tmp_1.subprog_11']} = elementwise_add(inputs={X=['linear_50.tmp_0.subprog_11'], Y=['linear_50.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gelu_12.tmp_0.subprog_11']} = gelu(inputs={X=['linear_50.tmp_1.subprog_11']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_51.tmp_0.subprog_11']} = matmul_v2(inputs={X=['gelu_12.tmp_0.subprog_11'], Y=['linear_51.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_51.tmp_1.subprog_11']} = elementwise_add(inputs={X=['linear_51.tmp_0.subprog_11'], Y=['linear_51.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_26@GRAD']} = sum(inputs={X=['tmp_26@GRAD@RENAME@block0@0', 'tmp_26@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_25@GRAD@RENAME@block0@0'], Y@GRAD=['linear_51.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_26@GRAD'], X=['tmp_25.subprog_11'], Y=['linear_51.tmp_1.subprog_11']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_51.tmp_0@GRAD'], Y@GRAD=['linear_51.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_51.tmp_1@GRAD'], X=['linear_51.tmp_0.subprog_11'], Y=['linear_51.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_51.b_0', 'linear_51.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['gelu_12.tmp_0@GRAD'], Y@GRAD=['linear_51.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_51.tmp_0@GRAD'], X=['gelu_12.tmp_0.subprog_11'], Y=['linear_51.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_51.w_0', 'linear_51.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['linear_50.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['gelu_12.tmp_0@GRAD'], X=['linear_50.tmp_1.subprog_11']}, approximate = True, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_50.tmp_0@GRAD'], Y@GRAD=['linear_50.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_50.tmp_1@GRAD'], X=['linear_50.tmp_0.subprog_11'], Y=['linear_50.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_50.b_0', 'linear_50.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_25.tmp_2@GRAD'], Y@GRAD=['linear_50.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_50.tmp_0@GRAD'], X=['layer_norm_25.tmp_2.subprog_11'], Y=['linear_50.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_50.w_0', 'linear_50.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_25.b_0@GRAD'], Scale@GRAD=['layer_norm_25.w_0@GRAD'], X@GRAD=['tmp_25@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_25.b_0'], Mean=['layer_norm_25.tmp_0.subprog_11'], Scale=['layer_norm_25.w_0'], Variance=['layer_norm_25.tmp_1.subprog_11'], X=['tmp_25.subprog_11'], Y@GRAD=['layer_norm_25.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['layer_norm_25.b_0', 'layer_norm_25.b_0@GRAD', 'layer_norm_25.w_0', 'layer_norm_25.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_25@GRAD']} = sum(inputs={X=['tmp_25@GRAD@RENAME@block0@0', 'tmp_25@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_24@GRAD@RENAME@block0@0'], Y@GRAD=['linear_49.tmp_1@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_25@GRAD'], X=['tmp_24'], Y=['linear_49.tmp_1.subprog_11']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_49.tmp_0@GRAD'], Y@GRAD=['linear_49.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_49.tmp_1@GRAD'], X=['linear_49.tmp_0.subprog_11'], Y=['linear_49.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_49.b_0', 'linear_49.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['reshape2_25.tmp_0@GRAD'], Y@GRAD=['linear_49.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_49.tmp_0@GRAD'], X=['reshape2_25.tmp_0.subprog_11'], Y=['linear_49.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_49.w_0', 'linear_49.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_51.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_25.tmp_0@GRAD'], XShape=['reshape2_25.tmp_1.subprog_11']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], shape = [0, 0, -1], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_25.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_51.tmp_0@GRAD'], XShape=['transpose_51.tmp_1.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['fused_softmax_mask_upper_triangle_12.tmp_0@GRAD'], Y@GRAD=['transpose_50.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_25.tmp_0@GRAD'], X=['fused_softmax_mask_upper_triangle_12.tmp_0.subprog_11'], Y=['transpose_50.tmp_0.subprog_11']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['scale_25.tmp_0@GRAD']} = fused_softmax_mask_upper_triangle_grad(inputs={Out=['fused_softmax_mask_upper_triangle_12.tmp_0.subprog_11'], Out@GRAD=['fused_softmax_mask_upper_triangle_12.tmp_0@GRAD']}, op_device = , op_role = 1)
    {Out=['matmul_v2_24.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_25.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 24.0, with_quant_attr = False)
    {X@GRAD=['scale_24.tmp_0@GRAD'], Y@GRAD=['transpose_49.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_24.tmp_0@GRAD'], X=['scale_24.tmp_0.subprog_11'], Y=['transpose_49.tmp_0.subprog_11']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['transpose_48.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_24.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0052083334885537624, with_quant_attr = False)
    {X@GRAD=['split_12.tmp_2@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_50.tmp_0@GRAD'], XShape=['transpose_50.tmp_1.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_12.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_49.tmp_0@GRAD'], XShape=['transpose_49.tmp_1.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['split_12.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_48.tmp_0@GRAD'], XShape=['transpose_48.tmp_1.subprog_11']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_24.tmp_0@GRAD']} = concat(inputs={AxisTensor=[], X=['split_12.tmp_0@GRAD', 'split_12.tmp_1@GRAD', 'split_12.tmp_2@GRAD']}, axis = 3, num = 3, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {X@GRAD=['linear_48.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_24.tmp_0@GRAD'], XShape=['reshape2_24.tmp_1.subprog_11']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = [], shape = [0, 0, -1, 192], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['linear_48.tmp_0@GRAD'], Y@GRAD=['linear_48.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['linear_48.tmp_1@GRAD'], X=['linear_48.tmp_0.subprog_11'], Y=['linear_48.b_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_48.b_0', 'linear_48.b_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['layer_norm_24.tmp_2@GRAD'], Y@GRAD=['linear_48.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_48.tmp_0@GRAD'], X=['layer_norm_24.tmp_2.subprog_11'], Y=['linear_48.w_0']}, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['linear_48.w_0', 'linear_48.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Bias@GRAD=['layer_norm_24.b_0@GRAD'], Scale@GRAD=['layer_norm_24.w_0@GRAD'], X@GRAD=['tmp_24@GRAD@RENAME@block0@1']} = layer_norm_grad(inputs={Bias=['layer_norm_24.b_0'], Mean=['layer_norm_24.tmp_0.subprog_11'], Scale=['layer_norm_24.w_0'], Variance=['layer_norm_24.tmp_1.subprog_11'], X=['tmp_24'], Y@GRAD=['layer_norm_24.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = //auto_parallel/rc_12/, op_role = 1, op_role_var = ['layer_norm_24.b_0', 'layer_norm_24.b_0@GRAD', 'layer_norm_24.w_0', 'layer_norm_24.w_0@GRAD'], with_quant_attr = False)
    {Out=['tmp_24@GRAD']} = sum(inputs={X=['tmp_24@GRAD@RENAME@block0@0', 'tmp_24@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_24@GRAD']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], peer = 1, ring_id = 36, use_calc_stream = True, with_quant_attr = False)
    send_v2(inputs={X=['embedding_0.w_0@GRAD@RENAME@block0@0']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], peer = 1, ring_id = 36, use_calc_stream = True, with_quant_attr = False)
    {Out=['gradient_merge_step']} = increment(inputs={X=['gradient_merge_step']}, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], step = 1.0, with_quant_attr = False)
    {Out=['gradient_merge_step']} = elementwise_mod(inputs={X=['gradient_merge_step'], Y=['gradient_merge_k']}, axis = -1, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gradient_merge_cond']} = equal(inputs={X=['gradient_merge_step'], Y=['gradient_merge_zero']}, axis = -1, force_cpu = False, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['layer_norm_38.w_0_beta2_pow_acc_0', 'linear_93.b_0_fp32_master_0_moment2_0', 'layer_norm_24.b_0_beta2_pow_acc_0', 'opt_tmp_125', 'linear_69.b_0_fp32_master_0', 'layer_norm_41.b_0@GRAD@MERGE', 'linear_84.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_87.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_46.b_0_moment1_0', 'linear_70.b_0_fp32_master_0', 'linear_64.w_0_fp32_master_0_moment1_0', 'linear_82.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_40.w_0@GRAD@MERGE', 'layer_norm_32.b_0@GRAD@MERGE', 'linear_61.b_0_fp32_master_0', 'linear_66.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_81.w_0_fp32_master_0_moment2_0', 'linear_93.b_0_fp32_master_0', 'linear_66.b_0_fp32_master_0_moment1_0', 'layer_norm_24.b_0', 'layer_norm_32.w_0_moment1_0', 'linear_77.b_0_fp32_master_0_moment1_0', 'linear_63.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_42.b_0_moment1_0', 'layer_norm_27.b_0@GRAD@MERGE', 'linear_82.w_0_fp32_master_0_moment2_0', 'opt_tmp_140', 'linear_83.w_0@GRAD@MERGE', 'linear_76.w_0_fp32_master_0_moment1_0', 'linear_56.w_0_fp32_master_0_moment1_0', 'linear_65.w_0_fp32_master_0_moment2_0', 'linear_64.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_44.w_0', 'linear_77.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_49.w_0_fp32_master_0', 'linear_69.w_0', 'linear_88.b_0_fp32_master_0', 'layer_norm_48.w_0_moment1_0', 'layer_norm_35.w_0@GRAD@MERGE', 'linear_51.w_0_fp32_master_0_moment2_0', 'linear_54.w_0', 'linear_77.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_95.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_50.w_0@GRAD@MERGE', 'linear_58.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_85.b_0', 'linear_64.b_0@GRAD@MERGE', 'layer_norm_46.b_0_beta2_pow_acc_0', 'layer_norm_27.w_0@GRAD@MERGE', 'linear_52.w_0_fp32_master_0', 'linear_82.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_48.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_32.w_0', 'num_good_steps_0', 'linear_73.b_0_fp32_master_0', 'linear_67.w_0_fp32_master_0_moment1_0', 'linear_49.b_0', 'linear_52.w_0@GRAD@MERGE', 'linear_95.w_0_fp32_master_0', 'linear_76.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_75.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_35.b_0', 'layer_norm_36.w_0_beta2_pow_acc_0', 'linear_67.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_33.b_0_moment1_0', 'linear_73.w_0@GRAD@MERGE', 'linear_64.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_90.w_0_fp32_master_0_moment1_0', 'layer_norm_45.w_0_moment2_0', 'linear_64.b_0_fp32_master_0', 'linear_62.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_67.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_72.w_0@GRAD@MERGE', 'linear_88.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_48.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_91.w_0_fp32_master_0_moment1_0', 'layer_norm_44.b_0_moment1_0', 'layer_norm_28.b_0_moment2_0', 'layer_norm_40.b_0@GRAD@MERGE', 'layer_norm_44.b_0', 'linear_94.b_0_fp32_master_0_moment1_0', 'linear_62.w_0_fp32_master_0_moment2_0', 'layer_norm_42.b_0', 'linear_90.b_0_fp32_master_0_moment2_0', 'linear_68.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_143', 'linear_71.w_0_fp32_master_0_moment1_0', 'layer_norm_34.b_0_moment2_0', 'linear_89.b_0_fp32_master_0', 'linear_68.w_0_fp32_master_0_moment1_0', 'linear_56.w_0@GRAD@MERGE', 'linear_50.w_0', 'linear_82.b_0_fp32_master_0', 'linear_67.b_0@GRAD@MERGE', 'linear_88.w_0_fp32_master_0_moment1_0', 'linear_56.b_0', 'linear_80.w_0', 'layer_norm_31.b_0_moment2_0', 'linear_52.b_0@GRAD@MERGE', 'linear_95.w_0_fp32_master_0_moment1_0', 'linear_69.b_0_fp32_master_0_moment1_0', 'linear_84.w_0_fp32_master_0', 'layer_norm_40.b_0_beta1_pow_acc_0', 'layer_norm_44.w_0_beta2_pow_acc_0', 'linear_79.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_77.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_108', 'linear_71.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_62.b_0_fp32_master_0', 'linear_72.w_0_fp32_master_0_moment1_0', 'linear_84.b_0_fp32_master_0_moment1_0', 'linear_78.b_0_fp32_master_0', 'layer_norm_47.w_0_beta2_pow_acc_0', 'linear_59.b_0_fp32_master_0', 'linear_56.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_75.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_36.b_0@GRAD@MERGE', 'linear_51.w_0@GRAD@MERGE', 'layer_norm_38.w_0', 'linear_79.b_0_fp32_master_0_moment2_0', 'layer_norm_32.w_0@GRAD@MERGE', 'linear_79.b_0', 'layer_norm_32.w_0_beta2_pow_acc_0', 'linear_70.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_83.w_0', 'linear_61.b_0@GRAD@MERGE', 'linear_71.w_0_fp32_master_0', 'layer_norm_34.w_0_beta2_pow_acc_0', 'layer_norm_41.b_0_moment2_0', 'linear_88.w_0_fp32_master_0_moment2_0', 'linear_59.w_0_fp32_master_0', 'linear_70.b_0@GRAD@MERGE', 'layer_norm_46.b_0_moment2_0', 'linear_73.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_64.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_94.b_0', 'layer_norm_26.w_0', 'linear_95.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_45.b_0_moment1_0', 'opt_tmp_142', 'linear_74.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_elementwise_max_0', 'layer_norm_32.w_0_beta1_pow_acc_0', 'opt_tmp_129', 'layer_norm_38.b_0', 'linear_60.b_0', 'linear_56.b_0_fp32_master_0_moment1_0', 'layer_norm_32.b_0_moment2_0', 'linear_69.w_0_fp32_master_0_moment2_0', 'layer_norm_24.b_0@GRAD@MERGE', 'linear_72.w_0_fp32_master_0', 'linear_92.b_0_fp32_master_0', 'linear_93.w_0_fp32_master_0_moment2_0', 'linear_90.b_0_fp32_master_0_moment1_0', 'linear_72.b_0@GRAD@MERGE', 'layer_norm_48.w_0', 'layer_norm_29.b_0_beta2_pow_acc_0', 'linear_72.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_30.b_0@GRAD@MERGE', 'linear_54.b_0@GRAD@MERGE', 'opt_tmp_141', 'layer_norm_38.w_0_moment1_0', 'linear_90.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.b_0@GRAD@MERGE', 'linear_74.b_0_fp32_master_0_moment2_0', 'linear_50.b_0_fp32_master_0_moment1_0', 'layer_norm_27.w_0_moment2_0', 'opt_tmp_100', 'linear_76.w_0', 'linear_76.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_90.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_70.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_86.w_0@GRAD@MERGE', 'opt_tmp_124', 'linear_78.w_0@GRAD@MERGE', 'layer_norm_43.w_0@GRAD@MERGE', 'opt_opt_fill_constant_1.tmp_0', 'linear_54.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_39.b_0_moment2_0', 'opt_tmp_118', 'layer_norm_39.w_0_moment1_0', 'linear_58.b_0@GRAD@MERGE', 'opt_tmp_101', 'linear_88.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_61.w_0_fp32_master_0_moment2_0', 'linear_94.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_31.b_0', 'layer_norm_43.w_0_moment1_0', 'linear_49.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_59.w_0', 'linear_52.b_0_fp32_master_0_moment2_0', 'linear_94.w_0_fp32_master_0_moment2_0', 'linear_94.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_59.b_0@GRAD@MERGE', 'opt_tmp_168', 'linear_52.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_37.w_0_moment1_0', 'layer_norm_25.w_0', 'layer_norm_36.b_0_beta1_pow_acc_0', 'opt_tmp_117', 'layer_norm_48.w_0_beta2_pow_acc_0', 'linear_75.b_0_fp32_master_0', 'linear_65.b_0_fp32_master_0', 'linear_70.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_64.b_0_fp32_master_0_moment2_0', 'linear_86.w_0_fp32_master_0', 'layer_norm_24.w_0_beta1_pow_acc_0', 'linear_51.b_0_fp32_master_0_moment1_0', 'layer_norm_33.b_0_beta2_pow_acc_0', 'linear_85.w_0_fp32_master_0_moment1_0', 'linear_61.w_0', 'linear_66.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_48.w_0_fp32_master_0', 'layer_norm_36.b_0_beta2_pow_acc_0', 'linear_88.w_0_fp32_master_0', 'linear_72.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_52.w_0_fp32_master_0_moment2_0', 'linear_57.b_0_fp32_master_0_moment1_0', 'linear_80.b_0', 'linear_64.w_0@GRAD@MERGE', 'layer_norm_40.b_0_beta2_pow_acc_0', 'linear_83.b_0@GRAD@MERGE', 'find_infinite_scale.@fp32_0@cast_int32', 'opt_tmp_148', 'opt_tmp_188', 'linear_64.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_69.w_0_fp32_master_0', 'linear_78.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_194', 'layer_norm_28.w_0_moment1_0', 'linear_80.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_86.b_0_fp32_master_0_moment1_0', 'linear_87.w_0_fp32_master_0_moment2_0', 'linear_90.w_0', 'layer_norm_46.w_0_beta1_pow_acc_0', 'linear_75.w_0', 'linear_51.b_0', 'linear_74.w_0_fp32_master_0_moment1_0', 'layer_norm_26.w_0_beta1_pow_acc_0', 'linear_88.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_68.b_0_fp32_master_0_moment2_0', 'layer_norm_26.b_0', 'layer_norm_38.b_0_beta2_pow_acc_0', 'linear_54.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_38.b_0_beta1_pow_acc_0', 'linear_83.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_86.b_0', 'opt_tmp_185', 'linear_91.b_0_fp32_master_0_moment1_0', 'linear_59.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_60.b_0_fp32_master_0', 'layer_norm_45.w_0_moment1_0', 'layer_norm_46.b_0@GRAD@MERGE', 'linear_94.w_0', 'linear_86.w_0_fp32_master_0_moment2_0', 'linear_93.b_0@GRAD@MERGE', 'layer_norm_24.w_0_beta2_pow_acc_0', 'linear_89.w_0_fp32_master_0_moment2_0', 'linear_60.w_0', 'layer_norm_24.w_0@GRAD@MERGE', 'layer_norm_29.b_0_moment2_0', 'layer_norm_38.w_0_beta1_pow_acc_0', 'linear_65.b_0_fp32_master_0_moment1_0', 'opt_tmp_172', 'layer_norm_45.w_0_beta1_pow_acc_0', 'layer_norm_48.b_0_moment2_0', 'opt_tmp_184', 'layer_norm_37.w_0_beta2_pow_acc_0', 'opt_tmp_173', 'layer_norm_39.b_0', 'linear_87.b_0_fp32_master_0_moment1_0', 'linear_58.w_0_fp32_master_0_moment1_0', 'opt_tmp_123', 'linear_49.b_0_fp32_master_0', 'linear_67.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_192', 'linear_76.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_58.b_0', 'linear_59.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_54.w_0_fp32_master_0', 'linear_63.w_0@GRAD@MERGE', 'linear_75.w_0_fp32_master_0_moment1_0', 'linear_94.w_0@GRAD@MERGE', 'layer_norm_37.b_0_beta1_pow_acc_0', 'linear_48.b_0@GRAD@MERGE', 'opt_tmp_161', 'linear_58.w_0', 'layer_norm_45.w_0@GRAD@MERGE', 'layer_norm_35.b_0_moment1_0', 'linear_89.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_48.w_0_beta1_pow_acc_0', 'layer_norm_34.b_0_moment1_0', 'linear_50.w_0_fp32_master_0_moment2_0', 'linear_80.w_0@GRAD@MERGE', 'linear_82.b_0_fp32_master_0_moment2_0', 'linear_74.b_0_fp32_master_0', 'linear_92.b_0@GRAD@MERGE', 'linear_93.w_0@GRAD@MERGE', 'linear_51.b_0_fp32_master_0_moment2_0', 'layer_norm_25.w_0_beta1_pow_acc_0', 'layer_norm_42.b_0_beta2_pow_acc_0', 'layer_norm_33.w_0_moment2_0', 'linear_54.w_0_fp32_master_0_moment1_0', 'linear_68.b_0', 'linear_78.b_0@GRAD@MERGE', 'linear_49.b_0_fp32_master_0_moment1_0', 'linear_81.w_0_fp32_master_0_moment1_0', 'linear_89.w_0@GRAD@MERGE', 'linear_68.b_0@GRAD@MERGE', 'layer_norm_43.w_0_moment2_0', 'layer_norm_48.b_0_beta2_pow_acc_0', 'linear_75.b_0@GRAD@MERGE', 'linear_48.b_0_fp32_master_0_moment1_0', 'linear_50.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_41.b_0_moment1_0', 'linear_71.b_0@GRAD@MERGE', 'linear_71.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_88.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_51.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_90.w_0@GRAD@MERGE', 'linear_91.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_70.w_0', 'layer_norm_34.w_0', 'layer_norm_48.b_0@GRAD@MERGE', 'linear_78.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_73.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_65.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_56.w_0', 'layer_norm_45.b_0', 'opt_tmp_127', 'layer_norm_27.w_0', 'linear_84.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_93.w_0_fp32_master_0', 'layer_norm_38.b_0_moment1_0', 'opt_tmp_112', 'layer_norm_33.w_0', 'linear_65.w_0@GRAD@MERGE', 'linear_91.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_67.b_0_fp32_master_0', 'linear_95.b_0@GRAD@MERGE', 'layer_norm_42.b_0_moment2_0', 'layer_norm_42.b_0@GRAD@MERGE', 'layer_norm_25.w_0_moment2_0', 'linear_79.w_0', 'linear_86.w_0', 'linear_85.b_0@GRAD@MERGE', 'linear_80.w_0_fp32_master_0', 'layer_norm_45.w_0', 'layer_norm_27.b_0_moment2_0', 'layer_norm_44.w_0_beta1_pow_acc_0', 'layer_norm_43.w_0_beta2_pow_acc_0', 'linear_68.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_75.b_0', 'opt_tmp_158', 'layer_norm_44.b_0_moment2_0', 'linear_64.b_0', 'opt_tmp_106', 'linear_64.b_0_fp32_master_0_moment1_0', 'linear_75.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_85.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_151', 'layer_norm_46.w_0@GRAD@MERGE', 'linear_70.w_0_fp32_master_0_moment2_0', 'layer_norm_37.b_0_moment1_0', 'linear_77.w_0_fp32_master_0_moment2_0', 'linear_79.w_0_fp32_master_0_moment1_0', 'linear_86.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_42.w_0_beta1_pow_acc_0', 'linear_78.b_0_fp32_master_0_moment1_0', 'layer_norm_25.b_0_beta1_pow_acc_0', 'linear_54.b_0_fp32_master_0_moment1_0', 'layer_norm_28.b_0', 'layer_norm_37.b_0@GRAD@MERGE', 'linear_66.w_0', 'linear_80.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_90.b_0', 'layer_norm_30.w_0_beta2_pow_acc_0', 'layer_norm_37.b_0', 'layer_norm_41.w_0_moment1_0', 'linear_56.b_0_fp32_master_0', 'layer_norm_32.b_0_moment1_0', 'linear_91.w_0_fp32_master_0_moment2_0', 'linear_92.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_34.w_0_moment1_0', 'linear_72.b_0_fp32_master_0_moment1_0', 'linear_58.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_63.w_0_fp32_master_0', 'linear_57.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_177', 'layer_norm_30.b_0_moment2_0', 'linear_55.w_0@GRAD@MERGE', 'linear_67.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_116', 'linear_66.w_0_fp32_master_0', 'linear_54.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_76.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_80.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_30.w_0_moment1_0', 'linear_85.w_0', 'linear_61.w_0@GRAD@MERGE', 'opt_tmp_144', 'linear_57.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.w_0_fp32_master_0_moment2_0', 'linear_87.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_46.w_0', 'layer_norm_25.w_0@GRAD@MERGE', 'linear_67.b_0_fp32_master_0_moment1_0', 'linear_79.w_0_fp32_master_0', 'linear_94.b_0@GRAD@MERGE', 'linear_61.b_0_fp32_master_0_moment2_0', 'linear_80.b_0_fp32_master_0_moment2_0', 'linear_71.b_0', 'layer_norm_39.b_0_moment1_0', 'linear_82.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.w_0_fp32_master_0', 'linear_64.w_0_fp32_master_0_moment2_0', 'opt_tmp_120', 'linear_73.b_0_fp32_master_0_moment1_0', 'layer_norm_41.b_0_beta2_pow_acc_0', 'linear_50.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_80.b_0_fp32_master_0_moment1_0', 'linear_92.b_0_fp32_master_0_moment2_0', 'linear_51.w_0_fp32_master_0_moment1_0', 'layer_norm_34.b_0', 'linear_83.b_0_fp32_master_0_moment1_0', 'linear_89.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_83.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_81.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_27.w_0_beta1_pow_acc_0', 'layer_norm_47.b_0_beta1_pow_acc_0', 'find_infinite_scale.@fp32_0', 'linear_57.w_0@GRAD@MERGE', 'linear_77.w_0_fp32_master_0', 'layer_norm_29.w_0_moment1_0', 'layer_norm_30.w_0@GRAD@MERGE', 'layer_norm_39.w_0', 'linear_75.w_0_fp32_master_0_moment2_0', 'linear_53.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_63.b_0_fp32_master_0_moment1_0', 'layer_norm_37.b_0_beta2_pow_acc_0', 'layer_norm_40.b_0_moment2_0', 'linear_52.b_0', 'linear_65.w_0', 'linear_74.w_0', 'layer_norm_37.w_0_moment2_0', 'linear_66.b_0', 'linear_86.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_61.b_0_fp32_master_0_moment1_0', 'opt_tmp_153', 'linear_60.b_0_fp32_master_0_moment1_0', 'linear_70.w_0_fp32_master_0', 'linear_68.w_0', 'linear_71.b_0_fp32_master_0_moment1_0', 'layer_norm_43.w_0', 'linear_62.w_0@GRAD@MERGE', 'layer_norm_43.b_0@GRAD@MERGE', 'linear_63.b_0_fp32_master_0_moment2_0', 'linear_78.b_0_fp32_master_0_moment2_0', 'layer_norm_34.w_0_moment2_0', 'layer_norm_30.b_0_moment1_0', 'linear_81.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_35.b_0_beta2_pow_acc_0', 'linear_72.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_31.b_0@GRAD@MERGE', 'layer_norm_35.w_0_beta1_pow_acc_0', 'linear_85.b_0_fp32_master_0', 'opt_tmp_155', 'layer_norm_47.w_0', 'linear_86.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_83.b_0_fp32_master_0', 'linear_93.b_0_fp32_master_0_moment1_0', 'linear_54.w_0@GRAD@MERGE', 'linear_57.w_0_fp32_master_0_moment1_0', 'linear_85.b_0_fp32_master_0_moment1_0', 'layer_norm_24.w_0_moment1_0', 'layer_norm_35.w_0_beta2_pow_acc_0', 'linear_53.b_0@GRAD@MERGE', 'linear_57.b_0_fp32_master_0', 'linear_88.b_0', 'linear_71.w_0_fp32_master_0_moment2_0', 'linear_72.b_0', 'layer_norm_41.w_0_moment2_0', 'linear_56.b_0@GRAD@MERGE', 'layer_norm_38.w_0_moment2_0', 'linear_95.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_35.w_0_moment1_0', 'layer_norm_29.b_0_moment1_0', 'linear_66.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_44.b_0_beta2_pow_acc_0', 'opt_tmp_181', 'layer_norm_24.b_0_moment2_0', 'layer_norm_31.w_0_moment1_0', 'opt_tmp_121', 'linear_74.w_0_fp32_master_0_moment2_0', 'layer_norm_26.b_0_moment2_0', 'linear_83.w_0_fp32_master_0_moment1_0', 'layer_norm_35.w_0_moment2_0', 'linear_71.b_0_fp32_master_0_moment2_0', 'layer_norm_45.b_0_beta2_pow_acc_0', 'linear_93.w_0_fp32_master_0_moment1_0', 'linear_89.w_0', 'layer_norm_48.w_0_moment2_0', 'layer_norm_39.w_0_moment2_0', 'linear_81.b_0_fp32_master_0_moment2_0', 'linear_90.b_0@GRAD@MERGE', 'layer_norm_42.w_0@GRAD@MERGE', 'linear_89.b_0_fp32_master_0_moment1_0', 'linear_91.b_0_fp32_master_0', 'layer_norm_25.b_0', 'linear_60.b_0_fp32_master_0_moment2_0', 'linear_55.b_0_fp32_master_0_moment1_0', 'linear_48.b_0_fp32_master_0', 'layer_norm_26.b_0@GRAD@MERGE', 'linear_51.w_0_fp32_master_0', 'layer_norm_43.b_0_moment2_0', 'linear_70.w_0@GRAD@MERGE', 'linear_50.b_0_fp32_master_0', 'layer_norm_38.b_0@GRAD@MERGE', 'layer_norm_27.w_0_moment1_0', 'linear_68.w_0_fp32_master_0_moment2_0', 'linear_81.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.w_0', 'linear_75.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_66.w_0@GRAD@MERGE', 'layer_norm_34.b_0@GRAD@MERGE', 'layer_norm_27.w_0_beta2_pow_acc_0', 'linear_81.b_0_fp32_master_0_moment1_0', 'linear_95.b_0', 'linear_66.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_92.w_0@GRAD@MERGE', 'opt_tmp_130', 'linear_77.b_0_fp32_master_0', 'linear_67.b_0', 'linear_87.b_0', 'layer_norm_42.w_0_moment2_0', 'linear_89.b_0@GRAD@MERGE', 'opt_tmp_159', 'linear_48.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_52.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.b_0_fp32_master_0', 'layer_norm_34.b_0_beta2_pow_acc_0', 'layer_norm_25.b_0_moment1_0', 'layer_norm_40.b_0_moment1_0', 'linear_54.b_0_fp32_master_0_moment2_0', 'layer_norm_43.b_0_beta1_pow_acc_0', 'layer_norm_47.b_0_moment1_0', 'linear_86.b_0_fp32_master_0', 'layer_norm_26.w_0_moment1_0', 'linear_62.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_77.b_0', 'linear_85.w_0_fp32_master_0_moment2_0', 'linear_78.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_179', 'layer_norm_36.w_0@GRAD@MERGE', 'linear_52.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_28.b_0_beta2_pow_acc_0', 'linear_77.w_0', 'linear_84.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_56.b_0_fp32_master_0_moment2_0', 'opt_tmp_145', 'layer_norm_37.w_0@GRAD@MERGE', 'linear_87.w_0_fp32_master_0', 'linear_82.w_0_fp32_master_0_moment1_0', 'linear_77.w_0_fp32_master_0_moment1_0', 'linear_79.w_0_fp32_master_0_moment2_0', 'layer_norm_33.b_0', 'linear_62.b_0', 'layer_norm_35.b_0_beta1_pow_acc_0', 'layer_norm_41.w_0', 'linear_94.b_0_fp32_master_0', 'layer_norm_39.w_0@GRAD@MERGE', 'layer_norm_45.b_0@GRAD@MERGE', 'linear_81.w_0', 'linear_55.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_47.b_0', 'layer_norm_44.w_0_moment1_0', 'layer_norm_25.b_0_moment2_0', 'opt_tmp_137', 'linear_74.w_0@GRAD@MERGE', 'linear_68.w_0@GRAD@MERGE', 'layer_norm_28.b_0_beta1_pow_acc_0', 'linear_80.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_34.w_0@GRAD@MERGE', 'layer_norm_47.b_0_moment2_0', 'opt_tmp_104', 'linear_85.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_48.w_0@GRAD@MERGE', 'layer_norm_30.b_0', 'linear_87.w_0', 'linear_95.w_0_fp32_master_0_moment2_0', 'layer_norm_31.w_0', 'linear_83.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_80.w_0_fp32_master_0_moment2_0', 'linear_87.b_0_fp32_master_0_moment2_0', 'linear_84.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_86.b_0_fp32_master_0_moment2_0', 'layer_norm_29.w_0', 'linear_85.w_0_fp32_master_0', 'linear_63.b_0', 'layer_norm_39.b_0_beta2_pow_acc_0', 'layer_norm_39.b_0@GRAD@MERGE', 'find_infinite_scale.tmp_0', 'linear_82.b_0@GRAD@MERGE', 'layer_norm_47.w_0_moment1_0', 'layer_norm_31.w_0@GRAD@MERGE', 'layer_norm_25.b_0@GRAD@MERGE', 'opt_tmp_156', 'layer_norm_47.w_0@GRAD@MERGE', 'linear_84.w_0_fp32_master_0_moment1_0', 'opt_tmp_102', 'linear_85.w_0@GRAD@MERGE', 'opt_tmp_131', 'linear_49.w_0_fp32_master_0_moment1_0', 'linear_66.w_0_fp32_master_0_moment2_0', 'layer_norm_37.w_0', 'linear_62.w_0_fp32_master_0_moment1_0', 'linear_63.w_0_fp32_master_0_moment1_0', 'linear_56.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_81.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_41.w_0@GRAD@MERGE', 'linear_58.w_0_fp32_master_0_moment2_0', 'layer_norm_25.b_0_beta2_pow_acc_0', 'layer_norm_32.b_0_beta2_pow_acc_0', 'opt_tmp_128', 'linear_74.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_109', 'linear_82.b_0', 'layer_norm_36.b_0', 'linear_92.w_0', 'linear_94.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_71.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_62.b_0@GRAD@MERGE', 'opt_tmp_115', 'linear_60.w_0_fp32_master_0_moment2_0', 'linear_53.b_0_fp32_master_0_moment1_0', 'layer_norm_32.b_0_beta1_pow_acc_0', 'linear_75.b_0_fp32_master_0_moment1_0', 'linear_58.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_28.w_0_beta2_pow_acc_0', 'layer_norm_34.w_0_beta1_pow_acc_0', 'linear_51.w_0', 'layer_norm_24.b_0_moment1_0', 'linear_69.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_62.w_0', 'layer_norm_40.w_0_beta1_pow_acc_0', 'layer_norm_40.w_0_moment2_0', 'layer_norm_48.b_0_beta1_pow_acc_0', 'opt_tmp_139', 'linear_79.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_29.w_0_beta1_pow_acc_0', 'linear_79.b_0@GRAD@MERGE', 'linear_49.w_0', 'linear_87.w_0_fp32_master_0_moment1_0', 'opt_tmp_157', 'layer_norm_48.b_0_moment1_0', 'linear_63.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_35.w_0', 'opt_tmp_149', 'opt_tmp_147', 'linear_63.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_37.b_0_moment2_0', 'linear_79.b_0_fp32_master_0_moment1_0', 'linear_71.w_0', 'linear_95.b_0_fp32_master_0_moment1_0', 'linear_71.w_0@GRAD@MERGE', 'memcopy__0', 'linear_53.b_0_fp32_master_0_moment2_0', 'linear_84.w_0@GRAD@MERGE', 'opt_tmp_170', 'opt_tmp_182', 'linear_74.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_86.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_29.w_0@GRAD@MERGE', 'layer_norm_31.b_0_beta1_pow_acc_0', 'linear_55.b_0@GRAD@MERGE', 'linear_93.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_63.w_0', 'opt_tmp_110', 'layer_norm_28.w_0', 'linear_49.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_82.w_0', 'layer_norm_41.b_0', 'linear_86.b_0@GRAD@MERGE', 'opt_tmp_154', 'linear_91.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_50.w_0_fp32_master_0', 'linear_93.b_0', 'linear_90.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_31.b_0_beta2_pow_acc_0', 'layer_norm_24.b_0_beta1_pow_acc_0', 'layer_norm_40.w_0_moment1_0', 'linear_53.w_0@GRAD@MERGE', 'layer_norm_36.w_0_moment1_0', 'opt_opt_sum_2.tmp_0', 'opt_tmp_162', 'linear_61.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_75.b_0_fp32_master_0_moment2_0', 'layer_norm_28.w_0_beta1_pow_acc_0', 'linear_68.w_0_fp32_master_0', 'linear_76.b_0_fp32_master_0_moment2_0', 'layer_norm_44.w_0_moment2_0', 'linear_76.w_0_fp32_master_0_moment2_0', 'linear_84.b_0@GRAD@MERGE', 'linear_76.w_0_fp32_master_0', 'opt_tmp_150', 'linear_57.w_0_fp32_master_0', 'linear_59.w_0_fp32_master_0_moment2_0', 'linear_52.b_0_fp32_master_0_moment1_0', 'linear_75.w_0_fp32_master_0', 'opt_tmp_119', 'linear_81.b_0', 'layer_norm_43.b_0_moment1_0', 'layer_norm_48.w_0@GRAD@MERGE', 'layer_norm_29.b_0_beta1_pow_acc_0', 'linear_58.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_93.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_46.b_0', 'opt_tmp_169', 'layer_norm_37.w_0_beta1_pow_acc_0', 'layer_norm_42.b_0_beta1_pow_acc_0', 'layer_norm_28.w_0_moment2_0', 'find_infinite_scale.@fp16_0@cast_int32', 'linear_56.w_0_fp32_master_0', 'layer_norm_30.b_0_beta1_pow_acc_0', 'linear_59.w_0_fp32_master_0_moment1_0', 'linear_72.b_0_fp32_master_0_moment2_0', 'opt_tmp_103', 'opt_tmp_152', 'linear_88.b_0_fp32_master_0_moment1_0', 'linear_89.w_0_fp32_master_0_moment1_0', 'linear_74.w_0_fp32_master_0', 'linear_84.b_0_fp32_master_0_moment2_0', 'linear_68.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_89.b_0', 'linear_92.b_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_135', 'linear_54.b_0', 'linear_94.b_0_fp32_master_0_moment2_0', 'linear_53.b_0_fp32_master_0', 'linear_60.w_0_fp32_master_0_moment1_0', 'linear_61.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.b_0_fp32_master_0_beta2_pow_acc_0', 'loss_scaling_0', 'layer_norm_31.w_0_beta1_pow_acc_0', 'linear_65.b_0@GRAD@MERGE', 'linear_50.b_0', 'linear_70.b_0', 'layer_norm_33.b_0_moment2_0', 'layer_norm_28.b_0@GRAD@MERGE', 'linear_67.w_0@GRAD@MERGE', 'linear_93.w_0', 'linear_64.w_0', 'layer_norm_31.b_0_moment1_0', 'linear_76.b_0_fp32_master_0_moment1_0', 'layer_norm_43.b_0', 'layer_norm_33.b_0_beta1_pow_acc_0', 'layer_norm_42.w_0_beta2_pow_acc_0', 'linear_48.w_0_fp32_master_0_moment1_0', 'layer_norm_35.b_0_moment2_0', 'linear_95.b_0_fp32_master_0_moment2_0', 'layer_norm_29.b_0@GRAD@MERGE', 'layer_norm_28.w_0@GRAD@MERGE', 'linear_58.b_0_fp32_master_0_moment2_0', 'linear_95.w_0', 'linear_65.w_0_fp32_master_0', 'linear_95.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_48.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_24.w_0_moment2_0', 'opt_tmp_164', 'linear_88.w_0', 'layer_norm_42.w_0_moment1_0', 'linear_54.w_0_fp32_master_0_moment2_0', 'linear_51.b_0_fp32_master_0', 'layer_norm_30.w_0_beta1_pow_acc_0', 'opt_tmp_176', 'linear_73.w_0_fp32_master_0_moment2_0', 'linear_83.w_0_fp32_master_0_moment2_0', 'layer_norm_39.w_0_beta2_pow_acc_0', 'layer_norm_24.w_0', 'linear_91.b_0', 'linear_87.b_0@GRAD@MERGE', 'layer_norm_46.b_0_beta1_pow_acc_0', 'linear_57.w_0', 'linear_66.w_0_fp32_master_0_moment1_0', 'linear_70.w_0_fp32_master_0_moment1_0', 'linear_82.w_0@GRAD@MERGE', 'opt_tmp_171', 'opt_tmp_175', 'linear_84.b_0_fp32_master_0', 'linear_60.w_0@GRAD@MERGE', 'linear_55.w_0', 'layer_norm_45.w_0_beta2_pow_acc_0', 'linear_81.w_0@GRAD@MERGE', 'linear_83.w_0_fp32_master_0', 'linear_50.w_0_fp32_master_0_moment1_0', 'linear_63.b_0@GRAD@MERGE', 'linear_91.w_0@GRAD@MERGE', 'linear_80.b_0@GRAD@MERGE', 'find_infinite_scale.@fp16_0', 'linear_60.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_73.b_0@GRAD@MERGE', 'layer_norm_27.b_0_beta1_pow_acc_0', 'linear_63.b_0_fp32_master_0', 'layer_norm_32.b_0', 'linear_76.w_0@GRAD@MERGE', 'opt_tmp_186', 'opt_tmp_193', 'linear_87.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_51.b_0@GRAD@MERGE', 'layer_norm_47.b_0_beta2_pow_acc_0', 'linear_53.w_0', 'opt_tmp_132', 'linear_70.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_87.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_59.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_58.w_0_fp32_master_0', 'linear_59.b_0', 'linear_72.b_0_fp32_master_0', 'linear_69.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_59.b_0_fp32_master_0_moment2_0', 'linear_53.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_55.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_72.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_52.w_0', 'linear_85.b_0_fp32_master_0_moment2_0', 'linear_74.b_0@GRAD@MERGE', 'linear_92.b_0_fp32_master_0_moment1_0', 'linear_53.b_0', 'opt_elementwise_div_0', 'linear_59.b_0_fp32_master_0_moment1_0', 'linear_67.w_0_fp32_master_0', 'opt_tmp_105', 'linear_82.w_0_fp32_master_0', 'linear_90.b_0_fp32_master_0', 'layer_norm_44.b_0_beta1_pow_acc_0', 'concat.tmp_0', 'linear_78.w_0_fp32_master_0', 'linear_57.b_0@GRAD@MERGE', 'linear_58.w_0@GRAD@MERGE', 'linear_50.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_49.w_0@GRAD@MERGE', 'linear_57.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_78.b_0', 'opt_tmp_191', 'layer_norm_36.w_0_moment2_0', 'linear_81.b_0_fp32_master_0', 'opt_tmp_114', 'layer_norm_33.b_0@GRAD@MERGE', 'linear_49.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_69.b_0@GRAD@MERGE', 'linear_73.b_0_fp32_master_0_moment2_0', 'linear_62.w_0_fp32_master_0', 'linear_92.w_0_fp32_master_0_moment1_0', 'linear_48.w_0', 'linear_73.w_0_fp32_master_0', 'linear_79.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_41.b_0_beta1_pow_acc_0', 'linear_69.w_0_fp32_master_0_moment1_0', 'linear_73.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_69.w_0@GRAD@MERGE', 'linear_57.b_0_fp32_master_0_moment2_0', 'linear_78.w_0', 'linear_66.b_0@GRAD@MERGE', 'opt_tmp_167', 'linear_90.w_0_fp32_master_0', 'linear_83.b_0_fp32_master_0_moment2_0', 'opt_tmp_111', 'linear_56.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.w_0_fp32_master_0', 'linear_82.b_0_fp32_master_0_moment1_0', 'layer_norm_29.b_0', 'linear_50.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_63.w_0_fp32_master_0_moment2_0', 'linear_52.w_0_fp32_master_0_moment1_0', 'linear_91.b_0@GRAD@MERGE', 'linear_56.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_77.w_0@GRAD@MERGE', 'linear_61.w_0_fp32_master_0_moment1_0', 'linear_65.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_89.w_0_fp32_master_0', 'linear_57.b_0', 'linear_81.b_0@GRAD@MERGE', 'opt_tmp_122', 'opt_tmp_113', 'linear_72.w_0', 'linear_78.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_53.w_0_fp32_master_0_moment1_0', 'layer_norm_41.w_0_beta2_pow_acc_0', 'linear_65.b_0', 'linear_61.w_0_fp32_master_0', 'layer_norm_41.w_0_beta1_pow_acc_0', 'layer_norm_30.b_0_beta2_pow_acc_0', 'linear_85.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_93.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_160', 'linear_92.w_0_fp32_master_0', 'linear_53.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_67.w_0_fp32_master_0_moment2_0', 'linear_63.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_43.b_0_beta2_pow_acc_0', 'layer_norm_47.w_0_moment2_0', 'opt_opt_sqrt_0.tmp_0', 'linear_65.b_0_fp32_master_0_moment2_0', 'linear_80.w_0_fp32_master_0_moment1_0', 'opt_tmp_189', 'linear_89.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_133', 'linear_56.w_0_fp32_master_0_moment2_0', 'linear_65.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_88.b_0@GRAD@MERGE', 'linear_52.b_0_fp32_master_0', 'linear_49.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_69.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_76.b_0@GRAD@MERGE', 'opt_tmp_166', 'layer_norm_26.w_0_moment2_0', 'opt_tmp_180', 'linear_66.b_0_fp32_master_0', 'linear_66.b_0_fp32_master_0_moment2_0', 'linear_70.b_0_fp32_master_0_moment2_0', 'layer_norm_28.b_0_moment1_0', 'linear_61.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_36.b_0_moment1_0', 'linear_77.b_0_fp32_master_0_moment2_0', 'linear_49.b_0@GRAD@MERGE', 'linear_85.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_53.w_0_fp32_master_0_moment2_0', 'opt_tmp_107', 'linear_94.w_0_fp32_master_0', 'opt_tmp_163', 'linear_62.b_0_fp32_master_0_moment1_0', 'linear_59.w_0@GRAD@MERGE', 'linear_72.w_0_fp32_master_0_moment2_0', 'linear_62.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.b_0_fp32_master_0_moment2_0', 'opt_tmp_126', 'layer_norm_30.w_0', 'linear_61.b_0', 'opt_tmp_146', 'opt_tmp_174', 'layer_norm_36.w_0', 'linear_83.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_183', 'layer_norm_27.b_0', 'linear_74.b_0', 'layer_norm_33.w_0_beta1_pow_acc_0', 'linear_57.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_78.w_0_fp32_master_0_moment2_0', 'linear_48.w_0_fp32_master_0_moment2_0', 'linear_50.b_0@GRAD@MERGE', 'layer_norm_47.b_0@GRAD@MERGE', 'linear_58.b_0_fp32_master_0', 'linear_77.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_26.b_0_moment1_0', 'linear_73.b_0', 'linear_88.w_0@GRAD@MERGE', 'linear_71.b_0_fp32_master_0', 'opt_tmp_134', 'layer_norm_39.w_0_beta1_pow_acc_0', 'linear_87.w_0@GRAD@MERGE', 'linear_57.w_0_fp32_master_0_moment2_0', 'layer_norm_36.w_0_beta1_pow_acc_0', 'linear_53.w_0_fp32_master_0', 'linear_49.w_0_fp32_master_0_moment2_0', 'linear_48.b_0', 'opt_tmp_190', 'linear_65.w_0_fp32_master_0_moment1_0', 'linear_90.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_95.w_0@GRAD@MERGE', 'linear_88.b_0_fp32_master_0_moment2_0', 'linear_67.b_0_fp32_master_0_moment2_0', 'layer_norm_39.b_0_beta1_pow_acc_0', 'linear_84.b_0', 'linear_94.w_0_fp32_master_0_moment1_0', 'linear_51.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_55.w_0_fp32_master_0_moment1_0', 'opt_tmp_187', 'linear_52.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_71.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_46.w_0_beta2_pow_acc_0', 'layer_norm_47.w_0_beta1_pow_acc_0', 'linear_83.b_0', 'layer_norm_38.b_0_moment2_0', 'linear_59.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_27.b_0_beta2_pow_acc_0', 'layer_norm_33.w_0_moment1_0', 'linear_60.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_26.b_0_beta1_pow_acc_0', 'linear_58.b_0_fp32_master_0_moment1_0', 'linear_76.b_0', 'opt_tmp_136', 'linear_50.b_0_fp32_master_0_moment2_0', 'linear_86.w_0_fp32_master_0_moment1_0', 'layer_norm_25.w_0_beta2_pow_acc_0', 'layer_norm_43.w_0_beta1_pow_acc_0', 'linear_92.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_73.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_68.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_33.w_0_beta2_pow_acc_0', 'linear_89.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_46.w_0_moment1_0', 'linear_55.b_0_fp32_master_0_moment2_0', 'linear_93.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_95.b_0_fp32_master_0', 'layer_norm_35.b_0@GRAD@MERGE', 'opt_tmp_165', 'linear_54.b_0_fp32_master_0', 'linear_55.b_0', 'linear_65.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_54.b_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_178', 'linear_67.w_0', 'layer_norm_46.w_0_moment2_0', 'linear_90.w_0_fp32_master_0_moment2_0', 'layer_norm_26.w_0_beta2_pow_acc_0', 'linear_81.w_0_fp32_master_0', 'linear_94.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_78.w_0_fp32_master_0_moment1_0', 'layer_norm_30.w_0_moment2_0', 'linear_69.b_0_fp32_master_0_moment2_0', 'linear_84.w_0', 'linear_69.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_44.w_0@GRAD@MERGE', 'layer_norm_26.b_0_beta2_pow_acc_0', 'layer_norm_45.b_0_moment2_0', 'linear_87.b_0_fp32_master_0', 'layer_norm_42.w_0', 'linear_70.b_0_fp32_master_0_moment1_0', 'linear_79.w_0@GRAD@MERGE', 'linear_69.b_0', 'linear_55.w_0_fp32_master_0', 'layer_norm_45.b_0_beta1_pow_acc_0', 'layer_norm_40.w_0_beta2_pow_acc_0', 'linear_92.b_0', 'layer_norm_48.b_0', 'linear_73.w_0_fp32_master_0_moment1_0', 'layer_norm_36.b_0_moment2_0', 'linear_79.b_0_fp32_master_0', 'layer_norm_29.w_0_moment2_0', 'linear_89.b_0_fp32_master_0_moment2_0', 'linear_51.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_82.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_27.b_0_moment1_0', 'layer_norm_33.w_0@GRAD@MERGE', 'linear_79.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_77.b_0@GRAD@MERGE', 'linear_53.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_29.w_0_beta2_pow_acc_0', 'linear_61.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_32.w_0_moment2_0', 'linear_76.b_0_fp32_master_0', 'linear_62.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_34.b_0_beta1_pow_acc_0', 'linear_62.b_0_fp32_master_0_moment2_0', 'layer_norm_31.w_0_beta2_pow_acc_0', 'layer_norm_40.b_0', 'linear_64.w_0_fp32_master_0', 'layer_norm_31.w_0_moment2_0', 'linear_92.w_0_fp32_master_0_moment2_0', 'opt_tmp_99', 'linear_80.b_0_fp32_master_0', 'layer_norm_40.w_0', 'linear_92.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_138', 'layer_norm_44.b_0@GRAD@MERGE', 'linear_49.b_0_fp32_master_0_moment2_0', 'layer_norm_26.w_0@GRAD@MERGE', 'linear_74.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_74.b_0_fp32_master_0_moment1_0', 'linear_75.w_0@GRAD@MERGE', 'linear_68.b_0_fp32_master_0', 'linear_68.b_0_fp32_master_0_moment1_0', 'layer_norm_38.w_0@GRAD@MERGE', 'linear_48.b_0_fp32_master_0_moment2_0', 'layer_norm_25.w_0_moment1_0', 'linear_73.w_0', 'num_bad_steps_0', 'linear_51.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_84.w_0_fp32_master_0_moment2_0'], Scope=['_generated_var_0']} = conditional_block(inputs={Cond=['gradient_merge_cond'], Input=['layer_norm_38.w_0_beta2_pow_acc_0', 'linear_93.b_0_fp32_master_0_moment2_0', 'layer_norm_24.b_0_beta2_pow_acc_0', 'linear_69.b_0_fp32_master_0', 'layer_norm_41.b_0@GRAD@MERGE', 'linear_84.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_87.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_46.b_0_moment1_0', 'linear_70.b_0_fp32_master_0', 'linear_64.w_0_fp32_master_0_moment1_0', 'linear_82.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_40.w_0@GRAD@MERGE', 'layer_norm_32.b_0@GRAD@MERGE', 'linear_61.b_0_fp32_master_0', 'linear_66.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_81.w_0_fp32_master_0_moment2_0', 'linear_93.b_0_fp32_master_0', 'linear_66.b_0_fp32_master_0_moment1_0', 'layer_norm_24.b_0', 'layer_norm_32.w_0_moment1_0', 'linear_77.b_0_fp32_master_0_moment1_0', 'linear_63.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_42.b_0_moment1_0', 'layer_norm_27.b_0@GRAD@MERGE', 'linear_82.w_0_fp32_master_0_moment2_0', 'linear_83.w_0@GRAD@MERGE', 'linear_76.w_0_fp32_master_0_moment1_0', 'linear_56.w_0_fp32_master_0_moment1_0', 'linear_65.w_0_fp32_master_0_moment2_0', 'linear_64.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_44.w_0', 'linear_77.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_49.w_0_fp32_master_0', 'linear_69.w_0', 'linear_88.b_0_fp32_master_0', 'layer_norm_48.w_0_moment1_0', 'layer_norm_35.w_0@GRAD@MERGE', 'linear_51.w_0_fp32_master_0_moment2_0', 'linear_54.w_0', 'linear_77.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_95.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_50.w_0@GRAD@MERGE', 'linear_58.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_85.b_0', 'linear_64.b_0@GRAD@MERGE', 'layer_norm_46.b_0_beta2_pow_acc_0', 'layer_norm_27.w_0@GRAD@MERGE', 'linear_52.w_0_fp32_master_0', 'linear_82.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_48.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_32.w_0', 'num_good_steps_0', 'linear_73.b_0_fp32_master_0', 'linear_67.w_0_fp32_master_0_moment1_0', 'linear_49.b_0', 'linear_52.w_0@GRAD@MERGE', 'linear_95.w_0_fp32_master_0', 'linear_76.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_75.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_35.b_0', 'layer_norm_36.w_0_beta2_pow_acc_0', 'linear_67.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_33.b_0_moment1_0', 'linear_73.w_0@GRAD@MERGE', 'linear_64.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_90.w_0_fp32_master_0_moment1_0', 'layer_norm_45.w_0_moment2_0', 'linear_64.b_0_fp32_master_0', 'linear_62.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_67.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_72.w_0@GRAD@MERGE', 'linear_88.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_48.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_91.w_0_fp32_master_0_moment1_0', 'layer_norm_44.b_0_moment1_0', 'layer_norm_28.b_0_moment2_0', 'layer_norm_40.b_0@GRAD@MERGE', 'layer_norm_44.b_0', 'linear_94.b_0_fp32_master_0_moment1_0', 'linear_62.w_0_fp32_master_0_moment2_0', 'layer_norm_42.b_0', 'linear_90.b_0_fp32_master_0_moment2_0', 'linear_68.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_71.w_0_fp32_master_0_moment1_0', 'layer_norm_34.b_0_moment2_0', 'linear_89.b_0_fp32_master_0', 'linear_68.w_0_fp32_master_0_moment1_0', 'linear_56.w_0@GRAD@MERGE', 'linear_50.w_0', 'linear_82.b_0_fp32_master_0', 'linear_67.b_0@GRAD@MERGE', 'linear_88.w_0_fp32_master_0_moment1_0', 'linear_56.b_0', 'linear_80.w_0', 'layer_norm_31.b_0_moment2_0', 'linear_52.b_0@GRAD@MERGE', 'linear_95.w_0_fp32_master_0_moment1_0', 'linear_69.b_0_fp32_master_0_moment1_0', 'linear_84.w_0_fp32_master_0', 'layer_norm_40.b_0_beta1_pow_acc_0', 'layer_norm_44.w_0_beta2_pow_acc_0', 'linear_79.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_77.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_71.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_62.b_0_fp32_master_0', 'linear_72.w_0_fp32_master_0_moment1_0', 'linear_84.b_0_fp32_master_0_moment1_0', 'linear_78.b_0_fp32_master_0', 'layer_norm_47.w_0_beta2_pow_acc_0', 'linear_59.b_0_fp32_master_0', 'linear_56.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_75.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_36.b_0@GRAD@MERGE', 'linear_51.w_0@GRAD@MERGE', 'layer_norm_38.w_0', 'linear_79.b_0_fp32_master_0_moment2_0', 'layer_norm_32.w_0@GRAD@MERGE', 'linear_79.b_0', 'layer_norm_32.w_0_beta2_pow_acc_0', 'linear_70.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_83.w_0', 'linear_61.b_0@GRAD@MERGE', 'linear_71.w_0_fp32_master_0', 'layer_norm_34.w_0_beta2_pow_acc_0', 'layer_norm_41.b_0_moment2_0', 'linear_88.w_0_fp32_master_0_moment2_0', 'linear_59.w_0_fp32_master_0', 'linear_70.b_0@GRAD@MERGE', 'layer_norm_46.b_0_moment2_0', 'linear_73.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_64.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_94.b_0', 'layer_norm_26.w_0', 'linear_95.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_45.b_0_moment1_0', 'linear_74.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_32.w_0_beta1_pow_acc_0', 'layer_norm_38.b_0', 'linear_60.b_0', 'linear_56.b_0_fp32_master_0_moment1_0', 'layer_norm_32.b_0_moment2_0', 'linear_69.w_0_fp32_master_0_moment2_0', 'layer_norm_24.b_0@GRAD@MERGE', 'linear_72.w_0_fp32_master_0', 'linear_92.b_0_fp32_master_0', 'linear_93.w_0_fp32_master_0_moment2_0', 'linear_90.b_0_fp32_master_0_moment1_0', 'linear_72.b_0@GRAD@MERGE', 'layer_norm_48.w_0', 'layer_norm_29.b_0_beta2_pow_acc_0', 'linear_72.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_30.b_0@GRAD@MERGE', 'linear_54.b_0@GRAD@MERGE', 'layer_norm_38.w_0_moment1_0', 'linear_90.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.b_0@GRAD@MERGE', 'linear_74.b_0_fp32_master_0_moment2_0', 'linear_50.b_0_fp32_master_0_moment1_0', 'layer_norm_27.w_0_moment2_0', 'linear_76.w_0', 'linear_76.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_90.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_70.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_86.w_0@GRAD@MERGE', 'linear_78.w_0@GRAD@MERGE', 'layer_norm_43.w_0@GRAD@MERGE', 'linear_54.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_39.b_0_moment2_0', 'linear_88.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_39.w_0_moment1_0', 'linear_58.b_0@GRAD@MERGE', 'linear_61.w_0_fp32_master_0_moment2_0', 'linear_94.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_31.b_0', 'layer_norm_43.w_0_moment1_0', 'linear_49.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_59.w_0', 'linear_52.b_0_fp32_master_0_moment2_0', 'linear_94.w_0_fp32_master_0_moment2_0', 'linear_94.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_59.b_0@GRAD@MERGE', 'linear_52.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_37.w_0_moment1_0', 'layer_norm_25.w_0', 'layer_norm_36.b_0_beta1_pow_acc_0', 'layer_norm_48.w_0_beta2_pow_acc_0', 'linear_75.b_0_fp32_master_0', 'linear_65.b_0_fp32_master_0', 'linear_70.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_64.b_0_fp32_master_0_moment2_0', 'linear_86.w_0_fp32_master_0', 'layer_norm_24.w_0_beta1_pow_acc_0', 'linear_51.b_0_fp32_master_0_moment1_0', 'layer_norm_33.b_0_beta2_pow_acc_0', 'linear_85.w_0_fp32_master_0_moment1_0', 'linear_61.w_0', 'linear_66.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_48.w_0_fp32_master_0', 'layer_norm_36.b_0_beta2_pow_acc_0', 'linear_88.w_0_fp32_master_0', 'linear_72.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_52.w_0_fp32_master_0_moment2_0', 'linear_57.b_0_fp32_master_0_moment1_0', 'linear_80.b_0', 'linear_64.w_0@GRAD@MERGE', 'layer_norm_40.b_0_beta2_pow_acc_0', 'linear_83.b_0@GRAD@MERGE', 'linear_64.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_80.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_78.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_69.w_0_fp32_master_0', 'layer_norm_28.w_0_moment1_0', 'linear_86.b_0_fp32_master_0_moment1_0', 'linear_87.w_0_fp32_master_0_moment2_0', 'linear_90.w_0', 'layer_norm_46.w_0_beta1_pow_acc_0', 'learning_rate_0', 'linear_75.w_0', 'linear_51.b_0', 'linear_74.w_0_fp32_master_0_moment1_0', 'layer_norm_26.w_0_beta1_pow_acc_0', 'linear_88.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_68.b_0_fp32_master_0_moment2_0', 'layer_norm_26.b_0', 'layer_norm_38.b_0_beta2_pow_acc_0', 'linear_54.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_38.b_0_beta1_pow_acc_0', 'linear_83.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_86.b_0', 'linear_91.b_0_fp32_master_0_moment1_0', 'linear_59.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_60.b_0_fp32_master_0', 'layer_norm_45.w_0_moment1_0', 'layer_norm_46.b_0@GRAD@MERGE', 'linear_94.w_0', 'linear_86.w_0_fp32_master_0_moment2_0', 'linear_93.b_0@GRAD@MERGE', 'layer_norm_24.w_0_beta2_pow_acc_0', 'linear_89.w_0_fp32_master_0_moment2_0', 'linear_60.w_0', 'layer_norm_24.w_0@GRAD@MERGE', 'layer_norm_29.b_0_moment2_0', 'layer_norm_38.w_0_beta1_pow_acc_0', 'linear_65.b_0_fp32_master_0_moment1_0', 'layer_norm_45.w_0_beta1_pow_acc_0', 'layer_norm_48.b_0_moment2_0', 'layer_norm_37.w_0_beta2_pow_acc_0', 'layer_norm_39.b_0', 'linear_87.b_0_fp32_master_0_moment1_0', 'linear_58.w_0_fp32_master_0_moment1_0', 'linear_49.b_0_fp32_master_0', 'linear_67.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_76.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_58.b_0', 'linear_59.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_54.w_0_fp32_master_0', 'linear_63.w_0@GRAD@MERGE', 'linear_75.w_0_fp32_master_0_moment1_0', 'linear_94.w_0@GRAD@MERGE', 'layer_norm_37.b_0_beta1_pow_acc_0', 'linear_48.b_0@GRAD@MERGE', 'linear_58.w_0', 'layer_norm_45.w_0@GRAD@MERGE', 'layer_norm_35.b_0_moment1_0', 'linear_89.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_48.w_0_beta1_pow_acc_0', 'layer_norm_34.b_0_moment1_0', 'linear_50.w_0_fp32_master_0_moment2_0', 'linear_80.w_0@GRAD@MERGE', 'linear_82.b_0_fp32_master_0_moment2_0', 'linear_74.b_0_fp32_master_0', 'linear_92.b_0@GRAD@MERGE', 'linear_93.w_0@GRAD@MERGE', 'linear_51.b_0_fp32_master_0_moment2_0', 'layer_norm_25.w_0_beta1_pow_acc_0', 'layer_norm_42.b_0_beta2_pow_acc_0', 'layer_norm_33.w_0_moment2_0', 'linear_54.w_0_fp32_master_0_moment1_0', 'linear_68.b_0', 'linear_78.b_0@GRAD@MERGE', 'linear_49.b_0_fp32_master_0_moment1_0', 'linear_81.w_0_fp32_master_0_moment1_0', 'linear_89.w_0@GRAD@MERGE', 'linear_68.b_0@GRAD@MERGE', 'layer_norm_43.w_0_moment2_0', 'layer_norm_48.b_0_beta2_pow_acc_0', 'linear_75.b_0@GRAD@MERGE', 'linear_48.b_0_fp32_master_0_moment1_0', 'linear_50.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_41.b_0_moment1_0', 'linear_71.b_0@GRAD@MERGE', 'linear_71.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_88.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_51.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_90.w_0@GRAD@MERGE', 'linear_91.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_70.w_0', 'layer_norm_34.w_0', 'layer_norm_48.b_0@GRAD@MERGE', 'linear_78.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_73.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_65.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_56.w_0', 'layer_norm_45.b_0', 'layer_norm_27.w_0', 'linear_84.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_93.w_0_fp32_master_0', 'layer_norm_38.b_0_moment1_0', 'layer_norm_33.w_0', 'linear_65.w_0@GRAD@MERGE', 'linear_91.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_67.b_0_fp32_master_0', 'linear_95.b_0@GRAD@MERGE', 'layer_norm_42.b_0_moment2_0', 'layer_norm_42.b_0@GRAD@MERGE', 'layer_norm_25.w_0_moment2_0', 'linear_79.w_0', 'linear_86.w_0', 'linear_85.b_0@GRAD@MERGE', 'linear_80.w_0_fp32_master_0', 'layer_norm_45.w_0', 'layer_norm_27.b_0_moment2_0', 'layer_norm_44.w_0_beta1_pow_acc_0', 'layer_norm_43.w_0_beta2_pow_acc_0', 'linear_68.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_75.b_0', 'layer_norm_44.b_0_moment2_0', 'linear_64.b_0', 'linear_64.b_0_fp32_master_0_moment1_0', 'linear_75.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_85.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_42.w_0_beta1_pow_acc_0', 'layer_norm_46.w_0@GRAD@MERGE', 'linear_70.w_0_fp32_master_0_moment2_0', 'layer_norm_37.b_0_moment1_0', 'linear_77.w_0_fp32_master_0_moment2_0', 'linear_79.w_0_fp32_master_0_moment1_0', 'linear_86.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_78.b_0_fp32_master_0_moment1_0', 'layer_norm_25.b_0_beta1_pow_acc_0', 'linear_54.b_0_fp32_master_0_moment1_0', 'layer_norm_28.b_0', 'layer_norm_37.b_0@GRAD@MERGE', 'linear_66.w_0', 'linear_80.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_90.b_0', 'layer_norm_30.w_0_beta2_pow_acc_0', 'layer_norm_37.b_0', 'layer_norm_41.w_0_moment1_0', 'linear_56.b_0_fp32_master_0', 'layer_norm_32.b_0_moment1_0', 'linear_91.w_0_fp32_master_0_moment2_0', 'linear_92.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_34.w_0_moment1_0', 'linear_72.b_0_fp32_master_0_moment1_0', 'linear_58.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_63.w_0_fp32_master_0', 'linear_57.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_30.b_0_moment2_0', 'linear_55.w_0@GRAD@MERGE', 'linear_67.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_66.w_0_fp32_master_0', 'linear_54.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_76.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_80.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_30.w_0_moment1_0', 'linear_85.w_0', 'linear_61.w_0@GRAD@MERGE', 'linear_57.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.w_0_fp32_master_0_moment2_0', 'linear_87.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_46.w_0', 'layer_norm_25.w_0@GRAD@MERGE', 'linear_67.b_0_fp32_master_0_moment1_0', 'linear_79.w_0_fp32_master_0', 'linear_94.b_0@GRAD@MERGE', 'linear_61.b_0_fp32_master_0_moment2_0', 'linear_80.b_0_fp32_master_0_moment2_0', 'linear_71.b_0', 'layer_norm_39.b_0_moment1_0', 'linear_82.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.w_0_fp32_master_0', 'linear_64.w_0_fp32_master_0_moment2_0', 'linear_73.b_0_fp32_master_0_moment1_0', 'layer_norm_41.b_0_beta2_pow_acc_0', 'linear_50.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_80.b_0_fp32_master_0_moment1_0', 'linear_92.b_0_fp32_master_0_moment2_0', 'linear_51.w_0_fp32_master_0_moment1_0', 'layer_norm_34.b_0', 'linear_83.b_0_fp32_master_0_moment1_0', 'linear_89.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_83.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_81.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_27.w_0_beta1_pow_acc_0', 'layer_norm_47.b_0_beta1_pow_acc_0', 'linear_57.w_0@GRAD@MERGE', 'linear_77.w_0_fp32_master_0', 'layer_norm_29.w_0_moment1_0', 'layer_norm_30.w_0@GRAD@MERGE', 'layer_norm_39.w_0', 'linear_75.w_0_fp32_master_0_moment2_0', 'linear_53.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_63.b_0_fp32_master_0_moment1_0', 'layer_norm_37.b_0_beta2_pow_acc_0', 'layer_norm_40.b_0_moment2_0', 'linear_52.b_0', 'linear_65.w_0', 'linear_74.w_0', 'layer_norm_37.w_0_moment2_0', 'linear_66.b_0', 'linear_86.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_61.b_0_fp32_master_0_moment1_0', 'linear_60.b_0_fp32_master_0_moment1_0', 'linear_70.w_0_fp32_master_0', 'linear_68.w_0', 'linear_71.b_0_fp32_master_0_moment1_0', 'layer_norm_43.w_0', 'linear_62.w_0@GRAD@MERGE', 'layer_norm_43.b_0@GRAD@MERGE', 'linear_63.b_0_fp32_master_0_moment2_0', 'linear_78.b_0_fp32_master_0_moment2_0', 'layer_norm_34.w_0_moment2_0', 'layer_norm_30.b_0_moment1_0', 'linear_81.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_35.b_0_beta2_pow_acc_0', 'linear_72.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_31.b_0@GRAD@MERGE', 'layer_norm_35.w_0_beta1_pow_acc_0', 'linear_85.b_0_fp32_master_0', 'layer_norm_47.w_0', 'linear_86.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_83.b_0_fp32_master_0', 'linear_93.b_0_fp32_master_0_moment1_0', 'linear_54.w_0@GRAD@MERGE', 'linear_57.w_0_fp32_master_0_moment1_0', 'linear_85.b_0_fp32_master_0_moment1_0', 'layer_norm_24.w_0_moment1_0', 'layer_norm_35.w_0_beta2_pow_acc_0', 'linear_53.b_0@GRAD@MERGE', 'linear_57.b_0_fp32_master_0', 'linear_88.b_0', 'linear_71.w_0_fp32_master_0_moment2_0', 'linear_72.b_0', 'layer_norm_41.w_0_moment2_0', 'linear_56.b_0@GRAD@MERGE', 'layer_norm_38.w_0_moment2_0', 'linear_95.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_35.w_0_moment1_0', 'layer_norm_29.b_0_moment1_0', 'linear_66.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_44.b_0_beta2_pow_acc_0', 'layer_norm_24.b_0_moment2_0', 'layer_norm_31.w_0_moment1_0', 'linear_74.w_0_fp32_master_0_moment2_0', 'layer_norm_26.b_0_moment2_0', 'linear_83.w_0_fp32_master_0_moment1_0', 'layer_norm_35.w_0_moment2_0', 'linear_71.b_0_fp32_master_0_moment2_0', 'layer_norm_45.b_0_beta2_pow_acc_0', 'linear_93.w_0_fp32_master_0_moment1_0', 'linear_89.w_0', 'layer_norm_48.w_0_moment2_0', 'layer_norm_39.w_0_moment2_0', 'linear_81.b_0_fp32_master_0_moment2_0', 'linear_90.b_0@GRAD@MERGE', 'layer_norm_42.w_0@GRAD@MERGE', 'linear_89.b_0_fp32_master_0_moment1_0', 'linear_91.b_0_fp32_master_0', 'layer_norm_25.b_0', 'linear_60.b_0_fp32_master_0_moment2_0', 'linear_55.b_0_fp32_master_0_moment1_0', 'linear_48.b_0_fp32_master_0', 'layer_norm_26.b_0@GRAD@MERGE', 'linear_51.w_0_fp32_master_0', 'layer_norm_43.b_0_moment2_0', 'linear_70.w_0@GRAD@MERGE', 'linear_50.b_0_fp32_master_0', 'layer_norm_38.b_0@GRAD@MERGE', 'layer_norm_27.w_0_moment1_0', 'linear_68.w_0_fp32_master_0_moment2_0', 'linear_81.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.w_0', 'linear_75.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_66.w_0@GRAD@MERGE', 'layer_norm_34.b_0@GRAD@MERGE', 'layer_norm_27.w_0_beta2_pow_acc_0', 'linear_81.b_0_fp32_master_0_moment1_0', 'linear_95.b_0', 'linear_66.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_92.w_0@GRAD@MERGE', 'linear_77.b_0_fp32_master_0', 'linear_87.b_0', 'linear_67.b_0', 'layer_norm_42.w_0_moment2_0', 'linear_89.b_0@GRAD@MERGE', 'linear_55.b_0_fp32_master_0', 'linear_48.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_52.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_34.b_0_beta2_pow_acc_0', 'layer_norm_40.b_0_moment1_0', 'layer_norm_25.b_0_moment1_0', 'layer_norm_43.b_0_beta1_pow_acc_0', 'linear_54.b_0_fp32_master_0_moment2_0', 'layer_norm_46.w_0_beta2_pow_acc_0', 'layer_norm_47.b_0_moment1_0', 'linear_86.b_0_fp32_master_0', 'layer_norm_26.w_0_moment1_0', 'linear_62.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_77.b_0', 'linear_85.w_0_fp32_master_0_moment2_0', 'linear_78.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_36.w_0@GRAD@MERGE', 'linear_52.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_28.b_0_beta2_pow_acc_0', 'linear_77.w_0', 'linear_84.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_56.b_0_fp32_master_0_moment2_0', 'layer_norm_37.w_0@GRAD@MERGE', 'linear_87.w_0_fp32_master_0', 'linear_82.w_0_fp32_master_0_moment1_0', 'linear_77.w_0_fp32_master_0_moment1_0', 'linear_79.w_0_fp32_master_0_moment2_0', 'layer_norm_33.b_0', 'linear_62.b_0', 'layer_norm_35.b_0_beta1_pow_acc_0', 'layer_norm_41.w_0', 'linear_94.b_0_fp32_master_0', 'layer_norm_39.w_0@GRAD@MERGE', 'layer_norm_45.b_0@GRAD@MERGE', 'linear_81.w_0', 'linear_55.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_47.b_0', 'layer_norm_44.w_0_moment1_0', 'layer_norm_25.b_0_moment2_0', 'linear_74.w_0@GRAD@MERGE', 'linear_68.w_0@GRAD@MERGE', 'layer_norm_28.b_0_beta1_pow_acc_0', 'linear_80.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_34.w_0@GRAD@MERGE', 'layer_norm_47.b_0_moment2_0', 'linear_85.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_48.w_0@GRAD@MERGE', 'layer_norm_30.b_0', 'linear_87.w_0', 'linear_95.w_0_fp32_master_0_moment2_0', 'layer_norm_31.w_0', 'linear_83.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_80.w_0_fp32_master_0_moment2_0', 'linear_87.b_0_fp32_master_0_moment2_0', 'linear_84.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_86.b_0_fp32_master_0_moment2_0', 'layer_norm_29.w_0', 'linear_85.w_0_fp32_master_0', 'linear_63.b_0', 'layer_norm_39.b_0_beta2_pow_acc_0', 'layer_norm_39.b_0@GRAD@MERGE', 'linear_82.b_0@GRAD@MERGE', 'layer_norm_47.w_0_moment1_0', 'layer_norm_31.w_0@GRAD@MERGE', 'layer_norm_25.b_0@GRAD@MERGE', 'layer_norm_47.w_0@GRAD@MERGE', 'linear_84.w_0_fp32_master_0_moment1_0', 'linear_85.w_0@GRAD@MERGE', 'linear_49.w_0_fp32_master_0_moment1_0', 'linear_66.w_0_fp32_master_0_moment2_0', 'layer_norm_37.w_0', 'linear_62.w_0_fp32_master_0_moment1_0', 'linear_63.w_0_fp32_master_0_moment1_0', 'linear_56.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_81.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_41.w_0@GRAD@MERGE', 'linear_58.w_0_fp32_master_0_moment2_0', 'layer_norm_25.b_0_beta2_pow_acc_0', 'layer_norm_32.b_0_beta2_pow_acc_0', 'linear_74.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_82.b_0', 'layer_norm_36.b_0', 'linear_92.w_0', 'linear_94.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_71.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_62.b_0@GRAD@MERGE', 'linear_60.w_0_fp32_master_0_moment2_0', 'linear_75.b_0_fp32_master_0_moment1_0', 'linear_53.b_0_fp32_master_0_moment1_0', 'layer_norm_32.b_0_beta1_pow_acc_0', 'linear_58.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_28.w_0_beta2_pow_acc_0', 'layer_norm_34.w_0_beta1_pow_acc_0', 'linear_51.w_0', 'layer_norm_24.b_0_moment1_0', 'linear_69.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_62.w_0', 'layer_norm_40.w_0_beta1_pow_acc_0', 'layer_norm_40.w_0_moment2_0', 'layer_norm_48.b_0_beta1_pow_acc_0', 'linear_79.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_29.w_0_beta1_pow_acc_0', 'linear_79.b_0@GRAD@MERGE', 'linear_49.w_0', 'linear_87.w_0_fp32_master_0_moment1_0', 'layer_norm_48.b_0_moment1_0', 'linear_63.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_35.w_0', 'layer_norm_37.b_0_moment2_0', 'linear_63.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_79.b_0_fp32_master_0_moment1_0', 'linear_95.b_0_fp32_master_0_moment1_0', 'linear_71.w_0', 'linear_71.w_0@GRAD@MERGE', 'linear_74.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_53.b_0_fp32_master_0_moment2_0', 'linear_84.w_0@GRAD@MERGE', 'linear_86.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_29.w_0@GRAD@MERGE', 'layer_norm_31.b_0_beta1_pow_acc_0', 'linear_55.b_0@GRAD@MERGE', 'linear_93.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_63.w_0', 'layer_norm_28.w_0', 'linear_49.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_55.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_82.w_0', 'layer_norm_41.b_0', 'linear_86.b_0@GRAD@MERGE', 'linear_91.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_50.w_0_fp32_master_0', 'linear_93.b_0', 'linear_90.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_31.b_0_beta2_pow_acc_0', 'layer_norm_24.b_0_beta1_pow_acc_0', 'layer_norm_40.w_0_moment1_0', 'linear_53.w_0@GRAD@MERGE', 'layer_norm_36.w_0_moment1_0', 'linear_75.b_0_fp32_master_0_moment2_0', 'linear_61.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_76.b_0_fp32_master_0_moment2_0', 'layer_norm_28.w_0_beta1_pow_acc_0', 'linear_68.w_0_fp32_master_0', 'layer_norm_44.w_0_moment2_0', 'linear_76.w_0_fp32_master_0_moment2_0', 'linear_84.b_0@GRAD@MERGE', 'linear_76.w_0_fp32_master_0', 'linear_57.w_0_fp32_master_0', 'linear_59.w_0_fp32_master_0_moment2_0', 'linear_81.b_0', 'linear_52.b_0_fp32_master_0_moment1_0', 'linear_75.w_0_fp32_master_0', 'layer_norm_43.b_0_moment1_0', 'layer_norm_48.w_0@GRAD@MERGE', 'layer_norm_29.b_0_beta1_pow_acc_0', 'linear_58.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_93.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_46.b_0', 'layer_norm_37.w_0_beta1_pow_acc_0', 'layer_norm_42.b_0_beta1_pow_acc_0', 'layer_norm_28.w_0_moment2_0', 'linear_56.w_0_fp32_master_0', 'layer_norm_30.b_0_beta1_pow_acc_0', 'linear_59.w_0_fp32_master_0_moment1_0', 'linear_72.b_0_fp32_master_0_moment2_0', 'linear_88.b_0_fp32_master_0_moment1_0', 'linear_89.w_0_fp32_master_0_moment1_0', 'linear_74.w_0_fp32_master_0', 'linear_84.b_0_fp32_master_0_moment2_0', 'linear_68.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_89.b_0', 'linear_92.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_94.b_0_fp32_master_0_moment2_0', 'linear_54.b_0', 'linear_53.b_0_fp32_master_0', 'linear_60.w_0_fp32_master_0_moment1_0', 'linear_61.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_60.b_0_fp32_master_0_beta2_pow_acc_0', 'loss_scaling_0', 'layer_norm_31.w_0_beta1_pow_acc_0', 'linear_65.b_0@GRAD@MERGE', 'linear_50.b_0', 'linear_70.b_0', 'layer_norm_33.b_0_moment2_0', 'layer_norm_28.b_0@GRAD@MERGE', 'linear_67.w_0@GRAD@MERGE', 'linear_93.w_0', 'linear_64.w_0', 'layer_norm_31.b_0_moment1_0', 'linear_76.b_0_fp32_master_0_moment1_0', 'layer_norm_43.b_0', 'layer_norm_33.b_0_beta1_pow_acc_0', 'layer_norm_42.w_0_beta2_pow_acc_0', 'linear_48.w_0_fp32_master_0_moment1_0', 'layer_norm_35.b_0_moment2_0', 'linear_95.b_0_fp32_master_0_moment2_0', 'layer_norm_29.b_0@GRAD@MERGE', 'layer_norm_28.w_0@GRAD@MERGE', 'linear_58.b_0_fp32_master_0_moment2_0', 'linear_95.w_0', 'linear_65.w_0_fp32_master_0', 'linear_95.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_48.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_24.w_0_moment2_0', 'linear_88.w_0', 'layer_norm_42.w_0_moment1_0', 'linear_54.w_0_fp32_master_0_moment2_0', 'linear_51.b_0_fp32_master_0', 'layer_norm_30.w_0_beta1_pow_acc_0', 'linear_73.w_0_fp32_master_0_moment2_0', 'linear_83.w_0_fp32_master_0_moment2_0', 'layer_norm_39.w_0_beta2_pow_acc_0', 'layer_norm_24.w_0', 'linear_91.b_0', 'linear_87.b_0@GRAD@MERGE', 'layer_norm_46.b_0_beta1_pow_acc_0', 'linear_57.w_0', 'linear_66.w_0_fp32_master_0_moment1_0', 'linear_70.w_0_fp32_master_0_moment1_0', 'linear_82.w_0@GRAD@MERGE', 'linear_84.b_0_fp32_master_0', 'linear_60.w_0@GRAD@MERGE', 'linear_55.w_0', 'layer_norm_45.w_0_beta2_pow_acc_0', 'linear_81.w_0@GRAD@MERGE', 'linear_83.w_0_fp32_master_0', 'linear_50.w_0_fp32_master_0_moment1_0', 'linear_63.b_0@GRAD@MERGE', 'linear_91.w_0@GRAD@MERGE', 'linear_80.b_0@GRAD@MERGE', 'linear_60.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_73.b_0@GRAD@MERGE', 'layer_norm_27.b_0_beta1_pow_acc_0', 'linear_63.b_0_fp32_master_0', 'layer_norm_32.b_0', 'linear_76.w_0@GRAD@MERGE', 'linear_87.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_51.b_0@GRAD@MERGE', 'layer_norm_47.b_0_beta2_pow_acc_0', 'linear_53.w_0', 'linear_70.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_87.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_59.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_58.w_0_fp32_master_0', 'linear_59.b_0', 'linear_72.b_0_fp32_master_0', 'linear_69.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_59.b_0_fp32_master_0_moment2_0', 'linear_53.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_55.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_72.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_52.w_0', 'linear_85.b_0_fp32_master_0_moment2_0', 'linear_74.b_0@GRAD@MERGE', 'linear_92.b_0_fp32_master_0_moment1_0', 'linear_53.b_0', 'linear_59.b_0_fp32_master_0_moment1_0', 'linear_67.w_0_fp32_master_0', 'linear_82.w_0_fp32_master_0', 'linear_90.b_0_fp32_master_0', 'layer_norm_44.b_0_beta1_pow_acc_0', 'linear_78.w_0_fp32_master_0', 'linear_57.b_0@GRAD@MERGE', 'linear_58.w_0@GRAD@MERGE', 'linear_50.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_49.w_0@GRAD@MERGE', 'linear_57.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_78.b_0', 'layer_norm_36.w_0_moment2_0', 'linear_81.b_0_fp32_master_0', 'layer_norm_33.b_0@GRAD@MERGE', 'linear_49.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_69.b_0@GRAD@MERGE', 'linear_73.b_0_fp32_master_0_moment2_0', 'linear_62.w_0_fp32_master_0', 'linear_92.w_0_fp32_master_0_moment1_0', 'linear_48.w_0', 'linear_73.w_0_fp32_master_0', 'linear_79.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_41.b_0_beta1_pow_acc_0', 'linear_69.w_0_fp32_master_0_moment1_0', 'linear_73.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_69.w_0@GRAD@MERGE', 'linear_57.b_0_fp32_master_0_moment2_0', 'linear_78.w_0', 'linear_66.b_0@GRAD@MERGE', 'linear_90.w_0_fp32_master_0', 'linear_83.b_0_fp32_master_0_moment2_0', 'linear_56.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.w_0_fp32_master_0', 'linear_82.b_0_fp32_master_0_moment1_0', 'layer_norm_29.b_0', 'linear_50.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_63.w_0_fp32_master_0_moment2_0', 'linear_52.w_0_fp32_master_0_moment1_0', 'linear_91.b_0@GRAD@MERGE', 'linear_56.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_77.w_0@GRAD@MERGE', 'linear_61.w_0_fp32_master_0_moment1_0', 'linear_65.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_89.w_0_fp32_master_0', 'linear_57.b_0', 'linear_81.b_0@GRAD@MERGE', 'linear_72.w_0', 'linear_78.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_53.w_0_fp32_master_0_moment1_0', 'layer_norm_41.w_0_beta2_pow_acc_0', 'linear_65.b_0', 'linear_61.w_0_fp32_master_0', 'layer_norm_41.w_0_beta1_pow_acc_0', 'layer_norm_30.b_0_beta2_pow_acc_0', 'linear_85.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_93.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_92.w_0_fp32_master_0', 'linear_53.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_67.w_0_fp32_master_0_moment2_0', 'linear_63.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_43.b_0_beta2_pow_acc_0', 'layer_norm_47.w_0_moment2_0', 'linear_65.b_0_fp32_master_0_moment2_0', 'linear_80.w_0_fp32_master_0_moment1_0', 'linear_89.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_56.w_0_fp32_master_0_moment2_0', 'linear_65.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_88.b_0@GRAD@MERGE', 'linear_52.b_0_fp32_master_0', 'linear_49.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_69.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_76.b_0@GRAD@MERGE', 'linear_66.b_0_fp32_master_0_moment2_0', 'layer_norm_26.w_0_moment2_0', 'linear_66.b_0_fp32_master_0', 'linear_70.b_0_fp32_master_0_moment2_0', 'layer_norm_36.b_0_moment1_0', 'linear_77.b_0_fp32_master_0_moment2_0', 'layer_norm_28.b_0_moment1_0', 'linear_61.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_49.b_0@GRAD@MERGE', 'linear_85.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_53.w_0_fp32_master_0_moment2_0', 'linear_94.w_0_fp32_master_0', 'linear_62.b_0_fp32_master_0_moment1_0', 'linear_59.w_0@GRAD@MERGE', 'linear_72.w_0_fp32_master_0_moment2_0', 'linear_62.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.b_0_fp32_master_0_moment2_0', 'layer_norm_30.w_0', 'linear_61.b_0', 'linear_83.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_36.w_0', 'layer_norm_27.b_0', 'linear_74.b_0', 'layer_norm_33.w_0_beta1_pow_acc_0', 'linear_57.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_78.w_0_fp32_master_0_moment2_0', 'linear_48.w_0_fp32_master_0_moment2_0', 'linear_50.b_0@GRAD@MERGE', 'layer_norm_47.b_0@GRAD@MERGE', 'linear_58.b_0_fp32_master_0', 'linear_77.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_26.b_0_moment1_0', 'linear_73.b_0', 'linear_88.w_0@GRAD@MERGE', 'linear_71.b_0_fp32_master_0', 'layer_norm_39.w_0_beta1_pow_acc_0', 'linear_87.w_0@GRAD@MERGE', 'linear_57.w_0_fp32_master_0_moment2_0', 'layer_norm_36.w_0_beta1_pow_acc_0', 'linear_53.w_0_fp32_master_0', 'linear_49.w_0_fp32_master_0_moment2_0', 'linear_48.b_0', 'linear_65.w_0_fp32_master_0_moment1_0', 'linear_90.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_95.w_0@GRAD@MERGE', 'linear_88.b_0_fp32_master_0_moment2_0', 'linear_67.b_0_fp32_master_0_moment2_0', 'layer_norm_39.b_0_beta1_pow_acc_0', 'linear_84.b_0', 'linear_94.w_0_fp32_master_0_moment1_0', 'linear_51.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_55.w_0_fp32_master_0_moment1_0', 'linear_52.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_71.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_47.w_0_beta1_pow_acc_0', 'linear_83.b_0', 'layer_norm_38.b_0_moment2_0', 'linear_59.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_27.b_0_beta2_pow_acc_0', 'layer_norm_33.w_0_moment1_0', 'linear_60.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_26.b_0_beta1_pow_acc_0', 'linear_58.b_0_fp32_master_0_moment1_0', 'linear_76.b_0', 'linear_86.w_0_fp32_master_0_moment1_0', 'linear_50.b_0_fp32_master_0_moment2_0', 'layer_norm_43.w_0_beta1_pow_acc_0', 'layer_norm_25.w_0_beta2_pow_acc_0', 'linear_92.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_73.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_68.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_33.w_0_beta2_pow_acc_0', 'linear_89.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_46.w_0_moment1_0', 'linear_55.b_0_fp32_master_0_moment2_0', 'linear_93.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_95.b_0_fp32_master_0', 'layer_norm_35.b_0@GRAD@MERGE', 'linear_54.b_0_fp32_master_0', 'linear_55.b_0', 'linear_65.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_54.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_67.w_0', 'layer_norm_46.w_0_moment2_0', 'linear_90.w_0_fp32_master_0_moment2_0', 'layer_norm_26.w_0_beta2_pow_acc_0', 'linear_81.w_0_fp32_master_0', 'linear_94.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_78.w_0_fp32_master_0_moment1_0', 'layer_norm_30.w_0_moment2_0', 'linear_69.b_0_fp32_master_0_moment2_0', 'linear_84.w_0', 'linear_69.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_44.w_0@GRAD@MERGE', 'layer_norm_26.b_0_beta2_pow_acc_0', 'layer_norm_45.b_0_moment2_0', 'linear_87.b_0_fp32_master_0', 'layer_norm_42.w_0', 'linear_70.b_0_fp32_master_0_moment1_0', 'linear_79.w_0@GRAD@MERGE', 'linear_69.b_0', 'linear_55.w_0_fp32_master_0', 'layer_norm_45.b_0_beta1_pow_acc_0', 'layer_norm_40.w_0_beta2_pow_acc_0', 'linear_92.b_0', 'layer_norm_48.b_0', 'linear_73.w_0_fp32_master_0_moment1_0', 'layer_norm_36.b_0_moment2_0', 'linear_79.b_0_fp32_master_0', 'layer_norm_29.w_0_moment2_0', 'linear_89.b_0_fp32_master_0_moment2_0', 'linear_51.b_0_fp32_master_0_beta2_pow_acc_0', 'linear_82.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_91.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_27.b_0_moment1_0', 'layer_norm_33.w_0@GRAD@MERGE', 'linear_79.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_77.b_0@GRAD@MERGE', 'linear_53.w_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_29.w_0_beta2_pow_acc_0', 'linear_61.w_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_32.w_0_moment2_0', 'linear_76.b_0_fp32_master_0', 'linear_62.b_0_fp32_master_0_beta2_pow_acc_0', 'layer_norm_34.b_0_beta1_pow_acc_0', 'linear_62.b_0_fp32_master_0_moment2_0', 'layer_norm_31.w_0_beta2_pow_acc_0', 'layer_norm_40.b_0', 'linear_64.w_0_fp32_master_0', 'layer_norm_31.w_0_moment2_0', 'linear_92.w_0_fp32_master_0_moment2_0', 'linear_80.b_0_fp32_master_0', 'layer_norm_40.w_0', 'linear_92.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_74.b_0_fp32_master_0_beta1_pow_acc_0', 'layer_norm_44.b_0@GRAD@MERGE', 'linear_49.b_0_fp32_master_0_moment2_0', 'layer_norm_26.w_0@GRAD@MERGE', 'linear_74.b_0_fp32_master_0_moment1_0', 'linear_75.w_0@GRAD@MERGE', 'linear_68.b_0_fp32_master_0', 'linear_68.b_0_fp32_master_0_moment1_0', 'layer_norm_38.w_0@GRAD@MERGE', 'linear_48.b_0_fp32_master_0_moment2_0', 'layer_norm_25.w_0_moment1_0', 'linear_73.w_0', 'num_bad_steps_0', 'linear_51.b_0_fp32_master_0_beta1_pow_acc_0', 'linear_84.w_0_fp32_master_0_moment2_0']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 2, op_role_var = [], sub_block = block[1], with_quant_attr = False)
}
{ // block_idx:1  parent_idx:0  forward_idx:-1  backward_idx:-1

    {Out=['layer_norm_24.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_24.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_24.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_24.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_48.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_48.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_48.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_48.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_49.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_49.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_49.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_49.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_25.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_25.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_25.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_25.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_50.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_50.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_50.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_50.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_51.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_51.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_51.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_51.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_26.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_26.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_26.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_26.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_52.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_52.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_52.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_52.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_53.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_53.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_53.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_53.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_27.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_27.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_27.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_27.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_54.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_54.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_54.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_54.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_55.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_55.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_55.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_55.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_28.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_28.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_28.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_28.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_56.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_56.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_56.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_56.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_57.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_57.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_57.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_57.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_29.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_29.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_29.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_29.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_58.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_58.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_58.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_58.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_59.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_59.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_59.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_59.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_30.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_30.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_30.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_30.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_60.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_60.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_60.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_60.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_61.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_61.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_61.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_61.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_31.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_31.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_31.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_31.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_62.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_62.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_62.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_62.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_63.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_63.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_63.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_63.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_32.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_32.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_32.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_32.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_64.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_64.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_64.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_64.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_65.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_65.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_65.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_65.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_33.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_33.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_33.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_33.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_66.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_66.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_66.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_66.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_67.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_67.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_67.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_67.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_34.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_34.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_34.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_34.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_68.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_68.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_68.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_68.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_69.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_69.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_69.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_69.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_35.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_35.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_35.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_35.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_70.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_70.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_70.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_70.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_71.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_71.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_71.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_71.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_36.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_36.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_36.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_36.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_72.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_72.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_72.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_72.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_73.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_73.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_73.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_73.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_37.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_37.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_37.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_37.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_74.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_74.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_74.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_74.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_75.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_75.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_75.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_75.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_38.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_38.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_38.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_38.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_76.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_76.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_76.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_76.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_77.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_77.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_77.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_77.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_39.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_39.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_39.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_39.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_78.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_78.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_78.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_78.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_79.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_79.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_79.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_79.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_40.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_40.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_40.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_40.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_80.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_80.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_80.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_80.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_81.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_81.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_81.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_81.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_41.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_41.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_41.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_41.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_82.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_82.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_82.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_82.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_83.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_83.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_83.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_83.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_42.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_42.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_42.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_42.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_84.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_84.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_84.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_84.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_85.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_85.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_85.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_85.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_43.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_43.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_43.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_43.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_86.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_86.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_86.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_86.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_87.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_87.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_87.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_87.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_44.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_44.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_44.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_44.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_88.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_88.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_88.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_88.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_89.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_89.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_89.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_89.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_45.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_45.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_45.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_45.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_90.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_90.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_90.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_90.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_91.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_91.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_91.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_91.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_46.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_46.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_46.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_46.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_92.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_92.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_92.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_92.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_93.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_93.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_93.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_93.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_47.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_47.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_47.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_47.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_94.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_94.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_94.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_94.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_95.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_95.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_95.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_95.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_48.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_48.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_48.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_48.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_48.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_48.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_48.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_48.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_48.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_48.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_48.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_48.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_48.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_48.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_48.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_48.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_95.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_95.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_95.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_95.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_95.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_95.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_95.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_95.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_95.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_95.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_95.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_95.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_94.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_94.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_94.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_94.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_94.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_94.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_94.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_94.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_94.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_94.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_94.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_94.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_47.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_47.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_47.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_47.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_47.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_47.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_47.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_47.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_47.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_47.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_47.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_47.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_93.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_93.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_93.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_93.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_93.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_93.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_93.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_93.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_93.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_93.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_93.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_93.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_92.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_92.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_92.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_92.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_92.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_92.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_92.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_92.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_92.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_92.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_92.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_92.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_46.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_46.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_46.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_46.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_46.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_46.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_46.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_46.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_46.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_46.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_46.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_46.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_91.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_91.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_91.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_91.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_91.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_91.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_91.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_91.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_91.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_91.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_91.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_91.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_90.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_90.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_90.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_90.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_90.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_90.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_90.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_90.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_90.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_90.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_90.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_90.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_45.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_45.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_45.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_45.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_45.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_45.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_45.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_45.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_45.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_45.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_45.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_45.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_89.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_89.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_89.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_89.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_89.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_89.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_89.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_89.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_89.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_89.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_89.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_89.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_88.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_88.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_88.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_88.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_88.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_88.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_88.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_88.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_88.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_88.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_88.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_88.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_44.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_44.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_44.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_44.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_44.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_44.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_44.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_44.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_44.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_44.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_44.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_44.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_87.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_87.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_87.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_87.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_87.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_87.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_87.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_87.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_87.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_87.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_87.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_87.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_86.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_86.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_86.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_86.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_86.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_86.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_86.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_86.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_86.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_86.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_86.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_86.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_43.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_43.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_43.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_43.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_43.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_43.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_43.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_43.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_43.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_43.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_43.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_43.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_85.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_85.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_85.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_85.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_85.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_85.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_85.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_85.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_85.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_85.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_85.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_85.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_84.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_84.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_84.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_84.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_84.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_84.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_84.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_84.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_84.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_84.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_84.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_84.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_42.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_42.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_42.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_42.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_42.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_42.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_42.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_42.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_42.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_42.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_42.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_42.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_83.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_83.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_83.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_83.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_83.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_83.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_83.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_83.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_83.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_83.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_83.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_83.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_82.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_82.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_82.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_82.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_82.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_82.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_82.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_82.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_82.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_82.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_82.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_82.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_41.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_41.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_41.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_41.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_41.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_41.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_41.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_41.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_41.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_41.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_41.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_41.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_81.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_81.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_81.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_81.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_81.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_81.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_81.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_81.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_81.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_81.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_81.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_81.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_80.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_80.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_80.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_80.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_80.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_80.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_80.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_80.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_80.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_80.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_80.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_80.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_40.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_40.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_40.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_40.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_40.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_40.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_40.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_40.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_40.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_40.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_40.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_40.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_79.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_79.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_79.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_79.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_79.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_79.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_79.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_79.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_79.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_79.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_79.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_79.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_78.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_78.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_78.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_78.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_78.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_78.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_78.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_78.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_78.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_78.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_78.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_78.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_39.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_39.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_39.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_39.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_39.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_39.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_39.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_39.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_39.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_39.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_39.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_39.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_77.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_77.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_77.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_77.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_77.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_77.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_77.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_77.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_77.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_77.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_77.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_77.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_76.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_76.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_76.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_76.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_76.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_76.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_76.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_76.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_76.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_76.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_76.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_76.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_38.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_38.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_38.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_38.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_38.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_38.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_38.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_38.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_38.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_38.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_38.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_38.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_75.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_75.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_75.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_75.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_75.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_75.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_75.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_75.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_75.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_75.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_75.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_75.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_74.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_74.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_74.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_74.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_74.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_74.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_74.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_74.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_74.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_74.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_74.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_74.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_37.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_37.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_37.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_37.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_37.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_37.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_37.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_37.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_37.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_37.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_37.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_37.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_73.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_73.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_73.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_73.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_73.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_73.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_73.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_73.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_73.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_73.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_73.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_73.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_72.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_72.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_72.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_72.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_72.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_72.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_72.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_72.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_72.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_72.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_72.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_72.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_36.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_36.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_36.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_36.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_36.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_36.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_36.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_36.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_36.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_36.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_36.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_36.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_71.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_71.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_71.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_71.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_71.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_71.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_71.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_71.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_71.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_71.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_71.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_71.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_70.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_70.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_70.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_70.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_70.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_70.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_70.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_70.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_70.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_70.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_70.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_70.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_35.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_35.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_35.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_35.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_35.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_35.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_35.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_35.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_35.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_35.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_35.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_35.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_69.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_69.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_69.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_69.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_69.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_69.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_69.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_69.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_69.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_69.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_69.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_69.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_68.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_68.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_68.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_68.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_68.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_68.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_68.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_68.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_68.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_68.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_68.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_68.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_34.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_34.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_34.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_34.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_34.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_34.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_34.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_34.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_34.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_34.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_34.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_34.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_67.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_67.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_67.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_67.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_67.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_67.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_67.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_67.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_67.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_67.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_67.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_67.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_66.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_66.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_66.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_66.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_66.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_66.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_66.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_66.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_66.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_66.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_66.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_66.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_33.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_33.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_33.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_33.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_33.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_33.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_33.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_33.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_33.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_33.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_33.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_33.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_65.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_65.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_65.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_65.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_65.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_65.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_65.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_65.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_65.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_65.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_65.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_65.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_64.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_64.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_64.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_64.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_64.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_64.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_64.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_64.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_64.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_64.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_64.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_64.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_32.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_32.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_32.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_32.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_32.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_32.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_32.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_32.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_32.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_32.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_32.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_32.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_63.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_63.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_63.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_63.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_63.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_63.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_63.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_63.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_63.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_63.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_63.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_63.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_62.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_62.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_62.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_62.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_62.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_62.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_62.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_62.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_62.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_62.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_62.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_62.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_31.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_31.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_31.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_31.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_31.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_31.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_31.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_31.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_31.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_31.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_31.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_31.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_61.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_61.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_61.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_61.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_61.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_61.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_61.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_61.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_61.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_61.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_61.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_61.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_60.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_60.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_60.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_60.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_60.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_60.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_60.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_60.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_60.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_60.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_60.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_60.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_30.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_30.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_30.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_30.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_30.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_30.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_30.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_30.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_30.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_30.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_30.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_30.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_59.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_59.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_59.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_59.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_59.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_59.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_59.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_59.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_59.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_59.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_59.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_59.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_58.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_58.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_58.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_58.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_58.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_58.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_58.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_58.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_58.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_58.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_58.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_58.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_29.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_29.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_29.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_29.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_29.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_29.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_29.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_29.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_29.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_29.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_29.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_29.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_57.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_57.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_57.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_57.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_57.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_57.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_57.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_57.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_57.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_57.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_57.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_57.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_56.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_56.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_56.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_56.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_56.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_56.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_56.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_56.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_56.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_56.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_56.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_56.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_28.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_28.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_28.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_28.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_28.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_28.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_28.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_28.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_28.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_28.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_28.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_28.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_55.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_55.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_55.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_55.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_55.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_55.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_55.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_55.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_55.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_55.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_55.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_55.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_54.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_54.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_54.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_54.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_54.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_54.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_54.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_54.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_54.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_54.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_54.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_54.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_27.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_27.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_27.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_27.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_27.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_27.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_27.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_27.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_27.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_27.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_27.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_27.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_53.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_53.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_53.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_53.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_53.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_53.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_53.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_53.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_53.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_53.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_53.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_53.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_52.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_52.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_52.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_52.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_52.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_52.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_52.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_52.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_52.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_52.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_52.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_52.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_26.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_26.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_26.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_26.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_26.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_26.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_26.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_26.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_26.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_26.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_26.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_26.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_51.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_51.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_51.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_51.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_51.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_51.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_51.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_51.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_51.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_51.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_51.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_51.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_50.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_50.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_50.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_50.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_50.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_50.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_50.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_50.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_50.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_50.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_50.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_50.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_25.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_25.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_25.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_25.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_25.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_25.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_25.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_25.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_25.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_25.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_25.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_25.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_49.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_49.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_49.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_49.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_49.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_49.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_49.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_49.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_49.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_49.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_49.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_49.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_48.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_48.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_48.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_48.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_48.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_48.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_48.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_48.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_48.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_48.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_48.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_48.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_24.b_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_24.b_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_24.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_24.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_24.b_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_24.b_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_24.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['layer_norm_24.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 31, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['layer_norm_24.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_24.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['layer_norm_24.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['layer_norm_24.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp32_0'], Out=['layer_norm_24.w_0@GRAD@MERGE', 'layer_norm_24.b_0@GRAD@MERGE', 'layer_norm_25.w_0@GRAD@MERGE', 'layer_norm_25.b_0@GRAD@MERGE', 'layer_norm_26.w_0@GRAD@MERGE', 'layer_norm_26.b_0@GRAD@MERGE', 'layer_norm_27.w_0@GRAD@MERGE', 'layer_norm_27.b_0@GRAD@MERGE', 'layer_norm_28.w_0@GRAD@MERGE', 'layer_norm_28.b_0@GRAD@MERGE', 'layer_norm_29.w_0@GRAD@MERGE', 'layer_norm_29.b_0@GRAD@MERGE', 'layer_norm_30.w_0@GRAD@MERGE', 'layer_norm_30.b_0@GRAD@MERGE', 'layer_norm_31.w_0@GRAD@MERGE', 'layer_norm_31.b_0@GRAD@MERGE', 'layer_norm_32.w_0@GRAD@MERGE', 'layer_norm_32.b_0@GRAD@MERGE', 'layer_norm_33.w_0@GRAD@MERGE', 'layer_norm_33.b_0@GRAD@MERGE', 'layer_norm_34.w_0@GRAD@MERGE', 'layer_norm_34.b_0@GRAD@MERGE', 'layer_norm_35.w_0@GRAD@MERGE', 'layer_norm_35.b_0@GRAD@MERGE', 'layer_norm_36.w_0@GRAD@MERGE', 'layer_norm_36.b_0@GRAD@MERGE', 'layer_norm_37.w_0@GRAD@MERGE', 'layer_norm_37.b_0@GRAD@MERGE', 'layer_norm_38.w_0@GRAD@MERGE', 'layer_norm_38.b_0@GRAD@MERGE', 'layer_norm_39.w_0@GRAD@MERGE', 'layer_norm_39.b_0@GRAD@MERGE', 'layer_norm_40.w_0@GRAD@MERGE', 'layer_norm_40.b_0@GRAD@MERGE', 'layer_norm_41.w_0@GRAD@MERGE', 'layer_norm_41.b_0@GRAD@MERGE', 'layer_norm_42.w_0@GRAD@MERGE', 'layer_norm_42.b_0@GRAD@MERGE', 'layer_norm_43.w_0@GRAD@MERGE', 'layer_norm_43.b_0@GRAD@MERGE', 'layer_norm_44.w_0@GRAD@MERGE', 'layer_norm_44.b_0@GRAD@MERGE', 'layer_norm_45.w_0@GRAD@MERGE', 'layer_norm_45.b_0@GRAD@MERGE', 'layer_norm_46.w_0@GRAD@MERGE', 'layer_norm_46.b_0@GRAD@MERGE', 'layer_norm_47.w_0@GRAD@MERGE', 'layer_norm_47.b_0@GRAD@MERGE', 'layer_norm_48.w_0@GRAD@MERGE', 'layer_norm_48.b_0@GRAD@MERGE']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['layer_norm_24.w_0@GRAD@MERGE', 'layer_norm_24.b_0@GRAD@MERGE', 'layer_norm_25.w_0@GRAD@MERGE', 'layer_norm_25.b_0@GRAD@MERGE', 'layer_norm_26.w_0@GRAD@MERGE', 'layer_norm_26.b_0@GRAD@MERGE', 'layer_norm_27.w_0@GRAD@MERGE', 'layer_norm_27.b_0@GRAD@MERGE', 'layer_norm_28.w_0@GRAD@MERGE', 'layer_norm_28.b_0@GRAD@MERGE', 'layer_norm_29.w_0@GRAD@MERGE', 'layer_norm_29.b_0@GRAD@MERGE', 'layer_norm_30.w_0@GRAD@MERGE', 'layer_norm_30.b_0@GRAD@MERGE', 'layer_norm_31.w_0@GRAD@MERGE', 'layer_norm_31.b_0@GRAD@MERGE', 'layer_norm_32.w_0@GRAD@MERGE', 'layer_norm_32.b_0@GRAD@MERGE', 'layer_norm_33.w_0@GRAD@MERGE', 'layer_norm_33.b_0@GRAD@MERGE', 'layer_norm_34.w_0@GRAD@MERGE', 'layer_norm_34.b_0@GRAD@MERGE', 'layer_norm_35.w_0@GRAD@MERGE', 'layer_norm_35.b_0@GRAD@MERGE', 'layer_norm_36.w_0@GRAD@MERGE', 'layer_norm_36.b_0@GRAD@MERGE', 'layer_norm_37.w_0@GRAD@MERGE', 'layer_norm_37.b_0@GRAD@MERGE', 'layer_norm_38.w_0@GRAD@MERGE', 'layer_norm_38.b_0@GRAD@MERGE', 'layer_norm_39.w_0@GRAD@MERGE', 'layer_norm_39.b_0@GRAD@MERGE', 'layer_norm_40.w_0@GRAD@MERGE', 'layer_norm_40.b_0@GRAD@MERGE', 'layer_norm_41.w_0@GRAD@MERGE', 'layer_norm_41.b_0@GRAD@MERGE', 'layer_norm_42.w_0@GRAD@MERGE', 'layer_norm_42.b_0@GRAD@MERGE', 'layer_norm_43.w_0@GRAD@MERGE', 'layer_norm_43.b_0@GRAD@MERGE', 'layer_norm_44.w_0@GRAD@MERGE', 'layer_norm_44.b_0@GRAD@MERGE', 'layer_norm_45.w_0@GRAD@MERGE', 'layer_norm_45.b_0@GRAD@MERGE', 'layer_norm_46.w_0@GRAD@MERGE', 'layer_norm_46.b_0@GRAD@MERGE', 'layer_norm_47.w_0@GRAD@MERGE', 'layer_norm_47.b_0@GRAD@MERGE', 'layer_norm_48.w_0@GRAD@MERGE', 'layer_norm_48.b_0@GRAD@MERGE']}, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp32_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp32_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp32_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp32_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, ring_id = 32, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp32_0']} = cast(inputs={X=['find_infinite_scale.@fp32_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, out_dtype = 0, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['linear_48.w_0@GRAD@MERGE', 'linear_48.b_0@GRAD@MERGE', 'linear_49.w_0@GRAD@MERGE', 'linear_49.b_0@GRAD@MERGE', 'linear_50.w_0@GRAD@MERGE', 'linear_50.b_0@GRAD@MERGE', 'linear_51.w_0@GRAD@MERGE', 'linear_51.b_0@GRAD@MERGE', 'linear_52.w_0@GRAD@MERGE', 'linear_52.b_0@GRAD@MERGE', 'linear_53.w_0@GRAD@MERGE', 'linear_53.b_0@GRAD@MERGE', 'linear_54.w_0@GRAD@MERGE', 'linear_54.b_0@GRAD@MERGE', 'linear_55.w_0@GRAD@MERGE', 'linear_55.b_0@GRAD@MERGE', 'linear_56.w_0@GRAD@MERGE', 'linear_56.b_0@GRAD@MERGE', 'linear_57.w_0@GRAD@MERGE', 'linear_57.b_0@GRAD@MERGE', 'linear_58.w_0@GRAD@MERGE', 'linear_58.b_0@GRAD@MERGE', 'linear_59.w_0@GRAD@MERGE', 'linear_59.b_0@GRAD@MERGE', 'linear_60.w_0@GRAD@MERGE', 'linear_60.b_0@GRAD@MERGE', 'linear_61.w_0@GRAD@MERGE', 'linear_61.b_0@GRAD@MERGE', 'linear_62.w_0@GRAD@MERGE', 'linear_62.b_0@GRAD@MERGE', 'linear_63.w_0@GRAD@MERGE', 'linear_63.b_0@GRAD@MERGE', 'linear_64.w_0@GRAD@MERGE', 'linear_64.b_0@GRAD@MERGE', 'linear_65.w_0@GRAD@MERGE', 'linear_65.b_0@GRAD@MERGE', 'linear_66.w_0@GRAD@MERGE', 'linear_66.b_0@GRAD@MERGE', 'linear_67.w_0@GRAD@MERGE', 'linear_67.b_0@GRAD@MERGE', 'linear_68.w_0@GRAD@MERGE', 'linear_68.b_0@GRAD@MERGE', 'linear_69.w_0@GRAD@MERGE', 'linear_69.b_0@GRAD@MERGE', 'linear_70.w_0@GRAD@MERGE', 'linear_70.b_0@GRAD@MERGE', 'linear_71.w_0@GRAD@MERGE', 'linear_71.b_0@GRAD@MERGE', 'linear_72.w_0@GRAD@MERGE', 'linear_72.b_0@GRAD@MERGE', 'linear_73.w_0@GRAD@MERGE', 'linear_73.b_0@GRAD@MERGE', 'linear_74.w_0@GRAD@MERGE', 'linear_74.b_0@GRAD@MERGE', 'linear_75.w_0@GRAD@MERGE', 'linear_75.b_0@GRAD@MERGE', 'linear_76.w_0@GRAD@MERGE', 'linear_76.b_0@GRAD@MERGE', 'linear_77.w_0@GRAD@MERGE', 'linear_77.b_0@GRAD@MERGE', 'linear_78.w_0@GRAD@MERGE', 'linear_78.b_0@GRAD@MERGE', 'linear_79.w_0@GRAD@MERGE', 'linear_79.b_0@GRAD@MERGE', 'linear_80.w_0@GRAD@MERGE', 'linear_80.b_0@GRAD@MERGE', 'linear_81.w_0@GRAD@MERGE', 'linear_81.b_0@GRAD@MERGE', 'linear_82.w_0@GRAD@MERGE', 'linear_82.b_0@GRAD@MERGE', 'linear_83.w_0@GRAD@MERGE', 'linear_83.b_0@GRAD@MERGE', 'linear_84.w_0@GRAD@MERGE', 'linear_84.b_0@GRAD@MERGE', 'linear_85.w_0@GRAD@MERGE', 'linear_85.b_0@GRAD@MERGE', 'linear_86.w_0@GRAD@MERGE', 'linear_86.b_0@GRAD@MERGE', 'linear_87.w_0@GRAD@MERGE', 'linear_87.b_0@GRAD@MERGE', 'linear_88.w_0@GRAD@MERGE', 'linear_88.b_0@GRAD@MERGE', 'linear_89.w_0@GRAD@MERGE', 'linear_89.b_0@GRAD@MERGE', 'linear_90.w_0@GRAD@MERGE', 'linear_90.b_0@GRAD@MERGE', 'linear_91.w_0@GRAD@MERGE', 'linear_91.b_0@GRAD@MERGE', 'linear_92.w_0@GRAD@MERGE', 'linear_92.b_0@GRAD@MERGE', 'linear_93.w_0@GRAD@MERGE', 'linear_93.b_0@GRAD@MERGE', 'linear_94.w_0@GRAD@MERGE', 'linear_94.b_0@GRAD@MERGE', 'linear_95.w_0@GRAD@MERGE', 'linear_95.b_0@GRAD@MERGE']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['linear_48.w_0@GRAD@MERGE', 'linear_48.b_0@GRAD@MERGE', 'linear_49.w_0@GRAD@MERGE', 'linear_49.b_0@GRAD@MERGE', 'linear_50.w_0@GRAD@MERGE', 'linear_50.b_0@GRAD@MERGE', 'linear_51.w_0@GRAD@MERGE', 'linear_51.b_0@GRAD@MERGE', 'linear_52.w_0@GRAD@MERGE', 'linear_52.b_0@GRAD@MERGE', 'linear_53.w_0@GRAD@MERGE', 'linear_53.b_0@GRAD@MERGE', 'linear_54.w_0@GRAD@MERGE', 'linear_54.b_0@GRAD@MERGE', 'linear_55.w_0@GRAD@MERGE', 'linear_55.b_0@GRAD@MERGE', 'linear_56.w_0@GRAD@MERGE', 'linear_56.b_0@GRAD@MERGE', 'linear_57.w_0@GRAD@MERGE', 'linear_57.b_0@GRAD@MERGE', 'linear_58.w_0@GRAD@MERGE', 'linear_58.b_0@GRAD@MERGE', 'linear_59.w_0@GRAD@MERGE', 'linear_59.b_0@GRAD@MERGE', 'linear_60.w_0@GRAD@MERGE', 'linear_60.b_0@GRAD@MERGE', 'linear_61.w_0@GRAD@MERGE', 'linear_61.b_0@GRAD@MERGE', 'linear_62.w_0@GRAD@MERGE', 'linear_62.b_0@GRAD@MERGE', 'linear_63.w_0@GRAD@MERGE', 'linear_63.b_0@GRAD@MERGE', 'linear_64.w_0@GRAD@MERGE', 'linear_64.b_0@GRAD@MERGE', 'linear_65.w_0@GRAD@MERGE', 'linear_65.b_0@GRAD@MERGE', 'linear_66.w_0@GRAD@MERGE', 'linear_66.b_0@GRAD@MERGE', 'linear_67.w_0@GRAD@MERGE', 'linear_67.b_0@GRAD@MERGE', 'linear_68.w_0@GRAD@MERGE', 'linear_68.b_0@GRAD@MERGE', 'linear_69.w_0@GRAD@MERGE', 'linear_69.b_0@GRAD@MERGE', 'linear_70.w_0@GRAD@MERGE', 'linear_70.b_0@GRAD@MERGE', 'linear_71.w_0@GRAD@MERGE', 'linear_71.b_0@GRAD@MERGE', 'linear_72.w_0@GRAD@MERGE', 'linear_72.b_0@GRAD@MERGE', 'linear_73.w_0@GRAD@MERGE', 'linear_73.b_0@GRAD@MERGE', 'linear_74.w_0@GRAD@MERGE', 'linear_74.b_0@GRAD@MERGE', 'linear_75.w_0@GRAD@MERGE', 'linear_75.b_0@GRAD@MERGE', 'linear_76.w_0@GRAD@MERGE', 'linear_76.b_0@GRAD@MERGE', 'linear_77.w_0@GRAD@MERGE', 'linear_77.b_0@GRAD@MERGE', 'linear_78.w_0@GRAD@MERGE', 'linear_78.b_0@GRAD@MERGE', 'linear_79.w_0@GRAD@MERGE', 'linear_79.b_0@GRAD@MERGE', 'linear_80.w_0@GRAD@MERGE', 'linear_80.b_0@GRAD@MERGE', 'linear_81.w_0@GRAD@MERGE', 'linear_81.b_0@GRAD@MERGE', 'linear_82.w_0@GRAD@MERGE', 'linear_82.b_0@GRAD@MERGE', 'linear_83.w_0@GRAD@MERGE', 'linear_83.b_0@GRAD@MERGE', 'linear_84.w_0@GRAD@MERGE', 'linear_84.b_0@GRAD@MERGE', 'linear_85.w_0@GRAD@MERGE', 'linear_85.b_0@GRAD@MERGE', 'linear_86.w_0@GRAD@MERGE', 'linear_86.b_0@GRAD@MERGE', 'linear_87.w_0@GRAD@MERGE', 'linear_87.b_0@GRAD@MERGE', 'linear_88.w_0@GRAD@MERGE', 'linear_88.b_0@GRAD@MERGE', 'linear_89.w_0@GRAD@MERGE', 'linear_89.b_0@GRAD@MERGE', 'linear_90.w_0@GRAD@MERGE', 'linear_90.b_0@GRAD@MERGE', 'linear_91.w_0@GRAD@MERGE', 'linear_91.b_0@GRAD@MERGE', 'linear_92.w_0@GRAD@MERGE', 'linear_92.b_0@GRAD@MERGE', 'linear_93.w_0@GRAD@MERGE', 'linear_93.b_0@GRAD@MERGE', 'linear_94.w_0@GRAD@MERGE', 'linear_94.b_0@GRAD@MERGE', 'linear_95.w_0@GRAD@MERGE', 'linear_95.b_0@GRAD@MERGE']}, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, ring_id = 32, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp32_0', 'find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['layer_norm_24.w_0@GRAD@MERGE', 'layer_norm_24.b_0@GRAD@MERGE', 'layer_norm_25.w_0@GRAD@MERGE', 'layer_norm_25.b_0@GRAD@MERGE', 'layer_norm_26.w_0@GRAD@MERGE', 'layer_norm_26.b_0@GRAD@MERGE', 'layer_norm_27.w_0@GRAD@MERGE', 'layer_norm_27.b_0@GRAD@MERGE', 'layer_norm_28.w_0@GRAD@MERGE', 'layer_norm_28.b_0@GRAD@MERGE', 'layer_norm_29.w_0@GRAD@MERGE', 'layer_norm_29.b_0@GRAD@MERGE', 'layer_norm_30.w_0@GRAD@MERGE', 'layer_norm_30.b_0@GRAD@MERGE', 'layer_norm_31.w_0@GRAD@MERGE', 'layer_norm_31.b_0@GRAD@MERGE', 'layer_norm_32.w_0@GRAD@MERGE', 'layer_norm_32.b_0@GRAD@MERGE', 'layer_norm_33.w_0@GRAD@MERGE', 'layer_norm_33.b_0@GRAD@MERGE', 'layer_norm_34.w_0@GRAD@MERGE', 'layer_norm_34.b_0@GRAD@MERGE', 'layer_norm_35.w_0@GRAD@MERGE', 'layer_norm_35.b_0@GRAD@MERGE', 'layer_norm_36.w_0@GRAD@MERGE', 'layer_norm_36.b_0@GRAD@MERGE', 'layer_norm_37.w_0@GRAD@MERGE', 'layer_norm_37.b_0@GRAD@MERGE', 'layer_norm_38.w_0@GRAD@MERGE', 'layer_norm_38.b_0@GRAD@MERGE', 'layer_norm_39.w_0@GRAD@MERGE', 'layer_norm_39.b_0@GRAD@MERGE', 'layer_norm_40.w_0@GRAD@MERGE', 'layer_norm_40.b_0@GRAD@MERGE', 'layer_norm_41.w_0@GRAD@MERGE', 'layer_norm_41.b_0@GRAD@MERGE', 'layer_norm_42.w_0@GRAD@MERGE', 'layer_norm_42.b_0@GRAD@MERGE', 'layer_norm_43.w_0@GRAD@MERGE', 'layer_norm_43.b_0@GRAD@MERGE', 'layer_norm_44.w_0@GRAD@MERGE', 'layer_norm_44.b_0@GRAD@MERGE', 'layer_norm_45.w_0@GRAD@MERGE', 'layer_norm_45.b_0@GRAD@MERGE', 'layer_norm_46.w_0@GRAD@MERGE', 'layer_norm_46.b_0@GRAD@MERGE', 'layer_norm_47.w_0@GRAD@MERGE', 'layer_norm_47.b_0@GRAD@MERGE', 'layer_norm_48.w_0@GRAD@MERGE', 'layer_norm_48.b_0@GRAD@MERGE'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['layer_norm_24.w_0@GRAD@MERGE', 'layer_norm_24.b_0@GRAD@MERGE', 'layer_norm_25.w_0@GRAD@MERGE', 'layer_norm_25.b_0@GRAD@MERGE', 'layer_norm_26.w_0@GRAD@MERGE', 'layer_norm_26.b_0@GRAD@MERGE', 'layer_norm_27.w_0@GRAD@MERGE', 'layer_norm_27.b_0@GRAD@MERGE', 'layer_norm_28.w_0@GRAD@MERGE', 'layer_norm_28.b_0@GRAD@MERGE', 'layer_norm_29.w_0@GRAD@MERGE', 'layer_norm_29.b_0@GRAD@MERGE', 'layer_norm_30.w_0@GRAD@MERGE', 'layer_norm_30.b_0@GRAD@MERGE', 'layer_norm_31.w_0@GRAD@MERGE', 'layer_norm_31.b_0@GRAD@MERGE', 'layer_norm_32.w_0@GRAD@MERGE', 'layer_norm_32.b_0@GRAD@MERGE', 'layer_norm_33.w_0@GRAD@MERGE', 'layer_norm_33.b_0@GRAD@MERGE', 'layer_norm_34.w_0@GRAD@MERGE', 'layer_norm_34.b_0@GRAD@MERGE', 'layer_norm_35.w_0@GRAD@MERGE', 'layer_norm_35.b_0@GRAD@MERGE', 'layer_norm_36.w_0@GRAD@MERGE', 'layer_norm_36.b_0@GRAD@MERGE', 'layer_norm_37.w_0@GRAD@MERGE', 'layer_norm_37.b_0@GRAD@MERGE', 'layer_norm_38.w_0@GRAD@MERGE', 'layer_norm_38.b_0@GRAD@MERGE', 'layer_norm_39.w_0@GRAD@MERGE', 'layer_norm_39.b_0@GRAD@MERGE', 'layer_norm_40.w_0@GRAD@MERGE', 'layer_norm_40.b_0@GRAD@MERGE', 'layer_norm_41.w_0@GRAD@MERGE', 'layer_norm_41.b_0@GRAD@MERGE', 'layer_norm_42.w_0@GRAD@MERGE', 'layer_norm_42.b_0@GRAD@MERGE', 'layer_norm_43.w_0@GRAD@MERGE', 'layer_norm_43.b_0@GRAD@MERGE', 'layer_norm_44.w_0@GRAD@MERGE', 'layer_norm_44.b_0@GRAD@MERGE', 'layer_norm_45.w_0@GRAD@MERGE', 'layer_norm_45.b_0@GRAD@MERGE', 'layer_norm_46.w_0@GRAD@MERGE', 'layer_norm_46.b_0@GRAD@MERGE', 'layer_norm_47.w_0@GRAD@MERGE', 'layer_norm_47.b_0@GRAD@MERGE', 'layer_norm_48.w_0@GRAD@MERGE', 'layer_norm_48.b_0@GRAD@MERGE']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, stop_update = False, with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['linear_48.w_0@GRAD@MERGE', 'linear_48.b_0@GRAD@MERGE', 'linear_49.w_0@GRAD@MERGE', 'linear_49.b_0@GRAD@MERGE', 'linear_50.w_0@GRAD@MERGE', 'linear_50.b_0@GRAD@MERGE', 'linear_51.w_0@GRAD@MERGE', 'linear_51.b_0@GRAD@MERGE', 'linear_52.w_0@GRAD@MERGE', 'linear_52.b_0@GRAD@MERGE', 'linear_53.w_0@GRAD@MERGE', 'linear_53.b_0@GRAD@MERGE', 'linear_54.w_0@GRAD@MERGE', 'linear_54.b_0@GRAD@MERGE', 'linear_55.w_0@GRAD@MERGE', 'linear_55.b_0@GRAD@MERGE', 'linear_56.w_0@GRAD@MERGE', 'linear_56.b_0@GRAD@MERGE', 'linear_57.w_0@GRAD@MERGE', 'linear_57.b_0@GRAD@MERGE', 'linear_58.w_0@GRAD@MERGE', 'linear_58.b_0@GRAD@MERGE', 'linear_59.w_0@GRAD@MERGE', 'linear_59.b_0@GRAD@MERGE', 'linear_60.w_0@GRAD@MERGE', 'linear_60.b_0@GRAD@MERGE', 'linear_61.w_0@GRAD@MERGE', 'linear_61.b_0@GRAD@MERGE', 'linear_62.w_0@GRAD@MERGE', 'linear_62.b_0@GRAD@MERGE', 'linear_63.w_0@GRAD@MERGE', 'linear_63.b_0@GRAD@MERGE', 'linear_64.w_0@GRAD@MERGE', 'linear_64.b_0@GRAD@MERGE', 'linear_65.w_0@GRAD@MERGE', 'linear_65.b_0@GRAD@MERGE', 'linear_66.w_0@GRAD@MERGE', 'linear_66.b_0@GRAD@MERGE', 'linear_67.w_0@GRAD@MERGE', 'linear_67.b_0@GRAD@MERGE', 'linear_68.w_0@GRAD@MERGE', 'linear_68.b_0@GRAD@MERGE', 'linear_69.w_0@GRAD@MERGE', 'linear_69.b_0@GRAD@MERGE', 'linear_70.w_0@GRAD@MERGE', 'linear_70.b_0@GRAD@MERGE', 'linear_71.w_0@GRAD@MERGE', 'linear_71.b_0@GRAD@MERGE', 'linear_72.w_0@GRAD@MERGE', 'linear_72.b_0@GRAD@MERGE', 'linear_73.w_0@GRAD@MERGE', 'linear_73.b_0@GRAD@MERGE', 'linear_74.w_0@GRAD@MERGE', 'linear_74.b_0@GRAD@MERGE', 'linear_75.w_0@GRAD@MERGE', 'linear_75.b_0@GRAD@MERGE', 'linear_76.w_0@GRAD@MERGE', 'linear_76.b_0@GRAD@MERGE', 'linear_77.w_0@GRAD@MERGE', 'linear_77.b_0@GRAD@MERGE', 'linear_78.w_0@GRAD@MERGE', 'linear_78.b_0@GRAD@MERGE', 'linear_79.w_0@GRAD@MERGE', 'linear_79.b_0@GRAD@MERGE', 'linear_80.w_0@GRAD@MERGE', 'linear_80.b_0@GRAD@MERGE', 'linear_81.w_0@GRAD@MERGE', 'linear_81.b_0@GRAD@MERGE', 'linear_82.w_0@GRAD@MERGE', 'linear_82.b_0@GRAD@MERGE', 'linear_83.w_0@GRAD@MERGE', 'linear_83.b_0@GRAD@MERGE', 'linear_84.w_0@GRAD@MERGE', 'linear_84.b_0@GRAD@MERGE', 'linear_85.w_0@GRAD@MERGE', 'linear_85.b_0@GRAD@MERGE', 'linear_86.w_0@GRAD@MERGE', 'linear_86.b_0@GRAD@MERGE', 'linear_87.w_0@GRAD@MERGE', 'linear_87.b_0@GRAD@MERGE', 'linear_88.w_0@GRAD@MERGE', 'linear_88.b_0@GRAD@MERGE', 'linear_89.w_0@GRAD@MERGE', 'linear_89.b_0@GRAD@MERGE', 'linear_90.w_0@GRAD@MERGE', 'linear_90.b_0@GRAD@MERGE', 'linear_91.w_0@GRAD@MERGE', 'linear_91.b_0@GRAD@MERGE', 'linear_92.w_0@GRAD@MERGE', 'linear_92.b_0@GRAD@MERGE', 'linear_93.w_0@GRAD@MERGE', 'linear_93.b_0@GRAD@MERGE', 'linear_94.w_0@GRAD@MERGE', 'linear_94.b_0@GRAD@MERGE', 'linear_95.w_0@GRAD@MERGE', 'linear_95.b_0@GRAD@MERGE'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['linear_48.w_0@GRAD@MERGE', 'linear_48.b_0@GRAD@MERGE', 'linear_49.w_0@GRAD@MERGE', 'linear_49.b_0@GRAD@MERGE', 'linear_50.w_0@GRAD@MERGE', 'linear_50.b_0@GRAD@MERGE', 'linear_51.w_0@GRAD@MERGE', 'linear_51.b_0@GRAD@MERGE', 'linear_52.w_0@GRAD@MERGE', 'linear_52.b_0@GRAD@MERGE', 'linear_53.w_0@GRAD@MERGE', 'linear_53.b_0@GRAD@MERGE', 'linear_54.w_0@GRAD@MERGE', 'linear_54.b_0@GRAD@MERGE', 'linear_55.w_0@GRAD@MERGE', 'linear_55.b_0@GRAD@MERGE', 'linear_56.w_0@GRAD@MERGE', 'linear_56.b_0@GRAD@MERGE', 'linear_57.w_0@GRAD@MERGE', 'linear_57.b_0@GRAD@MERGE', 'linear_58.w_0@GRAD@MERGE', 'linear_58.b_0@GRAD@MERGE', 'linear_59.w_0@GRAD@MERGE', 'linear_59.b_0@GRAD@MERGE', 'linear_60.w_0@GRAD@MERGE', 'linear_60.b_0@GRAD@MERGE', 'linear_61.w_0@GRAD@MERGE', 'linear_61.b_0@GRAD@MERGE', 'linear_62.w_0@GRAD@MERGE', 'linear_62.b_0@GRAD@MERGE', 'linear_63.w_0@GRAD@MERGE', 'linear_63.b_0@GRAD@MERGE', 'linear_64.w_0@GRAD@MERGE', 'linear_64.b_0@GRAD@MERGE', 'linear_65.w_0@GRAD@MERGE', 'linear_65.b_0@GRAD@MERGE', 'linear_66.w_0@GRAD@MERGE', 'linear_66.b_0@GRAD@MERGE', 'linear_67.w_0@GRAD@MERGE', 'linear_67.b_0@GRAD@MERGE', 'linear_68.w_0@GRAD@MERGE', 'linear_68.b_0@GRAD@MERGE', 'linear_69.w_0@GRAD@MERGE', 'linear_69.b_0@GRAD@MERGE', 'linear_70.w_0@GRAD@MERGE', 'linear_70.b_0@GRAD@MERGE', 'linear_71.w_0@GRAD@MERGE', 'linear_71.b_0@GRAD@MERGE', 'linear_72.w_0@GRAD@MERGE', 'linear_72.b_0@GRAD@MERGE', 'linear_73.w_0@GRAD@MERGE', 'linear_73.b_0@GRAD@MERGE', 'linear_74.w_0@GRAD@MERGE', 'linear_74.b_0@GRAD@MERGE', 'linear_75.w_0@GRAD@MERGE', 'linear_75.b_0@GRAD@MERGE', 'linear_76.w_0@GRAD@MERGE', 'linear_76.b_0@GRAD@MERGE', 'linear_77.w_0@GRAD@MERGE', 'linear_77.b_0@GRAD@MERGE', 'linear_78.w_0@GRAD@MERGE', 'linear_78.b_0@GRAD@MERGE', 'linear_79.w_0@GRAD@MERGE', 'linear_79.b_0@GRAD@MERGE', 'linear_80.w_0@GRAD@MERGE', 'linear_80.b_0@GRAD@MERGE', 'linear_81.w_0@GRAD@MERGE', 'linear_81.b_0@GRAD@MERGE', 'linear_82.w_0@GRAD@MERGE', 'linear_82.b_0@GRAD@MERGE', 'linear_83.w_0@GRAD@MERGE', 'linear_83.b_0@GRAD@MERGE', 'linear_84.w_0@GRAD@MERGE', 'linear_84.b_0@GRAD@MERGE', 'linear_85.w_0@GRAD@MERGE', 'linear_85.b_0@GRAD@MERGE', 'linear_86.w_0@GRAD@MERGE', 'linear_86.b_0@GRAD@MERGE', 'linear_87.w_0@GRAD@MERGE', 'linear_87.b_0@GRAD@MERGE', 'linear_88.w_0@GRAD@MERGE', 'linear_88.b_0@GRAD@MERGE', 'linear_89.w_0@GRAD@MERGE', 'linear_89.b_0@GRAD@MERGE', 'linear_90.w_0@GRAD@MERGE', 'linear_90.b_0@GRAD@MERGE', 'linear_91.w_0@GRAD@MERGE', 'linear_91.b_0@GRAD@MERGE', 'linear_92.w_0@GRAD@MERGE', 'linear_92.b_0@GRAD@MERGE', 'linear_93.w_0@GRAD@MERGE', 'linear_93.b_0@GRAD@MERGE', 'linear_94.w_0@GRAD@MERGE', 'linear_94.b_0@GRAD@MERGE', 'linear_95.w_0@GRAD@MERGE', 'linear_95.b_0@GRAD@MERGE']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_sum_2.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip_pass, op_role = 2, place_type = -1, shape = [], str_value = , value = 0.0, with_quant_attr = False)
    {Out=['opt_opt_sum_2.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['opt_opt_sum_2.tmp_0']}, op_device = , op_namescope = /auto_parallel/global_norm_synchronization, op_role = 2, ring_id = 0, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_opt_sum_2.tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_99']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_48.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_48.w_0@GRAD@MERGE'], Y=['opt_tmp_99']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_100']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_48.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_48.b_0@GRAD@MERGE'], Y=['opt_tmp_100']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_101']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_49.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_49.w_0@GRAD@MERGE'], Y=['opt_tmp_101']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_102']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_49.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_49.b_0@GRAD@MERGE'], Y=['opt_tmp_102']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_103']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_50.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_50.w_0@GRAD@MERGE'], Y=['opt_tmp_103']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_104']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_50.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_50.b_0@GRAD@MERGE'], Y=['opt_tmp_104']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_105']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_51.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_51.w_0@GRAD@MERGE'], Y=['opt_tmp_105']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_106']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_51.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_51.b_0@GRAD@MERGE'], Y=['opt_tmp_106']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_24.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_24.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_24.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_24.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_25.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_25.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_25.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_25.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_107']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_52.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_52.w_0@GRAD@MERGE'], Y=['opt_tmp_107']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_108']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_52.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_52.b_0@GRAD@MERGE'], Y=['opt_tmp_108']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_109']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_53.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_53.w_0@GRAD@MERGE'], Y=['opt_tmp_109']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_110']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_53.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_53.b_0@GRAD@MERGE'], Y=['opt_tmp_110']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_111']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_54.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_54.w_0@GRAD@MERGE'], Y=['opt_tmp_111']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_112']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_54.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_54.b_0@GRAD@MERGE'], Y=['opt_tmp_112']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_113']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_55.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_55.w_0@GRAD@MERGE'], Y=['opt_tmp_113']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_114']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_55.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_55.b_0@GRAD@MERGE'], Y=['opt_tmp_114']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_26.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_26.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_26.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_26.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_27.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_27.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_27.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_27.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_115']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_56.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_56.w_0@GRAD@MERGE'], Y=['opt_tmp_115']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_116']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_56.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_56.b_0@GRAD@MERGE'], Y=['opt_tmp_116']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_117']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_57.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_57.w_0@GRAD@MERGE'], Y=['opt_tmp_117']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_118']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_57.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_57.b_0@GRAD@MERGE'], Y=['opt_tmp_118']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_119']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_58.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_58.w_0@GRAD@MERGE'], Y=['opt_tmp_119']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_120']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_58.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_58.b_0@GRAD@MERGE'], Y=['opt_tmp_120']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_121']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_59.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_59.w_0@GRAD@MERGE'], Y=['opt_tmp_121']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_122']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_59.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_59.b_0@GRAD@MERGE'], Y=['opt_tmp_122']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_28.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_28.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_28.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_28.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_29.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_29.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_29.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_29.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_123']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_60.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_60.w_0@GRAD@MERGE'], Y=['opt_tmp_123']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_124']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_60.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_60.b_0@GRAD@MERGE'], Y=['opt_tmp_124']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_125']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_61.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_61.w_0@GRAD@MERGE'], Y=['opt_tmp_125']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_126']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_61.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_61.b_0@GRAD@MERGE'], Y=['opt_tmp_126']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_127']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_62.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_62.w_0@GRAD@MERGE'], Y=['opt_tmp_127']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_128']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_62.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_62.b_0@GRAD@MERGE'], Y=['opt_tmp_128']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_129']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_63.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_63.w_0@GRAD@MERGE'], Y=['opt_tmp_129']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_130']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_63.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_63.b_0@GRAD@MERGE'], Y=['opt_tmp_130']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_30.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_30.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_30.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_30.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_31.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_31.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_31.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_31.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_131']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_64.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_64.w_0@GRAD@MERGE'], Y=['opt_tmp_131']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_132']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_64.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_64.b_0@GRAD@MERGE'], Y=['opt_tmp_132']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_133']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_65.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_65.w_0@GRAD@MERGE'], Y=['opt_tmp_133']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_134']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_65.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_65.b_0@GRAD@MERGE'], Y=['opt_tmp_134']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_135']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_66.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_66.w_0@GRAD@MERGE'], Y=['opt_tmp_135']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_136']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_66.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_66.b_0@GRAD@MERGE'], Y=['opt_tmp_136']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_137']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_67.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_67.w_0@GRAD@MERGE'], Y=['opt_tmp_137']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_138']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_67.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_67.b_0@GRAD@MERGE'], Y=['opt_tmp_138']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_32.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_32.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_32.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_32.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_33.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_33.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_33.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_33.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_139']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_68.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_68.w_0@GRAD@MERGE'], Y=['opt_tmp_139']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_140']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_68.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_68.b_0@GRAD@MERGE'], Y=['opt_tmp_140']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_141']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_69.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_69.w_0@GRAD@MERGE'], Y=['opt_tmp_141']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_142']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_69.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_69.b_0@GRAD@MERGE'], Y=['opt_tmp_142']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_143']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_70.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_70.w_0@GRAD@MERGE'], Y=['opt_tmp_143']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_144']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_70.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_70.b_0@GRAD@MERGE'], Y=['opt_tmp_144']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_145']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_71.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_71.w_0@GRAD@MERGE'], Y=['opt_tmp_145']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_146']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_71.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_71.b_0@GRAD@MERGE'], Y=['opt_tmp_146']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_34.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_34.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_34.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_34.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_35.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_35.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_35.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_35.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_147']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_72.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_72.w_0@GRAD@MERGE'], Y=['opt_tmp_147']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_148']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_72.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_72.b_0@GRAD@MERGE'], Y=['opt_tmp_148']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_149']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_73.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_73.w_0@GRAD@MERGE'], Y=['opt_tmp_149']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_150']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_73.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_73.b_0@GRAD@MERGE'], Y=['opt_tmp_150']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_151']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_74.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_74.w_0@GRAD@MERGE'], Y=['opt_tmp_151']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_152']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_74.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_74.b_0@GRAD@MERGE'], Y=['opt_tmp_152']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_153']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_75.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_75.w_0@GRAD@MERGE'], Y=['opt_tmp_153']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_154']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_75.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_75.b_0@GRAD@MERGE'], Y=['opt_tmp_154']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_36.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_36.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_36.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_36.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_37.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_37.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_37.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_37.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_155']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_76.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_76.w_0@GRAD@MERGE'], Y=['opt_tmp_155']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_156']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_76.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_76.b_0@GRAD@MERGE'], Y=['opt_tmp_156']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_157']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_77.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_77.w_0@GRAD@MERGE'], Y=['opt_tmp_157']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_158']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_77.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_77.b_0@GRAD@MERGE'], Y=['opt_tmp_158']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_159']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_78.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_78.w_0@GRAD@MERGE'], Y=['opt_tmp_159']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_160']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_78.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_78.b_0@GRAD@MERGE'], Y=['opt_tmp_160']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_161']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_79.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_79.w_0@GRAD@MERGE'], Y=['opt_tmp_161']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_162']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_79.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_79.b_0@GRAD@MERGE'], Y=['opt_tmp_162']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_38.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_38.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_38.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_38.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_39.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_39.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_39.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_39.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_163']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_80.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_80.w_0@GRAD@MERGE'], Y=['opt_tmp_163']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_164']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_80.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_80.b_0@GRAD@MERGE'], Y=['opt_tmp_164']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_165']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_81.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_81.w_0@GRAD@MERGE'], Y=['opt_tmp_165']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_166']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_81.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_81.b_0@GRAD@MERGE'], Y=['opt_tmp_166']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_167']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_82.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_82.w_0@GRAD@MERGE'], Y=['opt_tmp_167']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_168']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_82.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_82.b_0@GRAD@MERGE'], Y=['opt_tmp_168']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_169']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_83.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_83.w_0@GRAD@MERGE'], Y=['opt_tmp_169']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_170']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_83.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_83.b_0@GRAD@MERGE'], Y=['opt_tmp_170']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_40.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_40.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_40.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_40.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_41.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_41.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_41.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_41.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_171']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_84.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_84.w_0@GRAD@MERGE'], Y=['opt_tmp_171']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_172']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_84.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_84.b_0@GRAD@MERGE'], Y=['opt_tmp_172']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_173']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_85.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_85.w_0@GRAD@MERGE'], Y=['opt_tmp_173']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_174']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_85.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_85.b_0@GRAD@MERGE'], Y=['opt_tmp_174']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_175']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_86.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_86.w_0@GRAD@MERGE'], Y=['opt_tmp_175']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_176']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_86.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_86.b_0@GRAD@MERGE'], Y=['opt_tmp_176']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_177']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_87.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_87.w_0@GRAD@MERGE'], Y=['opt_tmp_177']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_178']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_87.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_87.b_0@GRAD@MERGE'], Y=['opt_tmp_178']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_42.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_42.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_42.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_42.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_43.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_43.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_43.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_43.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_179']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_88.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_88.w_0@GRAD@MERGE'], Y=['opt_tmp_179']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_180']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_88.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_88.b_0@GRAD@MERGE'], Y=['opt_tmp_180']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_181']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_89.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_89.w_0@GRAD@MERGE'], Y=['opt_tmp_181']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_182']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_89.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_89.b_0@GRAD@MERGE'], Y=['opt_tmp_182']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_183']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_90.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_90.w_0@GRAD@MERGE'], Y=['opt_tmp_183']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_184']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_90.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_90.b_0@GRAD@MERGE'], Y=['opt_tmp_184']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_185']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_91.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_91.w_0@GRAD@MERGE'], Y=['opt_tmp_185']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_186']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_91.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_91.b_0@GRAD@MERGE'], Y=['opt_tmp_186']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_44.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_44.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_44.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_44.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_45.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_45.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_45.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_45.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_187']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_92.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_92.w_0@GRAD@MERGE'], Y=['opt_tmp_187']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_188']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_92.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_92.b_0@GRAD@MERGE'], Y=['opt_tmp_188']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_189']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_93.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_93.w_0@GRAD@MERGE'], Y=['opt_tmp_189']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_190']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_93.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_93.b_0@GRAD@MERGE'], Y=['opt_tmp_190']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_191']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_94.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_94.w_0@GRAD@MERGE'], Y=['opt_tmp_191']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_192']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_94.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_94.b_0@GRAD@MERGE'], Y=['opt_tmp_192']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_193']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_95.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_95.w_0@GRAD@MERGE'], Y=['opt_tmp_193']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_194']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_95.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_95.b_0@GRAD@MERGE'], Y=['opt_tmp_194']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_46.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_46.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_46.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_46.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_47.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_47.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_47.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_47.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_48.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_48.w_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['layer_norm_48.b_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['layer_norm_48.b_0@GRAD@MERGE'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Beta1PowOut=['linear_48.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_48.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_48.w_0_fp32_master_0'], Moment1Out=['linear_48.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_48.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_48.w_0']} = adamw(inputs={Beta1Pow=['linear_48.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_48.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_48.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_48.w_0_fp32_master_0'], Moment1=['linear_48.w_0_fp32_master_0_moment1_0'], Moment2=['linear_48.w_0_fp32_master_0_moment2_0'], Param=['linear_48.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_146/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_48.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_48.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_48.b_0_fp32_master_0'], Moment1Out=['linear_48.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_48.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_48.b_0']} = adamw(inputs={Beta1Pow=['linear_48.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_48.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_48.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_48.b_0_fp32_master_0'], Moment1=['linear_48.b_0_fp32_master_0_moment1_0'], Moment2=['linear_48.b_0_fp32_master_0_moment2_0'], Param=['linear_48.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_147/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_49.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_49.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_49.w_0_fp32_master_0'], Moment1Out=['linear_49.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_49.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_49.w_0']} = adamw(inputs={Beta1Pow=['linear_49.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_49.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_49.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_49.w_0_fp32_master_0'], Moment1=['linear_49.w_0_fp32_master_0_moment1_0'], Moment2=['linear_49.w_0_fp32_master_0_moment2_0'], Param=['linear_49.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_148/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_49.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_49.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_49.b_0_fp32_master_0'], Moment1Out=['linear_49.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_49.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_49.b_0']} = adamw(inputs={Beta1Pow=['linear_49.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_49.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_49.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_49.b_0_fp32_master_0'], Moment1=['linear_49.b_0_fp32_master_0_moment1_0'], Moment2=['linear_49.b_0_fp32_master_0_moment2_0'], Param=['linear_49.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_149/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_50.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_50.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_50.w_0_fp32_master_0'], Moment1Out=['linear_50.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_50.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_50.w_0']} = adamw(inputs={Beta1Pow=['linear_50.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_50.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_50.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_50.w_0_fp32_master_0'], Moment1=['linear_50.w_0_fp32_master_0_moment1_0'], Moment2=['linear_50.w_0_fp32_master_0_moment2_0'], Param=['linear_50.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_150/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_50.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_50.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_50.b_0_fp32_master_0'], Moment1Out=['linear_50.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_50.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_50.b_0']} = adamw(inputs={Beta1Pow=['linear_50.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_50.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_50.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_50.b_0_fp32_master_0'], Moment1=['linear_50.b_0_fp32_master_0_moment1_0'], Moment2=['linear_50.b_0_fp32_master_0_moment2_0'], Param=['linear_50.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_151/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_51.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_51.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_51.w_0_fp32_master_0'], Moment1Out=['linear_51.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_51.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_51.w_0']} = adamw(inputs={Beta1Pow=['linear_51.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_51.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_51.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_51.w_0_fp32_master_0'], Moment1=['linear_51.w_0_fp32_master_0_moment1_0'], Moment2=['linear_51.w_0_fp32_master_0_moment2_0'], Param=['linear_51.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_152/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_51.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_51.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_51.b_0_fp32_master_0'], Moment1Out=['linear_51.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_51.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_51.b_0']} = adamw(inputs={Beta1Pow=['linear_51.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_51.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_51.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_51.b_0_fp32_master_0'], Moment1=['linear_51.b_0_fp32_master_0_moment1_0'], Moment2=['linear_51.b_0_fp32_master_0_moment2_0'], Param=['linear_51.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_153/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_24.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_24.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_24.w_0_moment1_0'], Moment2Out=['layer_norm_24.w_0_moment2_0'], ParamOut=['layer_norm_24.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_24.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_24.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_24.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_24.w_0_moment1_0'], Moment2=['layer_norm_24.w_0_moment2_0'], Param=['layer_norm_24.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_154/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_24.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_24.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_24.b_0_moment1_0'], Moment2Out=['layer_norm_24.b_0_moment2_0'], ParamOut=['layer_norm_24.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_24.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_24.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_24.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_24.b_0_moment1_0'], Moment2=['layer_norm_24.b_0_moment2_0'], Param=['layer_norm_24.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_155/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_25.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_25.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_25.w_0_moment1_0'], Moment2Out=['layer_norm_25.w_0_moment2_0'], ParamOut=['layer_norm_25.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_25.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_25.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_25.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_25.w_0_moment1_0'], Moment2=['layer_norm_25.w_0_moment2_0'], Param=['layer_norm_25.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_156/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_25.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_25.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_25.b_0_moment1_0'], Moment2Out=['layer_norm_25.b_0_moment2_0'], ParamOut=['layer_norm_25.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_25.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_25.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_25.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_25.b_0_moment1_0'], Moment2=['layer_norm_25.b_0_moment2_0'], Param=['layer_norm_25.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_157/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_52.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_52.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_52.w_0_fp32_master_0'], Moment1Out=['linear_52.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_52.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_52.w_0']} = adamw(inputs={Beta1Pow=['linear_52.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_52.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_52.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_52.w_0_fp32_master_0'], Moment1=['linear_52.w_0_fp32_master_0_moment1_0'], Moment2=['linear_52.w_0_fp32_master_0_moment2_0'], Param=['linear_52.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_158/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_52.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_52.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_52.b_0_fp32_master_0'], Moment1Out=['linear_52.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_52.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_52.b_0']} = adamw(inputs={Beta1Pow=['linear_52.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_52.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_52.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_52.b_0_fp32_master_0'], Moment1=['linear_52.b_0_fp32_master_0_moment1_0'], Moment2=['linear_52.b_0_fp32_master_0_moment2_0'], Param=['linear_52.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_159/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_53.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_53.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_53.w_0_fp32_master_0'], Moment1Out=['linear_53.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_53.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_53.w_0']} = adamw(inputs={Beta1Pow=['linear_53.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_53.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_53.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_53.w_0_fp32_master_0'], Moment1=['linear_53.w_0_fp32_master_0_moment1_0'], Moment2=['linear_53.w_0_fp32_master_0_moment2_0'], Param=['linear_53.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_160/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_53.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_53.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_53.b_0_fp32_master_0'], Moment1Out=['linear_53.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_53.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_53.b_0']} = adamw(inputs={Beta1Pow=['linear_53.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_53.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_53.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_53.b_0_fp32_master_0'], Moment1=['linear_53.b_0_fp32_master_0_moment1_0'], Moment2=['linear_53.b_0_fp32_master_0_moment2_0'], Param=['linear_53.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_161/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_54.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_54.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_54.w_0_fp32_master_0'], Moment1Out=['linear_54.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_54.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_54.w_0']} = adamw(inputs={Beta1Pow=['linear_54.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_54.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_54.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_54.w_0_fp32_master_0'], Moment1=['linear_54.w_0_fp32_master_0_moment1_0'], Moment2=['linear_54.w_0_fp32_master_0_moment2_0'], Param=['linear_54.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_162/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_54.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_54.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_54.b_0_fp32_master_0'], Moment1Out=['linear_54.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_54.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_54.b_0']} = adamw(inputs={Beta1Pow=['linear_54.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_54.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_54.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_54.b_0_fp32_master_0'], Moment1=['linear_54.b_0_fp32_master_0_moment1_0'], Moment2=['linear_54.b_0_fp32_master_0_moment2_0'], Param=['linear_54.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_163/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_55.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_55.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_55.w_0_fp32_master_0'], Moment1Out=['linear_55.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_55.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_55.w_0']} = adamw(inputs={Beta1Pow=['linear_55.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_55.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_55.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_55.w_0_fp32_master_0'], Moment1=['linear_55.w_0_fp32_master_0_moment1_0'], Moment2=['linear_55.w_0_fp32_master_0_moment2_0'], Param=['linear_55.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_164/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_55.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_55.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_55.b_0_fp32_master_0'], Moment1Out=['linear_55.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_55.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_55.b_0']} = adamw(inputs={Beta1Pow=['linear_55.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_55.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_55.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_55.b_0_fp32_master_0'], Moment1=['linear_55.b_0_fp32_master_0_moment1_0'], Moment2=['linear_55.b_0_fp32_master_0_moment2_0'], Param=['linear_55.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_165/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_26.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_26.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_26.w_0_moment1_0'], Moment2Out=['layer_norm_26.w_0_moment2_0'], ParamOut=['layer_norm_26.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_26.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_26.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_26.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_26.w_0_moment1_0'], Moment2=['layer_norm_26.w_0_moment2_0'], Param=['layer_norm_26.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_166/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_26.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_26.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_26.b_0_moment1_0'], Moment2Out=['layer_norm_26.b_0_moment2_0'], ParamOut=['layer_norm_26.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_26.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_26.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_26.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_26.b_0_moment1_0'], Moment2=['layer_norm_26.b_0_moment2_0'], Param=['layer_norm_26.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_167/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_27.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_27.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_27.w_0_moment1_0'], Moment2Out=['layer_norm_27.w_0_moment2_0'], ParamOut=['layer_norm_27.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_27.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_27.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_27.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_27.w_0_moment1_0'], Moment2=['layer_norm_27.w_0_moment2_0'], Param=['layer_norm_27.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_168/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_27.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_27.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_27.b_0_moment1_0'], Moment2Out=['layer_norm_27.b_0_moment2_0'], ParamOut=['layer_norm_27.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_27.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_27.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_27.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_27.b_0_moment1_0'], Moment2=['layer_norm_27.b_0_moment2_0'], Param=['layer_norm_27.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_169/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_56.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_56.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_56.w_0_fp32_master_0'], Moment1Out=['linear_56.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_56.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_56.w_0']} = adamw(inputs={Beta1Pow=['linear_56.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_56.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_56.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_56.w_0_fp32_master_0'], Moment1=['linear_56.w_0_fp32_master_0_moment1_0'], Moment2=['linear_56.w_0_fp32_master_0_moment2_0'], Param=['linear_56.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_170/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_56.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_56.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_56.b_0_fp32_master_0'], Moment1Out=['linear_56.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_56.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_56.b_0']} = adamw(inputs={Beta1Pow=['linear_56.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_56.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_56.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_56.b_0_fp32_master_0'], Moment1=['linear_56.b_0_fp32_master_0_moment1_0'], Moment2=['linear_56.b_0_fp32_master_0_moment2_0'], Param=['linear_56.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_171/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_57.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_57.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_57.w_0_fp32_master_0'], Moment1Out=['linear_57.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_57.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_57.w_0']} = adamw(inputs={Beta1Pow=['linear_57.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_57.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_57.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_57.w_0_fp32_master_0'], Moment1=['linear_57.w_0_fp32_master_0_moment1_0'], Moment2=['linear_57.w_0_fp32_master_0_moment2_0'], Param=['linear_57.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_172/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_57.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_57.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_57.b_0_fp32_master_0'], Moment1Out=['linear_57.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_57.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_57.b_0']} = adamw(inputs={Beta1Pow=['linear_57.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_57.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_57.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_57.b_0_fp32_master_0'], Moment1=['linear_57.b_0_fp32_master_0_moment1_0'], Moment2=['linear_57.b_0_fp32_master_0_moment2_0'], Param=['linear_57.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_173/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_58.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_58.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_58.w_0_fp32_master_0'], Moment1Out=['linear_58.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_58.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_58.w_0']} = adamw(inputs={Beta1Pow=['linear_58.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_58.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_58.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_58.w_0_fp32_master_0'], Moment1=['linear_58.w_0_fp32_master_0_moment1_0'], Moment2=['linear_58.w_0_fp32_master_0_moment2_0'], Param=['linear_58.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_174/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_58.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_58.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_58.b_0_fp32_master_0'], Moment1Out=['linear_58.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_58.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_58.b_0']} = adamw(inputs={Beta1Pow=['linear_58.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_58.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_58.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_58.b_0_fp32_master_0'], Moment1=['linear_58.b_0_fp32_master_0_moment1_0'], Moment2=['linear_58.b_0_fp32_master_0_moment2_0'], Param=['linear_58.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_175/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_59.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_59.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_59.w_0_fp32_master_0'], Moment1Out=['linear_59.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_59.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_59.w_0']} = adamw(inputs={Beta1Pow=['linear_59.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_59.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_59.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_59.w_0_fp32_master_0'], Moment1=['linear_59.w_0_fp32_master_0_moment1_0'], Moment2=['linear_59.w_0_fp32_master_0_moment2_0'], Param=['linear_59.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_176/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_59.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_59.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_59.b_0_fp32_master_0'], Moment1Out=['linear_59.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_59.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_59.b_0']} = adamw(inputs={Beta1Pow=['linear_59.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_59.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_59.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_59.b_0_fp32_master_0'], Moment1=['linear_59.b_0_fp32_master_0_moment1_0'], Moment2=['linear_59.b_0_fp32_master_0_moment2_0'], Param=['linear_59.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_177/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_28.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_28.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_28.w_0_moment1_0'], Moment2Out=['layer_norm_28.w_0_moment2_0'], ParamOut=['layer_norm_28.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_28.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_28.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_28.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_28.w_0_moment1_0'], Moment2=['layer_norm_28.w_0_moment2_0'], Param=['layer_norm_28.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_178/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_28.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_28.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_28.b_0_moment1_0'], Moment2Out=['layer_norm_28.b_0_moment2_0'], ParamOut=['layer_norm_28.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_28.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_28.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_28.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_28.b_0_moment1_0'], Moment2=['layer_norm_28.b_0_moment2_0'], Param=['layer_norm_28.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_179/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_29.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_29.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_29.w_0_moment1_0'], Moment2Out=['layer_norm_29.w_0_moment2_0'], ParamOut=['layer_norm_29.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_29.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_29.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_29.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_29.w_0_moment1_0'], Moment2=['layer_norm_29.w_0_moment2_0'], Param=['layer_norm_29.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_180/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_29.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_29.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_29.b_0_moment1_0'], Moment2Out=['layer_norm_29.b_0_moment2_0'], ParamOut=['layer_norm_29.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_29.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_29.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_29.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_29.b_0_moment1_0'], Moment2=['layer_norm_29.b_0_moment2_0'], Param=['layer_norm_29.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_181/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_60.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_60.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_60.w_0_fp32_master_0'], Moment1Out=['linear_60.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_60.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_60.w_0']} = adamw(inputs={Beta1Pow=['linear_60.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_60.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_60.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_60.w_0_fp32_master_0'], Moment1=['linear_60.w_0_fp32_master_0_moment1_0'], Moment2=['linear_60.w_0_fp32_master_0_moment2_0'], Param=['linear_60.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_182/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_60.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_60.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_60.b_0_fp32_master_0'], Moment1Out=['linear_60.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_60.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_60.b_0']} = adamw(inputs={Beta1Pow=['linear_60.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_60.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_60.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_60.b_0_fp32_master_0'], Moment1=['linear_60.b_0_fp32_master_0_moment1_0'], Moment2=['linear_60.b_0_fp32_master_0_moment2_0'], Param=['linear_60.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_183/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_61.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_61.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_61.w_0_fp32_master_0'], Moment1Out=['linear_61.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_61.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_61.w_0']} = adamw(inputs={Beta1Pow=['linear_61.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_61.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_61.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_61.w_0_fp32_master_0'], Moment1=['linear_61.w_0_fp32_master_0_moment1_0'], Moment2=['linear_61.w_0_fp32_master_0_moment2_0'], Param=['linear_61.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_184/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_61.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_61.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_61.b_0_fp32_master_0'], Moment1Out=['linear_61.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_61.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_61.b_0']} = adamw(inputs={Beta1Pow=['linear_61.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_61.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_61.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_61.b_0_fp32_master_0'], Moment1=['linear_61.b_0_fp32_master_0_moment1_0'], Moment2=['linear_61.b_0_fp32_master_0_moment2_0'], Param=['linear_61.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_185/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_62.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_62.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_62.w_0_fp32_master_0'], Moment1Out=['linear_62.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_62.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_62.w_0']} = adamw(inputs={Beta1Pow=['linear_62.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_62.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_62.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_62.w_0_fp32_master_0'], Moment1=['linear_62.w_0_fp32_master_0_moment1_0'], Moment2=['linear_62.w_0_fp32_master_0_moment2_0'], Param=['linear_62.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_186/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_62.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_62.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_62.b_0_fp32_master_0'], Moment1Out=['linear_62.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_62.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_62.b_0']} = adamw(inputs={Beta1Pow=['linear_62.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_62.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_62.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_62.b_0_fp32_master_0'], Moment1=['linear_62.b_0_fp32_master_0_moment1_0'], Moment2=['linear_62.b_0_fp32_master_0_moment2_0'], Param=['linear_62.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_187/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_63.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_63.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_63.w_0_fp32_master_0'], Moment1Out=['linear_63.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_63.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_63.w_0']} = adamw(inputs={Beta1Pow=['linear_63.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_63.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_63.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_63.w_0_fp32_master_0'], Moment1=['linear_63.w_0_fp32_master_0_moment1_0'], Moment2=['linear_63.w_0_fp32_master_0_moment2_0'], Param=['linear_63.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_188/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_63.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_63.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_63.b_0_fp32_master_0'], Moment1Out=['linear_63.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_63.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_63.b_0']} = adamw(inputs={Beta1Pow=['linear_63.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_63.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_63.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_63.b_0_fp32_master_0'], Moment1=['linear_63.b_0_fp32_master_0_moment1_0'], Moment2=['linear_63.b_0_fp32_master_0_moment2_0'], Param=['linear_63.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_189/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_30.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_30.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_30.w_0_moment1_0'], Moment2Out=['layer_norm_30.w_0_moment2_0'], ParamOut=['layer_norm_30.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_30.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_30.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_30.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_30.w_0_moment1_0'], Moment2=['layer_norm_30.w_0_moment2_0'], Param=['layer_norm_30.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_190/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_30.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_30.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_30.b_0_moment1_0'], Moment2Out=['layer_norm_30.b_0_moment2_0'], ParamOut=['layer_norm_30.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_30.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_30.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_30.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_30.b_0_moment1_0'], Moment2=['layer_norm_30.b_0_moment2_0'], Param=['layer_norm_30.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_191/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_31.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_31.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_31.w_0_moment1_0'], Moment2Out=['layer_norm_31.w_0_moment2_0'], ParamOut=['layer_norm_31.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_31.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_31.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_31.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_31.w_0_moment1_0'], Moment2=['layer_norm_31.w_0_moment2_0'], Param=['layer_norm_31.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_192/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_31.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_31.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_31.b_0_moment1_0'], Moment2Out=['layer_norm_31.b_0_moment2_0'], ParamOut=['layer_norm_31.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_31.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_31.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_31.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_31.b_0_moment1_0'], Moment2=['layer_norm_31.b_0_moment2_0'], Param=['layer_norm_31.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_193/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_64.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_64.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_64.w_0_fp32_master_0'], Moment1Out=['linear_64.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_64.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_64.w_0']} = adamw(inputs={Beta1Pow=['linear_64.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_64.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_64.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_64.w_0_fp32_master_0'], Moment1=['linear_64.w_0_fp32_master_0_moment1_0'], Moment2=['linear_64.w_0_fp32_master_0_moment2_0'], Param=['linear_64.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_194/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_64.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_64.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_64.b_0_fp32_master_0'], Moment1Out=['linear_64.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_64.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_64.b_0']} = adamw(inputs={Beta1Pow=['linear_64.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_64.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_64.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_64.b_0_fp32_master_0'], Moment1=['linear_64.b_0_fp32_master_0_moment1_0'], Moment2=['linear_64.b_0_fp32_master_0_moment2_0'], Param=['linear_64.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_195/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_65.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_65.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_65.w_0_fp32_master_0'], Moment1Out=['linear_65.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_65.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_65.w_0']} = adamw(inputs={Beta1Pow=['linear_65.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_65.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_65.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_65.w_0_fp32_master_0'], Moment1=['linear_65.w_0_fp32_master_0_moment1_0'], Moment2=['linear_65.w_0_fp32_master_0_moment2_0'], Param=['linear_65.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_196/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_65.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_65.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_65.b_0_fp32_master_0'], Moment1Out=['linear_65.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_65.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_65.b_0']} = adamw(inputs={Beta1Pow=['linear_65.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_65.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_65.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_65.b_0_fp32_master_0'], Moment1=['linear_65.b_0_fp32_master_0_moment1_0'], Moment2=['linear_65.b_0_fp32_master_0_moment2_0'], Param=['linear_65.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_197/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_66.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_66.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_66.w_0_fp32_master_0'], Moment1Out=['linear_66.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_66.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_66.w_0']} = adamw(inputs={Beta1Pow=['linear_66.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_66.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_66.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_66.w_0_fp32_master_0'], Moment1=['linear_66.w_0_fp32_master_0_moment1_0'], Moment2=['linear_66.w_0_fp32_master_0_moment2_0'], Param=['linear_66.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_198/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_66.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_66.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_66.b_0_fp32_master_0'], Moment1Out=['linear_66.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_66.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_66.b_0']} = adamw(inputs={Beta1Pow=['linear_66.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_66.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_66.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_66.b_0_fp32_master_0'], Moment1=['linear_66.b_0_fp32_master_0_moment1_0'], Moment2=['linear_66.b_0_fp32_master_0_moment2_0'], Param=['linear_66.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_199/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_67.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_67.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_67.w_0_fp32_master_0'], Moment1Out=['linear_67.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_67.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_67.w_0']} = adamw(inputs={Beta1Pow=['linear_67.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_67.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_67.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_67.w_0_fp32_master_0'], Moment1=['linear_67.w_0_fp32_master_0_moment1_0'], Moment2=['linear_67.w_0_fp32_master_0_moment2_0'], Param=['linear_67.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_200/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_67.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_67.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_67.b_0_fp32_master_0'], Moment1Out=['linear_67.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_67.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_67.b_0']} = adamw(inputs={Beta1Pow=['linear_67.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_67.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_67.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_67.b_0_fp32_master_0'], Moment1=['linear_67.b_0_fp32_master_0_moment1_0'], Moment2=['linear_67.b_0_fp32_master_0_moment2_0'], Param=['linear_67.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_201/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_32.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_32.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_32.w_0_moment1_0'], Moment2Out=['layer_norm_32.w_0_moment2_0'], ParamOut=['layer_norm_32.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_32.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_32.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_32.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_32.w_0_moment1_0'], Moment2=['layer_norm_32.w_0_moment2_0'], Param=['layer_norm_32.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_202/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_32.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_32.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_32.b_0_moment1_0'], Moment2Out=['layer_norm_32.b_0_moment2_0'], ParamOut=['layer_norm_32.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_32.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_32.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_32.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_32.b_0_moment1_0'], Moment2=['layer_norm_32.b_0_moment2_0'], Param=['layer_norm_32.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_203/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_33.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_33.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_33.w_0_moment1_0'], Moment2Out=['layer_norm_33.w_0_moment2_0'], ParamOut=['layer_norm_33.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_33.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_33.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_33.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_33.w_0_moment1_0'], Moment2=['layer_norm_33.w_0_moment2_0'], Param=['layer_norm_33.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_204/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_33.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_33.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_33.b_0_moment1_0'], Moment2Out=['layer_norm_33.b_0_moment2_0'], ParamOut=['layer_norm_33.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_33.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_33.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_33.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_33.b_0_moment1_0'], Moment2=['layer_norm_33.b_0_moment2_0'], Param=['layer_norm_33.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_205/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_68.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_68.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_68.w_0_fp32_master_0'], Moment1Out=['linear_68.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_68.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_68.w_0']} = adamw(inputs={Beta1Pow=['linear_68.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_68.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_68.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_68.w_0_fp32_master_0'], Moment1=['linear_68.w_0_fp32_master_0_moment1_0'], Moment2=['linear_68.w_0_fp32_master_0_moment2_0'], Param=['linear_68.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_206/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_68.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_68.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_68.b_0_fp32_master_0'], Moment1Out=['linear_68.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_68.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_68.b_0']} = adamw(inputs={Beta1Pow=['linear_68.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_68.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_68.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_68.b_0_fp32_master_0'], Moment1=['linear_68.b_0_fp32_master_0_moment1_0'], Moment2=['linear_68.b_0_fp32_master_0_moment2_0'], Param=['linear_68.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_207/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_69.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_69.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_69.w_0_fp32_master_0'], Moment1Out=['linear_69.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_69.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_69.w_0']} = adamw(inputs={Beta1Pow=['linear_69.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_69.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_69.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_69.w_0_fp32_master_0'], Moment1=['linear_69.w_0_fp32_master_0_moment1_0'], Moment2=['linear_69.w_0_fp32_master_0_moment2_0'], Param=['linear_69.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_208/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_69.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_69.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_69.b_0_fp32_master_0'], Moment1Out=['linear_69.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_69.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_69.b_0']} = adamw(inputs={Beta1Pow=['linear_69.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_69.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_69.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_69.b_0_fp32_master_0'], Moment1=['linear_69.b_0_fp32_master_0_moment1_0'], Moment2=['linear_69.b_0_fp32_master_0_moment2_0'], Param=['linear_69.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_209/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_70.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_70.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_70.w_0_fp32_master_0'], Moment1Out=['linear_70.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_70.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_70.w_0']} = adamw(inputs={Beta1Pow=['linear_70.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_70.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_70.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_70.w_0_fp32_master_0'], Moment1=['linear_70.w_0_fp32_master_0_moment1_0'], Moment2=['linear_70.w_0_fp32_master_0_moment2_0'], Param=['linear_70.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_210/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_70.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_70.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_70.b_0_fp32_master_0'], Moment1Out=['linear_70.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_70.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_70.b_0']} = adamw(inputs={Beta1Pow=['linear_70.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_70.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_70.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_70.b_0_fp32_master_0'], Moment1=['linear_70.b_0_fp32_master_0_moment1_0'], Moment2=['linear_70.b_0_fp32_master_0_moment2_0'], Param=['linear_70.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_211/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_71.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_71.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_71.w_0_fp32_master_0'], Moment1Out=['linear_71.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_71.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_71.w_0']} = adamw(inputs={Beta1Pow=['linear_71.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_71.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_71.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_71.w_0_fp32_master_0'], Moment1=['linear_71.w_0_fp32_master_0_moment1_0'], Moment2=['linear_71.w_0_fp32_master_0_moment2_0'], Param=['linear_71.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_212/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_71.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_71.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_71.b_0_fp32_master_0'], Moment1Out=['linear_71.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_71.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_71.b_0']} = adamw(inputs={Beta1Pow=['linear_71.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_71.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_71.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_71.b_0_fp32_master_0'], Moment1=['linear_71.b_0_fp32_master_0_moment1_0'], Moment2=['linear_71.b_0_fp32_master_0_moment2_0'], Param=['linear_71.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_213/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_34.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_34.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_34.w_0_moment1_0'], Moment2Out=['layer_norm_34.w_0_moment2_0'], ParamOut=['layer_norm_34.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_34.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_34.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_34.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_34.w_0_moment1_0'], Moment2=['layer_norm_34.w_0_moment2_0'], Param=['layer_norm_34.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_214/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_34.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_34.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_34.b_0_moment1_0'], Moment2Out=['layer_norm_34.b_0_moment2_0'], ParamOut=['layer_norm_34.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_34.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_34.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_34.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_34.b_0_moment1_0'], Moment2=['layer_norm_34.b_0_moment2_0'], Param=['layer_norm_34.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_215/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_35.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_35.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_35.w_0_moment1_0'], Moment2Out=['layer_norm_35.w_0_moment2_0'], ParamOut=['layer_norm_35.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_35.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_35.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_35.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_35.w_0_moment1_0'], Moment2=['layer_norm_35.w_0_moment2_0'], Param=['layer_norm_35.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_216/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_35.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_35.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_35.b_0_moment1_0'], Moment2Out=['layer_norm_35.b_0_moment2_0'], ParamOut=['layer_norm_35.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_35.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_35.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_35.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_35.b_0_moment1_0'], Moment2=['layer_norm_35.b_0_moment2_0'], Param=['layer_norm_35.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_217/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_72.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_72.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_72.w_0_fp32_master_0'], Moment1Out=['linear_72.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_72.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_72.w_0']} = adamw(inputs={Beta1Pow=['linear_72.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_72.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_72.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_72.w_0_fp32_master_0'], Moment1=['linear_72.w_0_fp32_master_0_moment1_0'], Moment2=['linear_72.w_0_fp32_master_0_moment2_0'], Param=['linear_72.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_218/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_72.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_72.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_72.b_0_fp32_master_0'], Moment1Out=['linear_72.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_72.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_72.b_0']} = adamw(inputs={Beta1Pow=['linear_72.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_72.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_72.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_72.b_0_fp32_master_0'], Moment1=['linear_72.b_0_fp32_master_0_moment1_0'], Moment2=['linear_72.b_0_fp32_master_0_moment2_0'], Param=['linear_72.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_219/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_73.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_73.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_73.w_0_fp32_master_0'], Moment1Out=['linear_73.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_73.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_73.w_0']} = adamw(inputs={Beta1Pow=['linear_73.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_73.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_73.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_73.w_0_fp32_master_0'], Moment1=['linear_73.w_0_fp32_master_0_moment1_0'], Moment2=['linear_73.w_0_fp32_master_0_moment2_0'], Param=['linear_73.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_220/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_73.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_73.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_73.b_0_fp32_master_0'], Moment1Out=['linear_73.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_73.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_73.b_0']} = adamw(inputs={Beta1Pow=['linear_73.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_73.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_73.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_73.b_0_fp32_master_0'], Moment1=['linear_73.b_0_fp32_master_0_moment1_0'], Moment2=['linear_73.b_0_fp32_master_0_moment2_0'], Param=['linear_73.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_221/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_74.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_74.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_74.w_0_fp32_master_0'], Moment1Out=['linear_74.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_74.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_74.w_0']} = adamw(inputs={Beta1Pow=['linear_74.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_74.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_74.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_74.w_0_fp32_master_0'], Moment1=['linear_74.w_0_fp32_master_0_moment1_0'], Moment2=['linear_74.w_0_fp32_master_0_moment2_0'], Param=['linear_74.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_222/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_74.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_74.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_74.b_0_fp32_master_0'], Moment1Out=['linear_74.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_74.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_74.b_0']} = adamw(inputs={Beta1Pow=['linear_74.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_74.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_74.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_74.b_0_fp32_master_0'], Moment1=['linear_74.b_0_fp32_master_0_moment1_0'], Moment2=['linear_74.b_0_fp32_master_0_moment2_0'], Param=['linear_74.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_223/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_75.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_75.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_75.w_0_fp32_master_0'], Moment1Out=['linear_75.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_75.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_75.w_0']} = adamw(inputs={Beta1Pow=['linear_75.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_75.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_75.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_75.w_0_fp32_master_0'], Moment1=['linear_75.w_0_fp32_master_0_moment1_0'], Moment2=['linear_75.w_0_fp32_master_0_moment2_0'], Param=['linear_75.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_224/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_75.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_75.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_75.b_0_fp32_master_0'], Moment1Out=['linear_75.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_75.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_75.b_0']} = adamw(inputs={Beta1Pow=['linear_75.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_75.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_75.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_75.b_0_fp32_master_0'], Moment1=['linear_75.b_0_fp32_master_0_moment1_0'], Moment2=['linear_75.b_0_fp32_master_0_moment2_0'], Param=['linear_75.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_225/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_36.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_36.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_36.w_0_moment1_0'], Moment2Out=['layer_norm_36.w_0_moment2_0'], ParamOut=['layer_norm_36.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_36.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_36.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_36.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_36.w_0_moment1_0'], Moment2=['layer_norm_36.w_0_moment2_0'], Param=['layer_norm_36.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_226/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_36.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_36.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_36.b_0_moment1_0'], Moment2Out=['layer_norm_36.b_0_moment2_0'], ParamOut=['layer_norm_36.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_36.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_36.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_36.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_36.b_0_moment1_0'], Moment2=['layer_norm_36.b_0_moment2_0'], Param=['layer_norm_36.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_227/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_37.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_37.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_37.w_0_moment1_0'], Moment2Out=['layer_norm_37.w_0_moment2_0'], ParamOut=['layer_norm_37.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_37.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_37.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_37.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_37.w_0_moment1_0'], Moment2=['layer_norm_37.w_0_moment2_0'], Param=['layer_norm_37.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_228/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_37.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_37.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_37.b_0_moment1_0'], Moment2Out=['layer_norm_37.b_0_moment2_0'], ParamOut=['layer_norm_37.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_37.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_37.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_37.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_37.b_0_moment1_0'], Moment2=['layer_norm_37.b_0_moment2_0'], Param=['layer_norm_37.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_229/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_76.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_76.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_76.w_0_fp32_master_0'], Moment1Out=['linear_76.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_76.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_76.w_0']} = adamw(inputs={Beta1Pow=['linear_76.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_76.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_76.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_76.w_0_fp32_master_0'], Moment1=['linear_76.w_0_fp32_master_0_moment1_0'], Moment2=['linear_76.w_0_fp32_master_0_moment2_0'], Param=['linear_76.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_230/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_76.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_76.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_76.b_0_fp32_master_0'], Moment1Out=['linear_76.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_76.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_76.b_0']} = adamw(inputs={Beta1Pow=['linear_76.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_76.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_76.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_76.b_0_fp32_master_0'], Moment1=['linear_76.b_0_fp32_master_0_moment1_0'], Moment2=['linear_76.b_0_fp32_master_0_moment2_0'], Param=['linear_76.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_231/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_77.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_77.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_77.w_0_fp32_master_0'], Moment1Out=['linear_77.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_77.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_77.w_0']} = adamw(inputs={Beta1Pow=['linear_77.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_77.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_77.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_77.w_0_fp32_master_0'], Moment1=['linear_77.w_0_fp32_master_0_moment1_0'], Moment2=['linear_77.w_0_fp32_master_0_moment2_0'], Param=['linear_77.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_232/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_77.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_77.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_77.b_0_fp32_master_0'], Moment1Out=['linear_77.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_77.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_77.b_0']} = adamw(inputs={Beta1Pow=['linear_77.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_77.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_77.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_77.b_0_fp32_master_0'], Moment1=['linear_77.b_0_fp32_master_0_moment1_0'], Moment2=['linear_77.b_0_fp32_master_0_moment2_0'], Param=['linear_77.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_233/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_78.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_78.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_78.w_0_fp32_master_0'], Moment1Out=['linear_78.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_78.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_78.w_0']} = adamw(inputs={Beta1Pow=['linear_78.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_78.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_78.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_78.w_0_fp32_master_0'], Moment1=['linear_78.w_0_fp32_master_0_moment1_0'], Moment2=['linear_78.w_0_fp32_master_0_moment2_0'], Param=['linear_78.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_234/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_78.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_78.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_78.b_0_fp32_master_0'], Moment1Out=['linear_78.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_78.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_78.b_0']} = adamw(inputs={Beta1Pow=['linear_78.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_78.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_78.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_78.b_0_fp32_master_0'], Moment1=['linear_78.b_0_fp32_master_0_moment1_0'], Moment2=['linear_78.b_0_fp32_master_0_moment2_0'], Param=['linear_78.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_235/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_79.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_79.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_79.w_0_fp32_master_0'], Moment1Out=['linear_79.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_79.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_79.w_0']} = adamw(inputs={Beta1Pow=['linear_79.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_79.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_79.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_79.w_0_fp32_master_0'], Moment1=['linear_79.w_0_fp32_master_0_moment1_0'], Moment2=['linear_79.w_0_fp32_master_0_moment2_0'], Param=['linear_79.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_236/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_79.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_79.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_79.b_0_fp32_master_0'], Moment1Out=['linear_79.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_79.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_79.b_0']} = adamw(inputs={Beta1Pow=['linear_79.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_79.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_79.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_79.b_0_fp32_master_0'], Moment1=['linear_79.b_0_fp32_master_0_moment1_0'], Moment2=['linear_79.b_0_fp32_master_0_moment2_0'], Param=['linear_79.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_237/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_38.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_38.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_38.w_0_moment1_0'], Moment2Out=['layer_norm_38.w_0_moment2_0'], ParamOut=['layer_norm_38.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_38.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_38.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_38.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_38.w_0_moment1_0'], Moment2=['layer_norm_38.w_0_moment2_0'], Param=['layer_norm_38.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_238/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_38.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_38.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_38.b_0_moment1_0'], Moment2Out=['layer_norm_38.b_0_moment2_0'], ParamOut=['layer_norm_38.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_38.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_38.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_38.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_38.b_0_moment1_0'], Moment2=['layer_norm_38.b_0_moment2_0'], Param=['layer_norm_38.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_239/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_39.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_39.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_39.w_0_moment1_0'], Moment2Out=['layer_norm_39.w_0_moment2_0'], ParamOut=['layer_norm_39.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_39.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_39.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_39.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_39.w_0_moment1_0'], Moment2=['layer_norm_39.w_0_moment2_0'], Param=['layer_norm_39.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_240/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_39.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_39.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_39.b_0_moment1_0'], Moment2Out=['layer_norm_39.b_0_moment2_0'], ParamOut=['layer_norm_39.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_39.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_39.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_39.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_39.b_0_moment1_0'], Moment2=['layer_norm_39.b_0_moment2_0'], Param=['layer_norm_39.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_241/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_80.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_80.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_80.w_0_fp32_master_0'], Moment1Out=['linear_80.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_80.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_80.w_0']} = adamw(inputs={Beta1Pow=['linear_80.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_80.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_80.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_80.w_0_fp32_master_0'], Moment1=['linear_80.w_0_fp32_master_0_moment1_0'], Moment2=['linear_80.w_0_fp32_master_0_moment2_0'], Param=['linear_80.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_242/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_80.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_80.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_80.b_0_fp32_master_0'], Moment1Out=['linear_80.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_80.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_80.b_0']} = adamw(inputs={Beta1Pow=['linear_80.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_80.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_80.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_80.b_0_fp32_master_0'], Moment1=['linear_80.b_0_fp32_master_0_moment1_0'], Moment2=['linear_80.b_0_fp32_master_0_moment2_0'], Param=['linear_80.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_243/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_81.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_81.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_81.w_0_fp32_master_0'], Moment1Out=['linear_81.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_81.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_81.w_0']} = adamw(inputs={Beta1Pow=['linear_81.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_81.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_81.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_81.w_0_fp32_master_0'], Moment1=['linear_81.w_0_fp32_master_0_moment1_0'], Moment2=['linear_81.w_0_fp32_master_0_moment2_0'], Param=['linear_81.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_244/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_81.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_81.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_81.b_0_fp32_master_0'], Moment1Out=['linear_81.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_81.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_81.b_0']} = adamw(inputs={Beta1Pow=['linear_81.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_81.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_81.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_81.b_0_fp32_master_0'], Moment1=['linear_81.b_0_fp32_master_0_moment1_0'], Moment2=['linear_81.b_0_fp32_master_0_moment2_0'], Param=['linear_81.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_245/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_82.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_82.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_82.w_0_fp32_master_0'], Moment1Out=['linear_82.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_82.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_82.w_0']} = adamw(inputs={Beta1Pow=['linear_82.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_82.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_82.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_82.w_0_fp32_master_0'], Moment1=['linear_82.w_0_fp32_master_0_moment1_0'], Moment2=['linear_82.w_0_fp32_master_0_moment2_0'], Param=['linear_82.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_246/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_82.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_82.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_82.b_0_fp32_master_0'], Moment1Out=['linear_82.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_82.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_82.b_0']} = adamw(inputs={Beta1Pow=['linear_82.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_82.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_82.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_82.b_0_fp32_master_0'], Moment1=['linear_82.b_0_fp32_master_0_moment1_0'], Moment2=['linear_82.b_0_fp32_master_0_moment2_0'], Param=['linear_82.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_247/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_83.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_83.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_83.w_0_fp32_master_0'], Moment1Out=['linear_83.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_83.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_83.w_0']} = adamw(inputs={Beta1Pow=['linear_83.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_83.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_83.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_83.w_0_fp32_master_0'], Moment1=['linear_83.w_0_fp32_master_0_moment1_0'], Moment2=['linear_83.w_0_fp32_master_0_moment2_0'], Param=['linear_83.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_248/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_83.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_83.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_83.b_0_fp32_master_0'], Moment1Out=['linear_83.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_83.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_83.b_0']} = adamw(inputs={Beta1Pow=['linear_83.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_83.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_83.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_83.b_0_fp32_master_0'], Moment1=['linear_83.b_0_fp32_master_0_moment1_0'], Moment2=['linear_83.b_0_fp32_master_0_moment2_0'], Param=['linear_83.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_249/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_40.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_40.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_40.w_0_moment1_0'], Moment2Out=['layer_norm_40.w_0_moment2_0'], ParamOut=['layer_norm_40.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_40.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_40.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_40.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_40.w_0_moment1_0'], Moment2=['layer_norm_40.w_0_moment2_0'], Param=['layer_norm_40.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_250/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_40.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_40.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_40.b_0_moment1_0'], Moment2Out=['layer_norm_40.b_0_moment2_0'], ParamOut=['layer_norm_40.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_40.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_40.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_40.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_40.b_0_moment1_0'], Moment2=['layer_norm_40.b_0_moment2_0'], Param=['layer_norm_40.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_251/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_41.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_41.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_41.w_0_moment1_0'], Moment2Out=['layer_norm_41.w_0_moment2_0'], ParamOut=['layer_norm_41.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_41.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_41.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_41.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_41.w_0_moment1_0'], Moment2=['layer_norm_41.w_0_moment2_0'], Param=['layer_norm_41.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_252/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_41.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_41.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_41.b_0_moment1_0'], Moment2Out=['layer_norm_41.b_0_moment2_0'], ParamOut=['layer_norm_41.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_41.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_41.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_41.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_41.b_0_moment1_0'], Moment2=['layer_norm_41.b_0_moment2_0'], Param=['layer_norm_41.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_253/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_84.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_84.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_84.w_0_fp32_master_0'], Moment1Out=['linear_84.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_84.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_84.w_0']} = adamw(inputs={Beta1Pow=['linear_84.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_84.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_84.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_84.w_0_fp32_master_0'], Moment1=['linear_84.w_0_fp32_master_0_moment1_0'], Moment2=['linear_84.w_0_fp32_master_0_moment2_0'], Param=['linear_84.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_254/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_84.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_84.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_84.b_0_fp32_master_0'], Moment1Out=['linear_84.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_84.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_84.b_0']} = adamw(inputs={Beta1Pow=['linear_84.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_84.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_84.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_84.b_0_fp32_master_0'], Moment1=['linear_84.b_0_fp32_master_0_moment1_0'], Moment2=['linear_84.b_0_fp32_master_0_moment2_0'], Param=['linear_84.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_255/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_85.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_85.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_85.w_0_fp32_master_0'], Moment1Out=['linear_85.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_85.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_85.w_0']} = adamw(inputs={Beta1Pow=['linear_85.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_85.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_85.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_85.w_0_fp32_master_0'], Moment1=['linear_85.w_0_fp32_master_0_moment1_0'], Moment2=['linear_85.w_0_fp32_master_0_moment2_0'], Param=['linear_85.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_256/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_85.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_85.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_85.b_0_fp32_master_0'], Moment1Out=['linear_85.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_85.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_85.b_0']} = adamw(inputs={Beta1Pow=['linear_85.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_85.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_85.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_85.b_0_fp32_master_0'], Moment1=['linear_85.b_0_fp32_master_0_moment1_0'], Moment2=['linear_85.b_0_fp32_master_0_moment2_0'], Param=['linear_85.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_257/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_86.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_86.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_86.w_0_fp32_master_0'], Moment1Out=['linear_86.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_86.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_86.w_0']} = adamw(inputs={Beta1Pow=['linear_86.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_86.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_86.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_86.w_0_fp32_master_0'], Moment1=['linear_86.w_0_fp32_master_0_moment1_0'], Moment2=['linear_86.w_0_fp32_master_0_moment2_0'], Param=['linear_86.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_258/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_86.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_86.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_86.b_0_fp32_master_0'], Moment1Out=['linear_86.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_86.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_86.b_0']} = adamw(inputs={Beta1Pow=['linear_86.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_86.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_86.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_86.b_0_fp32_master_0'], Moment1=['linear_86.b_0_fp32_master_0_moment1_0'], Moment2=['linear_86.b_0_fp32_master_0_moment2_0'], Param=['linear_86.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_259/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_87.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_87.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_87.w_0_fp32_master_0'], Moment1Out=['linear_87.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_87.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_87.w_0']} = adamw(inputs={Beta1Pow=['linear_87.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_87.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_87.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_87.w_0_fp32_master_0'], Moment1=['linear_87.w_0_fp32_master_0_moment1_0'], Moment2=['linear_87.w_0_fp32_master_0_moment2_0'], Param=['linear_87.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_260/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_87.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_87.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_87.b_0_fp32_master_0'], Moment1Out=['linear_87.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_87.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_87.b_0']} = adamw(inputs={Beta1Pow=['linear_87.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_87.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_87.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_87.b_0_fp32_master_0'], Moment1=['linear_87.b_0_fp32_master_0_moment1_0'], Moment2=['linear_87.b_0_fp32_master_0_moment2_0'], Param=['linear_87.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_261/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_42.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_42.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_42.w_0_moment1_0'], Moment2Out=['layer_norm_42.w_0_moment2_0'], ParamOut=['layer_norm_42.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_42.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_42.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_42.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_42.w_0_moment1_0'], Moment2=['layer_norm_42.w_0_moment2_0'], Param=['layer_norm_42.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_262/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_42.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_42.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_42.b_0_moment1_0'], Moment2Out=['layer_norm_42.b_0_moment2_0'], ParamOut=['layer_norm_42.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_42.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_42.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_42.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_42.b_0_moment1_0'], Moment2=['layer_norm_42.b_0_moment2_0'], Param=['layer_norm_42.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_263/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_43.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_43.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_43.w_0_moment1_0'], Moment2Out=['layer_norm_43.w_0_moment2_0'], ParamOut=['layer_norm_43.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_43.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_43.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_43.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_43.w_0_moment1_0'], Moment2=['layer_norm_43.w_0_moment2_0'], Param=['layer_norm_43.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_264/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_43.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_43.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_43.b_0_moment1_0'], Moment2Out=['layer_norm_43.b_0_moment2_0'], ParamOut=['layer_norm_43.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_43.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_43.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_43.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_43.b_0_moment1_0'], Moment2=['layer_norm_43.b_0_moment2_0'], Param=['layer_norm_43.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_265/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_88.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_88.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_88.w_0_fp32_master_0'], Moment1Out=['linear_88.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_88.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_88.w_0']} = adamw(inputs={Beta1Pow=['linear_88.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_88.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_88.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_88.w_0_fp32_master_0'], Moment1=['linear_88.w_0_fp32_master_0_moment1_0'], Moment2=['linear_88.w_0_fp32_master_0_moment2_0'], Param=['linear_88.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_266/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_88.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_88.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_88.b_0_fp32_master_0'], Moment1Out=['linear_88.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_88.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_88.b_0']} = adamw(inputs={Beta1Pow=['linear_88.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_88.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_88.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_88.b_0_fp32_master_0'], Moment1=['linear_88.b_0_fp32_master_0_moment1_0'], Moment2=['linear_88.b_0_fp32_master_0_moment2_0'], Param=['linear_88.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_267/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_89.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_89.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_89.w_0_fp32_master_0'], Moment1Out=['linear_89.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_89.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_89.w_0']} = adamw(inputs={Beta1Pow=['linear_89.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_89.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_89.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_89.w_0_fp32_master_0'], Moment1=['linear_89.w_0_fp32_master_0_moment1_0'], Moment2=['linear_89.w_0_fp32_master_0_moment2_0'], Param=['linear_89.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_268/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_89.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_89.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_89.b_0_fp32_master_0'], Moment1Out=['linear_89.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_89.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_89.b_0']} = adamw(inputs={Beta1Pow=['linear_89.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_89.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_89.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_89.b_0_fp32_master_0'], Moment1=['linear_89.b_0_fp32_master_0_moment1_0'], Moment2=['linear_89.b_0_fp32_master_0_moment2_0'], Param=['linear_89.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_269/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_90.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_90.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_90.w_0_fp32_master_0'], Moment1Out=['linear_90.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_90.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_90.w_0']} = adamw(inputs={Beta1Pow=['linear_90.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_90.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_90.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_90.w_0_fp32_master_0'], Moment1=['linear_90.w_0_fp32_master_0_moment1_0'], Moment2=['linear_90.w_0_fp32_master_0_moment2_0'], Param=['linear_90.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_270/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_90.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_90.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_90.b_0_fp32_master_0'], Moment1Out=['linear_90.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_90.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_90.b_0']} = adamw(inputs={Beta1Pow=['linear_90.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_90.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_90.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_90.b_0_fp32_master_0'], Moment1=['linear_90.b_0_fp32_master_0_moment1_0'], Moment2=['linear_90.b_0_fp32_master_0_moment2_0'], Param=['linear_90.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_271/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_91.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_91.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_91.w_0_fp32_master_0'], Moment1Out=['linear_91.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_91.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_91.w_0']} = adamw(inputs={Beta1Pow=['linear_91.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_91.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_91.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_91.w_0_fp32_master_0'], Moment1=['linear_91.w_0_fp32_master_0_moment1_0'], Moment2=['linear_91.w_0_fp32_master_0_moment2_0'], Param=['linear_91.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_272/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_91.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_91.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_91.b_0_fp32_master_0'], Moment1Out=['linear_91.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_91.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_91.b_0']} = adamw(inputs={Beta1Pow=['linear_91.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_91.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_91.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_91.b_0_fp32_master_0'], Moment1=['linear_91.b_0_fp32_master_0_moment1_0'], Moment2=['linear_91.b_0_fp32_master_0_moment2_0'], Param=['linear_91.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_273/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_44.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_44.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_44.w_0_moment1_0'], Moment2Out=['layer_norm_44.w_0_moment2_0'], ParamOut=['layer_norm_44.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_44.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_44.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_44.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_44.w_0_moment1_0'], Moment2=['layer_norm_44.w_0_moment2_0'], Param=['layer_norm_44.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_274/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_44.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_44.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_44.b_0_moment1_0'], Moment2Out=['layer_norm_44.b_0_moment2_0'], ParamOut=['layer_norm_44.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_44.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_44.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_44.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_44.b_0_moment1_0'], Moment2=['layer_norm_44.b_0_moment2_0'], Param=['layer_norm_44.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_275/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_45.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_45.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_45.w_0_moment1_0'], Moment2Out=['layer_norm_45.w_0_moment2_0'], ParamOut=['layer_norm_45.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_45.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_45.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_45.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_45.w_0_moment1_0'], Moment2=['layer_norm_45.w_0_moment2_0'], Param=['layer_norm_45.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_276/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_45.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_45.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_45.b_0_moment1_0'], Moment2Out=['layer_norm_45.b_0_moment2_0'], ParamOut=['layer_norm_45.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_45.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_45.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_45.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_45.b_0_moment1_0'], Moment2=['layer_norm_45.b_0_moment2_0'], Param=['layer_norm_45.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_277/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_92.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_92.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_92.w_0_fp32_master_0'], Moment1Out=['linear_92.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_92.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_92.w_0']} = adamw(inputs={Beta1Pow=['linear_92.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_92.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_92.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_92.w_0_fp32_master_0'], Moment1=['linear_92.w_0_fp32_master_0_moment1_0'], Moment2=['linear_92.w_0_fp32_master_0_moment2_0'], Param=['linear_92.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_278/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_92.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_92.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_92.b_0_fp32_master_0'], Moment1Out=['linear_92.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_92.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_92.b_0']} = adamw(inputs={Beta1Pow=['linear_92.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_92.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_92.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_92.b_0_fp32_master_0'], Moment1=['linear_92.b_0_fp32_master_0_moment1_0'], Moment2=['linear_92.b_0_fp32_master_0_moment2_0'], Param=['linear_92.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_279/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_93.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_93.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_93.w_0_fp32_master_0'], Moment1Out=['linear_93.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_93.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_93.w_0']} = adamw(inputs={Beta1Pow=['linear_93.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_93.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_93.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_93.w_0_fp32_master_0'], Moment1=['linear_93.w_0_fp32_master_0_moment1_0'], Moment2=['linear_93.w_0_fp32_master_0_moment2_0'], Param=['linear_93.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_280/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_93.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_93.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_93.b_0_fp32_master_0'], Moment1Out=['linear_93.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_93.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_93.b_0']} = adamw(inputs={Beta1Pow=['linear_93.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_93.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_93.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_93.b_0_fp32_master_0'], Moment1=['linear_93.b_0_fp32_master_0_moment1_0'], Moment2=['linear_93.b_0_fp32_master_0_moment2_0'], Param=['linear_93.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_281/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_94.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_94.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_94.w_0_fp32_master_0'], Moment1Out=['linear_94.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_94.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_94.w_0']} = adamw(inputs={Beta1Pow=['linear_94.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_94.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_94.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_94.w_0_fp32_master_0'], Moment1=['linear_94.w_0_fp32_master_0_moment1_0'], Moment2=['linear_94.w_0_fp32_master_0_moment2_0'], Param=['linear_94.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_282/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_94.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_94.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_94.b_0_fp32_master_0'], Moment1Out=['linear_94.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_94.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_94.b_0']} = adamw(inputs={Beta1Pow=['linear_94.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_94.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_94.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_94.b_0_fp32_master_0'], Moment1=['linear_94.b_0_fp32_master_0_moment1_0'], Moment2=['linear_94.b_0_fp32_master_0_moment2_0'], Param=['linear_94.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_283/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_95.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_95.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_95.w_0_fp32_master_0'], Moment1Out=['linear_95.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_95.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_95.w_0']} = adamw(inputs={Beta1Pow=['linear_95.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_95.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_95.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_95.w_0_fp32_master_0'], Moment1=['linear_95.w_0_fp32_master_0_moment1_0'], Moment2=['linear_95.w_0_fp32_master_0_moment2_0'], Param=['linear_95.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_284/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_95.b_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_95.b_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_95.b_0_fp32_master_0'], Moment1Out=['linear_95.b_0_fp32_master_0_moment1_0'], Moment2Out=['linear_95.b_0_fp32_master_0_moment2_0'], ParamOut=['linear_95.b_0']} = adamw(inputs={Beta1Pow=['linear_95.b_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_95.b_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_95.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_95.b_0_fp32_master_0'], Moment1=['linear_95.b_0_fp32_master_0_moment1_0'], Moment2=['linear_95.b_0_fp32_master_0_moment2_0'], Param=['linear_95.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_285/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_46.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_46.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_46.w_0_moment1_0'], Moment2Out=['layer_norm_46.w_0_moment2_0'], ParamOut=['layer_norm_46.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_46.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_46.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_46.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_46.w_0_moment1_0'], Moment2=['layer_norm_46.w_0_moment2_0'], Param=['layer_norm_46.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_286/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_46.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_46.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_46.b_0_moment1_0'], Moment2Out=['layer_norm_46.b_0_moment2_0'], ParamOut=['layer_norm_46.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_46.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_46.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_46.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_46.b_0_moment1_0'], Moment2=['layer_norm_46.b_0_moment2_0'], Param=['layer_norm_46.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_287/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_47.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_47.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_47.w_0_moment1_0'], Moment2Out=['layer_norm_47.w_0_moment2_0'], ParamOut=['layer_norm_47.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_47.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_47.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_47.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_47.w_0_moment1_0'], Moment2=['layer_norm_47.w_0_moment2_0'], Param=['layer_norm_47.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_288/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_47.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_47.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_47.b_0_moment1_0'], Moment2Out=['layer_norm_47.b_0_moment2_0'], ParamOut=['layer_norm_47.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_47.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_47.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_47.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_47.b_0_moment1_0'], Moment2=['layer_norm_47.b_0_moment2_0'], Param=['layer_norm_47.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_289/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_48.w_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_48.w_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_48.w_0_moment1_0'], Moment2Out=['layer_norm_48.w_0_moment2_0'], ParamOut=['layer_norm_48.w_0']} = adamw(inputs={Beta1Pow=['layer_norm_48.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_48.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_48.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_48.w_0_moment1_0'], Moment2=['layer_norm_48.w_0_moment2_0'], Param=['layer_norm_48.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_290/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['layer_norm_48.b_0_beta1_pow_acc_0'], Beta2PowOut=['layer_norm_48.b_0_beta2_pow_acc_0'], Moment1Out=['layer_norm_48.b_0_moment1_0'], Moment2Out=['layer_norm_48.b_0_moment2_0'], ParamOut=['layer_norm_48.b_0']} = adamw(inputs={Beta1Pow=['layer_norm_48.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['layer_norm_48.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['layer_norm_48.b_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=[], Moment1=['layer_norm_48.b_0_moment1_0'], Moment2=['layer_norm_48.b_0_moment2_0'], Param=['layer_norm_48.b_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_291/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Out=['layer_norm_24.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_24.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_24.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_24.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_48.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_48.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_48.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_48.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_49.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_49.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_49.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_49.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_25.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_25.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_25.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_25.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_50.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_50.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_50.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_50.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_51.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_51.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_51.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_51.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_26.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_26.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_26.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_26.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_52.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_52.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_52.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_52.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_53.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_53.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_53.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_53.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_27.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_27.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_27.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_27.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_54.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_54.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_54.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_54.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_55.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_55.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_55.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_55.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_28.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_28.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_28.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_28.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_56.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_56.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_56.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_56.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_57.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_57.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_57.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_57.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_29.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_29.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_29.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_29.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_58.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_58.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_58.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_58.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_59.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_59.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_59.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_59.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_30.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_30.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_30.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_30.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_60.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_60.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_60.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_60.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_61.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_61.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_61.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_61.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_31.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_31.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_31.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_31.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_62.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_62.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_62.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_62.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_63.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_63.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_63.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_63.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_32.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_32.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_32.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_32.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_64.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_64.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_64.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_64.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_65.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_65.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_65.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_65.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_33.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_33.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_33.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_33.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_66.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_66.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_66.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_66.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_67.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_67.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_67.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_67.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_34.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_34.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_34.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_34.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_68.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_68.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_68.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_68.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_69.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_69.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_69.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_69.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_35.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_35.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_35.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_35.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_70.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_70.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_70.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_70.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_71.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_71.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_71.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_71.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_36.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_36.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_36.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_36.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_72.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_72.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_72.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_72.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_73.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_73.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_73.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_73.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_37.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_37.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_37.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_37.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_74.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_74.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_74.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_74.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_75.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_75.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_75.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_75.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_38.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_38.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_38.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_38.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_76.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_76.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_76.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_76.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_77.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_77.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_77.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_77.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_39.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_39.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_39.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_39.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_78.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_78.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_78.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_78.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_79.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_79.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_79.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_79.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_40.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_40.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_40.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_40.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_80.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_80.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_80.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_80.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_81.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_81.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_81.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_81.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_41.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_41.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_41.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_41.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_82.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_82.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_82.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_82.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_83.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_83.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_83.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_83.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_42.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_42.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_42.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_42.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_84.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_84.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_84.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_84.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_85.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_85.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_85.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_85.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_43.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_43.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_43.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_43.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_86.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_86.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_86.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_86.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_87.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_87.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_87.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_87.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_44.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_44.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_44.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_44.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_88.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_88.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_88.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_88.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_89.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_89.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_89.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_89.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_45.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_45.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_45.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_45.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_90.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_90.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_90.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_90.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_91.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_91.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_91.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_91.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_46.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_46.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_46.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_46.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_92.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_92.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_92.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_92.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_93.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_93.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_93.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_93.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_47.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_47.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_47.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_47.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_94.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_94.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_94.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_94.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_95.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_95.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_95.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_95.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_48.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_48.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['layer_norm_48.b_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['layer_norm_48.b_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 5, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
}

[2024-03-01 03:14:34,469] [    INFO] process_group.py:150 - group_id: 0, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:58859
[2024-03-01 03:14:34,973] [    INFO] process_group.py:150 - group_id: 31, ranks: [1, 3], nranks: 2, trainer_endpoints: 172.17.0.3:58859
[2024-03-01 03:14:35,165] [    INFO] process_group.py:150 - group_id: 32, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:58859
[2024-03-01 03:14:35,662] [    INFO] process_group.py:150 - group_id: 34, ranks: [2, 3], nranks: 2, trainer_endpoints: 172.17.0.3:58859
/usr/local/lib/python3.9/dist-packages/paddle/distributed/auto_parallel/static/process_group.py:255: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-03-01 03:14:35,980] [    INFO] process_group.py:150 - group_id: 36, ranks: [3, 2], nranks: 2, trainer_endpoints: 172.17.0.3:58859
I0301 03:14:38.100553 30609 program_interpreter.cc:220] New Executor is Running.
/home/workspace/PaddleNLP/model_zoo/gpt-3/ppfleetx/utils/device.py:50: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
I0301 03:14:51.843945 30609 interpreter_util.cc:652] Standalone Executor is Used.
I0301 03:14:52.966132 31060 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
[32m[2024-03-01 03:15:02,040] [INFO][0m - [train] epoch: [0/1], batch: [9/30], loss: 11.086989212, avg_batch_cost: 2.35541 sec, speed: 0.42 step/s, ips_total: 1739 tokens/s, ips: 869 tokens/s, loss_scale: 32768.000000000, learning rate: 1.38889e-07, found_inf: 0, max_memory_allocated: 2631.4 MB, max_memory_reserved: 2809.7 MB, memory_allocated: 2306.7 MB, memory_reserved: 2809.7 MB[0m
[32m[2024-03-01 03:15:11,288] [INFO][0m - [train] epoch: [0/1], batch: [19/30], loss: 11.094606400, avg_batch_cost: 0.92467 sec, speed: 1.08 step/s, ips_total: 4430 tokens/s, ips: 2215 tokens/s, loss_scale: 32768.000000000, learning rate: 2.77778e-07, found_inf: 0, max_memory_allocated: 2631.4 MB, max_memory_reserved: 2809.7 MB, memory_allocated: 2306.7 MB, memory_reserved: 2809.7 MB[0m
[32m[2024-03-01 03:15:20,668] [INFO][0m - [train] epoch: [0/1], batch: [29/30], loss: 11.087526703, avg_batch_cost: 0.93789 sec, speed: 1.07 step/s, ips_total: 4367 tokens/s, ips: 2184 tokens/s, loss_scale: 32768.000000000, learning rate: 4.16667e-07, found_inf: 0, max_memory_allocated: 2631.4 MB, max_memory_reserved: 2809.7 MB, memory_allocated: 2306.7 MB, memory_reserved: 2809.7 MB[0m
[32m[2024-03-01 03:15:21,626] [INFO][0m - [Training] epoch: 0, total time: 43.14042 sec[0m
[32m[2024-03-01 03:15:21,626] [INFO][0m - The training process is complete and total cost of time for training is : 0:01:12[0m
I0301 03:15:23.455338 30609 process_group_nccl.cc:133] ProcessGroupNCCL destruct 
I0301 03:15:23.455500 30609 process_group_nccl.cc:133] ProcessGroupNCCL destruct 
I0301 03:15:23.455514 30609 process_group_nccl.cc:133] ProcessGroupNCCL destruct 
