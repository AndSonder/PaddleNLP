[2024-03-01 08:42:28,773] [    INFO] pipeline_pass_base.py:69 - sub_program type: forward, sum_program:
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    persist var feed : FEED_MINIBATCH)
    var input_ids : LOD_TENSOR.shape(4, 2048).dtype(int64).stop_gradient(True)
    var labels : LOD_TENSOR.shape(4, 2048).dtype(int64).stop_gradient(True)
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(8, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var range_0.tmp_0 : LOD_TENSOR.shape(2048,).dtype(int64).stop_gradient(True)
    var expand_0.tmp_0 : LOD_TENSOR.shape(8, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(8, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, 8, 2048).dtype(bool).stop_gradient(True)
    var tmp_0 : LOD_TENSOR.shape(8, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var expand_1.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_13.tmp_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var tril_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_1 : LOD_TENSOR.shape(0, 2048, 2048).dtype(bool).stop_gradient(True)
    var expand_2.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var bitwise_and_0.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var fill_constant_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_2.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var cast_0.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_1.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var elementwise_add_1 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_2 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_3 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_4 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_2.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var where_0.tmp_0 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var tmp_1 : LOD_TENSOR.shape(8, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var split@RESHARD.tmp_0 : LOD_TENSOR.shape(4, 2048).dtype(int64).stop_gradient(False)
    var split@RESHARD.tmp_1 : LOD_TENSOR.shape(4, 2048).dtype(int64).stop_gradient(False)
    var unsqueeze2_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_2.tmp_1 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_4.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_4.tmp_1 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_0 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_6 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(4, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_2 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_3 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, 4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_15 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_4 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_5 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_6.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_6.tmp_1 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_0 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_8.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_8.tmp_1 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_0 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_28 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(4, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, 4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_37 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_42 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_3.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)

    {Out=['input_ids']} = feed(inputs={X=['feed']}, col = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['labels']} = feed(inputs={X=['feed']}, col = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 1, op original_id: 1, process_mesh (annotated): {shape: [2,2,1], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['embedding_0.tmp_0']} = lookup_table_v2(inputs={Ids=['input_ids'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, with_quant_attr = False, dist_attr = {op type: lookup_table_v2, op id: 2, op original_id: 2, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; input_ids's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; embedding_0.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; embedding_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 3, op original_id: 3, process_mesh (annotated): {shape: [2,2,1], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [8, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_9.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 2048, value = 2048.0, with_quant_attr = False)
    {Out=['fill_constant_11.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1, value = 1.0, with_quant_attr = False)
    {Out=['range_0.tmp_0']} = range(inputs={End=['fill_constant_9.tmp_0'], Start=['fill_constant_7.tmp_0'], Step=['fill_constant_11.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_0.tmp_0']} = expand_v2(inputs={Shape=[], X=['range_0.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [8, 2048], with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XOut=['fill_constant_5.tmp_0']} = share_buffer(inputs={X=['fill_constant_5.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['fill_constant_5.tmp_0']}, axes = [1, 2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_0']} = cast(inputs={X=['unsqueeze2_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['expand_1.tmp_0']} = expand_v2(inputs={Shape=[], X=['tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [8, 1, 2048, 2048], with_quant_attr = False)
    {Out=['fill_constant_13.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2048, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['tril_0']} = tril_triu(inputs={X=['fill_constant_13.tmp_0']}, diagonal = 0, lower = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_1.tmp_0'], XOut=['tril_0']} = share_buffer(inputs={X=['tril_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_1.tmp_0'], XShape=['unsqueeze2_1.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['tril_0']}, axes = [0, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_2.tmp_0']} = expand_v2(inputs={Shape=[], X=['unsqueeze2_1.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [8, 1, 2048, 2048], with_quant_attr = False)
    {Out=['bitwise_and_0.tmp_0'], XOut=['expand_1.tmp_0']} = share_buffer(inputs={X=['expand_1.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['bitwise_and_0.tmp_0']} = bitwise_and(inputs={X=['expand_1.tmp_0'], Y=['expand_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['fill_constant_15.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_17.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = -3.4028234663852886e+38, value = -3.4028234663852886e+38, with_quant_attr = False)
    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['fill_constant_15.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_1.tmp_0']} = fill_any_like(inputs={X=['fill_constant_17.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_2.tmp_0']} = fill_any_like(inputs={X=['bitwise_and_0.tmp_0']}, dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['cast_0.tmp_0']} = cast(inputs={X=['full_like_2.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['cast_1.tmp_0']} = cast(inputs={X=['bitwise_and_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['elementwise_add_0'], XOut=['full_like_0.tmp_0']} = share_buffer(inputs={X=['full_like_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['elementwise_add_0']} = elementwise_add(inputs={X=['full_like_0.tmp_0'], Y=['full_like_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_1'], XOut=['elementwise_add_0']} = share_buffer(inputs={X=['elementwise_add_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['elementwise_add_1']} = elementwise_add(inputs={X=['elementwise_add_0'], Y=['cast_0.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_2'], XOut=['fill_constant_15.tmp_0']} = share_buffer(inputs={X=['fill_constant_15.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['elementwise_add_2']} = elementwise_add(inputs={X=['fill_constant_15.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_3'], XOut=['fill_constant_17.tmp_0']} = share_buffer(inputs={X=['fill_constant_17.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['elementwise_add_3']} = elementwise_add(inputs={X=['fill_constant_17.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_4'], XOut=['cast_1.tmp_0']} = share_buffer(inputs={X=['cast_1.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['elementwise_add_4']} = elementwise_add(inputs={X=['cast_1.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['elementwise_add_4']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['where_0.tmp_0'], XOut=['elementwise_add_2']} = share_buffer(inputs={X=['elementwise_add_2']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['where_0.tmp_0']} = where(inputs={Condition=['cast_2.tmp_0'], X=['elementwise_add_2'], Y=['elementwise_add_3']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1']} = cast(inputs={X=['where_0.tmp_0']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_2']} = cast(inputs={X=['embedding_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 33, op original_id: 33, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_2's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_0.tmp_0'], XOut=['tmp_2']} = share_buffer(inputs={X=['tmp_2']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['pow_0.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 34, op original_id: 34, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_2's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['pow_0.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 35, op original_id: 35, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_3'], XOut=['mean_0.tmp_0']} = share_buffer(inputs={X=['mean_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_3']} = scale(inputs={ScaleTensor=[], X=['mean_0.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 36, op original_id: 36, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_3's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_0.tmp_0'], XOut=['tmp_3']} = share_buffer(inputs={X=['tmp_3']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['rsqrt_0.tmp_0']} = rsqrt(inputs={X=['tmp_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 37, op original_id: 37, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_3's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_4']} = elementwise_mul(inputs={X=['rsqrt_0.tmp_0'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 38, op original_id: 38, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_4's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_5']} = elementwise_mul(inputs={X=['tmp_4'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 39, op original_id: 39, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_4's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_0.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_5's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_0.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 40, op original_id: 40, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_0.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0'], XOut=['linear_0.tmp_0']} = share_buffer(inputs={X=['linear_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 41, op original_id: 41, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_1.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 42, op original_id: 42, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_1.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0'], XOut=['linear_1.tmp_0']} = share_buffer(inputs={X=['linear_1.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 43, op original_id: 43, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_2.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 44, op original_id: 44, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_2.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_2.tmp_0'], XOut=['linear_2.tmp_0']} = share_buffer(inputs={X=['linear_2.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 45, op original_id: 45, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_0.tmp_0'], XShape=['squeeze_0.tmp_1']} = squeeze2(inputs={X=['eager_tmp_1']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 46, op original_id: 46, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_1.tmp_0'], XShape=['squeeze_1.tmp_1']} = squeeze2(inputs={X=['eager_tmp_2']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 47, op original_id: 47, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_2's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['split@RESHARD.tmp_0', 'split@RESHARD.tmp_1']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['expand_0.tmp_0']}, axis = 0, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['unsqueeze2_2.tmp_0'], XShape=['unsqueeze2_2.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 48, op original_id: 48, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_0.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_2.tmp_0'], X=['squeeze_0.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 49, op original_id: 49, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_3.tmp_0'], XOut=['gather_nd_0.tmp_0']} = share_buffer(inputs={X=['gather_nd_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_3.tmp_0'], XShape=['unsqueeze2_3.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_0.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 50, op original_id: 50, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_4.tmp_0'], XShape=['unsqueeze2_4.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 51, op original_id: 51, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_1.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_4.tmp_0'], X=['squeeze_1.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 52, op original_id: 52, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_5.tmp_0'], XOut=['gather_nd_1.tmp_0']} = share_buffer(inputs={X=['gather_nd_1.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_5.tmp_0'], XShape=['unsqueeze2_5.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_1.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 53, op original_id: 53, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_6']} = elementwise_mul(inputs={X=['reshape2_0.tmp_0'], Y=['unsqueeze2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 54, op original_id: 54, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_6's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 55, op original_id: 55, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 56, op original_id: 56, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_7'], XOut=['reshape2_0.tmp_0_slice_1']} = share_buffer(inputs={X=['reshape2_0.tmp_0_slice_1']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_7']} = scale(inputs={ScaleTensor=[], X=['reshape2_0.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 57, op original_id: 57, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_7's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_0.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_7', 'reshape2_0.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 58, op original_id: 58, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_7's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_8']} = elementwise_mul(inputs={X=['concat_0.tmp_0'], Y=['unsqueeze2_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 59, op original_id: 59, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_8's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_9'], XOut=['tmp_6']} = share_buffer(inputs={X=['tmp_6']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_9']} = elementwise_add(inputs={X=['tmp_6'], Y=['tmp_8']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 60, op original_id: 60, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_6's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_8's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_9's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_10']} = elementwise_mul(inputs={X=['reshape2_1.tmp_0'], Y=['unsqueeze2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 61, op original_id: 61, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_10's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 62, op original_id: 62, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 63, op original_id: 63, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_11'], XOut=['reshape2_1.tmp_0_slice_1']} = share_buffer(inputs={X=['reshape2_1.tmp_0_slice_1']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_11']} = scale(inputs={ScaleTensor=[], X=['reshape2_1.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 64, op original_id: 64, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_11's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_1.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_11', 'reshape2_1.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 65, op original_id: 65, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_11's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_12']} = elementwise_mul(inputs={X=['concat_1.tmp_0'], Y=['unsqueeze2_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 66, op original_id: 66, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_12's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_13'], XOut=['tmp_10']} = share_buffer(inputs={X=['tmp_10']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_13']} = elementwise_add(inputs={X=['tmp_10'], Y=['tmp_12']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 67, op original_id: 67, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_10's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_12's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_13's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['tmp_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 68, op original_id: 68, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_9's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['tmp_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 69, op original_id: 69, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_13's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 70, op original_id: 70, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_14'], XOut=['transpose_0.tmp_0']} = share_buffer(inputs={X=['transpose_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_14']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 71, op original_id: 71, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_14's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['transpose_1.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 72, op original_id: 72, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['tmp_14'], Y=['transpose_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 73, op original_id: 73, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_14's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['split@RESHARD.tmp_2', 'split@RESHARD.tmp_3']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['tmp_1']}, axis = 0, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [4, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 74, op original_id: 74, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_15'], XOut=['matmul_v2_0.tmp_0']} = share_buffer(inputs={X=['matmul_v2_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_15']} = elementwise_add(inputs={X=['matmul_v2_0.tmp_0'], Y=['reshape2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 75, op original_id: 75, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_15's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_0.tmp_0']} = cast(inputs={X=['tmp_15']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 76, op original_id: 76, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_15's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_0.tmp_1'], XOut=['softmax_0.tmp_0']} = share_buffer(inputs={X=['softmax_0.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['softmax_0.tmp_1']} = softmax(inputs={X=['softmax_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 77, op original_id: 77, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_16']} = cast(inputs={X=['softmax_0.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 78, op original_id: 78, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_0.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_16's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['tmp_16'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 79, op original_id: 79, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_16's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 80, op original_id: 80, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_4.tmp_0'], XOut=['transpose_4.tmp_0']} = share_buffer(inputs={X=['transpose_4.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_4.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [4, 2048, 4096], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 81, op original_id: 81, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_3.tmp_0']} = matmul_v2(inputs={X=['reshape2_4.tmp_0'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 82, op original_id: 82, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_3.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_17']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 83, op original_id: 83, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_17's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_18']} = cast(inputs={X=['tmp_17']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 84, op original_id: 84, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_18's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_1.tmp_0'], XOut=['tmp_18']} = share_buffer(inputs={X=['tmp_18']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['pow_1.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_18']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 85, op original_id: 85, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_18's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_1.tmp_0']} = reduce_mean(inputs={X=['pow_1.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 86, op original_id: 86, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_19'], XOut=['mean_1.tmp_0']} = share_buffer(inputs={X=['mean_1.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_19']} = scale(inputs={ScaleTensor=[], X=['mean_1.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 87, op original_id: 87, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_19's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_1.tmp_0'], XOut=['tmp_19']} = share_buffer(inputs={X=['tmp_19']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['rsqrt_1.tmp_0']} = rsqrt(inputs={X=['tmp_19']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 88, op original_id: 88, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_19's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_20']} = elementwise_mul(inputs={X=['rsqrt_1.tmp_0'], Y=['tmp_17']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 89, op original_id: 89, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_20's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_21']} = elementwise_mul(inputs={X=['tmp_20'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 90, op original_id: 90, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_20's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_1.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_21's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_4.tmp_0']} = matmul_v2(inputs={X=['tmp_21'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 91, op original_id: 91, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_21's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_4.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_0.tmp_0']} = silu(inputs={X=['linear_4.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 92, op original_id: 92, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_5.tmp_0']} = matmul_v2(inputs={X=['tmp_21'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 93, op original_id: 93, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_21's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_5.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_22']} = elementwise_mul(inputs={X=['silu_0.tmp_0'], Y=['linear_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 94, op original_id: 94, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; silu_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_22's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_6.tmp_0']} = matmul_v2(inputs={X=['tmp_22'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 95, op original_id: 95, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_22's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_6.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_23'], XOut=['tmp_17']} = share_buffer(inputs={X=['tmp_17']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_23']} = elementwise_add(inputs={X=['tmp_17'], Y=['linear_6.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 96, op original_id: 96, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_23's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_24']} = cast(inputs={X=['tmp_23']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 97, op original_id: 97, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_24's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_2.tmp_0'], XOut=['tmp_24']} = share_buffer(inputs={X=['tmp_24']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['pow_2.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_24']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 98, op original_id: 98, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_24's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_2.tmp_0']} = reduce_mean(inputs={X=['pow_2.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 99, op original_id: 99, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_25'], XOut=['mean_2.tmp_0']} = share_buffer(inputs={X=['mean_2.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_25']} = scale(inputs={ScaleTensor=[], X=['mean_2.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 100, op original_id: 100, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_25's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_2.tmp_0'], XOut=['tmp_25']} = share_buffer(inputs={X=['tmp_25']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['rsqrt_2.tmp_0']} = rsqrt(inputs={X=['tmp_25']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 101, op original_id: 101, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_25's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_26']} = elementwise_mul(inputs={X=['rsqrt_2.tmp_0'], Y=['tmp_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 102, op original_id: 102, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_26's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_27']} = elementwise_mul(inputs={X=['tmp_26'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 103, op original_id: 103, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_26's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_2.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_27's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_7.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 104, op original_id: 104, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_7.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0'], XOut=['linear_7.tmp_0']} = share_buffer(inputs={X=['linear_7.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 105, op original_id: 105, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_8.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 106, op original_id: 106, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_8.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0'], XOut=['linear_8.tmp_0']} = share_buffer(inputs={X=['linear_8.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 107, op original_id: 107, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_9.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 108, op original_id: 108, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_9.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_7.tmp_0'], XOut=['linear_9.tmp_0']} = share_buffer(inputs={X=['linear_9.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 109, op original_id: 109, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_2.tmp_0'], XShape=['squeeze_2.tmp_1']} = squeeze2(inputs={X=['eager_tmp_4']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 110, op original_id: 110, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_4's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_3.tmp_0'], XShape=['squeeze_3.tmp_1']} = squeeze2(inputs={X=['eager_tmp_5']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 111, op original_id: 111, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_6.tmp_0'], XShape=['unsqueeze2_6.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 112, op original_id: 112, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_2.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_6.tmp_0'], X=['squeeze_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 113, op original_id: 113, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_7.tmp_0'], XOut=['gather_nd_2.tmp_0']} = share_buffer(inputs={X=['gather_nd_2.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_7.tmp_0'], XShape=['unsqueeze2_7.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_2.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 114, op original_id: 114, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_8.tmp_0'], XShape=['unsqueeze2_8.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 115, op original_id: 115, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_3.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_8.tmp_0'], X=['squeeze_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 116, op original_id: 116, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_9.tmp_0'], XOut=['gather_nd_3.tmp_0']} = share_buffer(inputs={X=['gather_nd_3.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_9.tmp_0'], XShape=['unsqueeze2_9.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_3.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 117, op original_id: 117, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_28']} = elementwise_mul(inputs={X=['reshape2_5.tmp_0'], Y=['unsqueeze2_7.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 118, op original_id: 118, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_28's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 119, op original_id: 119, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 120, op original_id: 120, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_29'], XOut=['reshape2_5.tmp_0_slice_1']} = share_buffer(inputs={X=['reshape2_5.tmp_0_slice_1']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_29']} = scale(inputs={ScaleTensor=[], X=['reshape2_5.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 121, op original_id: 121, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_29's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_2.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_29', 'reshape2_5.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 122, op original_id: 122, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_29's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_30']} = elementwise_mul(inputs={X=['concat_2.tmp_0'], Y=['unsqueeze2_9.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 123, op original_id: 123, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_30's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_31'], XOut=['tmp_28']} = share_buffer(inputs={X=['tmp_28']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_31']} = elementwise_add(inputs={X=['tmp_28'], Y=['tmp_30']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 124, op original_id: 124, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_28's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_30's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_31's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_32']} = elementwise_mul(inputs={X=['reshape2_6.tmp_0'], Y=['unsqueeze2_7.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 125, op original_id: 125, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_32's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 126, op original_id: 126, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 127, op original_id: 127, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_33'], XOut=['reshape2_6.tmp_0_slice_1']} = share_buffer(inputs={X=['reshape2_6.tmp_0_slice_1']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_33']} = scale(inputs={ScaleTensor=[], X=['reshape2_6.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 128, op original_id: 128, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_33's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_3.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_33', 'reshape2_6.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 129, op original_id: 129, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_33's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_34']} = elementwise_mul(inputs={X=['concat_3.tmp_0'], Y=['unsqueeze2_9.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 130, op original_id: 130, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_34's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_35'], XOut=['tmp_32']} = share_buffer(inputs={X=['tmp_32']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_35']} = elementwise_add(inputs={X=['tmp_32'], Y=['tmp_34']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 131, op original_id: 131, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_32's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_34's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_35's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['tmp_31']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 132, op original_id: 132, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_31's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['tmp_35']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 133, op original_id: 133, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_35's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['reshape2_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 134, op original_id: 134, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_36'], XOut=['transpose_5.tmp_0']} = share_buffer(inputs={X=['transpose_5.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_36']} = scale(inputs={ScaleTensor=[], X=['transpose_5.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 135, op original_id: 135, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_36's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['transpose_6.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 136, op original_id: 136, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['tmp_36'], Y=['transpose_8.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 137, op original_id: 137, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_36's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [4, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 138, op original_id: 138, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_37'], XOut=['matmul_v2_2.tmp_0']} = share_buffer(inputs={X=['matmul_v2_2.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_37']} = elementwise_add(inputs={X=['matmul_v2_2.tmp_0'], Y=['reshape2_8.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 139, op original_id: 139, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_37's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_1.tmp_0']} = cast(inputs={X=['tmp_37']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 140, op original_id: 140, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_37's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_1.tmp_1'], XOut=['softmax_1.tmp_0']} = share_buffer(inputs={X=['softmax_1.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['softmax_1.tmp_1']} = softmax(inputs={X=['softmax_1.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 141, op original_id: 141, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_38']} = cast(inputs={X=['softmax_1.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 142, op original_id: 142, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_1.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_38's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['tmp_38'], Y=['transpose_7.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 143, op original_id: 143, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_38's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['matmul_v2_3.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 144, op original_id: 144, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_9.tmp_0'], XOut=['transpose_9.tmp_0']} = share_buffer(inputs={X=['transpose_9.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [4, 2048, 4096], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 145, op original_id: 145, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_10.tmp_0']} = matmul_v2(inputs={X=['reshape2_9.tmp_0'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 146, op original_id: 146, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_10.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_39']} = elementwise_add(inputs={X=['tmp_23'], Y=['linear_10.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 147, op original_id: 147, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_39's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_40']} = cast(inputs={X=['tmp_39']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 148, op original_id: 148, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_40's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_3.tmp_0'], XOut=['tmp_40']} = share_buffer(inputs={X=['tmp_40']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['pow_3.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_40']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 149, op original_id: 149, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_40's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_3.tmp_0']} = reduce_mean(inputs={X=['pow_3.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 150, op original_id: 150, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_41'], XOut=['mean_3.tmp_0']} = share_buffer(inputs={X=['mean_3.tmp_0']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_41']} = scale(inputs={ScaleTensor=[], X=['mean_3.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 151, op original_id: 151, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_41's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_3.tmp_0'], XOut=['tmp_41']} = share_buffer(inputs={X=['tmp_41']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['rsqrt_3.tmp_0']} = rsqrt(inputs={X=['tmp_41']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 152, op original_id: 152, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_41's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_42']} = elementwise_mul(inputs={X=['rsqrt_3.tmp_0'], Y=['tmp_39']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 153, op original_id: 153, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_42's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_43']} = elementwise_mul(inputs={X=['tmp_42'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 154, op original_id: 154, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_42's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_3.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_43's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_11.tmp_0']} = matmul_v2(inputs={X=['tmp_43'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 155, op original_id: 155, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_43's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_11.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_1.tmp_0']} = silu(inputs={X=['linear_11.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 156, op original_id: 156, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_12.tmp_0']} = matmul_v2(inputs={X=['tmp_43'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 157, op original_id: 157, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_43's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_12.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_44']} = elementwise_mul(inputs={X=['silu_1.tmp_0'], Y=['linear_12.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 158, op original_id: 158, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; silu_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_44's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_13.tmp_0']} = matmul_v2(inputs={X=['tmp_44'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 159, op original_id: 159, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_44's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_13.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_45'], XOut=['tmp_39']} = share_buffer(inputs={X=['tmp_39']}, op_role = 0, share_dims_and_dtype = [False])
    {Out=['tmp_45']} = elementwise_add(inputs={X=['tmp_39'], Y=['linear_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 160, op original_id: 160, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_45's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_45']} = c_sync_calc_stream(inputs={X=['tmp_45']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_45']}, dynamic_shape = False, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 32, use_calc_stream = False, with_quant_attr = False)
}

[2024-03-01 08:42:28,827] [    INFO] pipeline_pass_base.py:69 - sub_program type: backward, sum_program:
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var tmp_23 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_25.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_26.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_27.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_4 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_5 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var split@RESHARD.tmp_0 : LOD_TENSOR.shape(4, 2048).dtype(int64).stop_gradient(False)
    var unsqueeze2_6.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_2.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_8.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_3.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_28.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_29.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_30.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_31.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_32.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_33.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_34.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_35.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_36.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_2 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_37.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_38.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_41.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_42.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_3.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_43.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_44.subprog_2 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0.subprog_2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@recv_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RESHARD_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_44@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_13.w_0@GRAD : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    persist var linear_13.w_0@GRAD@MERGE : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_42@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_3.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var create_parameter_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_9.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_2.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_2.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_3.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_4.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_5.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_2.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_4.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_6.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_7.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_8.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_9.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_10.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_11.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_12.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_13.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_14.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_15.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_16.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_19.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_20.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_21.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_22.subprog_3 : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0.subprog_3 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    persist var linear_6.w_0@GRAD@MERGE : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_1.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(4, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_0.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD : LOD_TENSOR.shape(4, 2048, 4096).dtype(float16).stop_gradient(False)
    var input_ids : LOD_TENSOR.shape(4, 2048).dtype(int64).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    persist var embedding_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    persist var gradient_merge_step : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_k : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_zero : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_cond : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(True)

    {Out=['tmp_24.subprog_2']} = cast(inputs={X=['tmp_23']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_2.tmp_0.subprog_2']} = pow(inputs={FactorTensor=[], X=['tmp_24.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_2.tmp_0.subprog_2']} = reduce_mean(inputs={X=['pow_2.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_25.subprog_2'], XOut=['mean_2.tmp_0.subprog_2']} = share_buffer(inputs={X=['mean_2.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_25.subprog_2']} = scale(inputs={ScaleTensor=[], X=['mean_2.tmp_0.subprog_2']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_2.tmp_0.subprog_2'], XOut=['tmp_25.subprog_2']} = share_buffer(inputs={X=['tmp_25.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['rsqrt_2.tmp_0.subprog_2']} = rsqrt(inputs={X=['tmp_25.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_26.subprog_2']} = elementwise_mul(inputs={X=['rsqrt_2.tmp_0.subprog_2'], Y=['tmp_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_27.subprog_2']} = elementwise_mul(inputs={X=['tmp_26.subprog_2'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_7.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_5.tmp_0.subprog_2'], XOut=['linear_7.tmp_0.subprog_2']} = share_buffer(inputs={X=['linear_7.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_5.tmp_0.subprog_2'], XShape=['reshape2_5.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_8.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_6.tmp_0.subprog_2'], XOut=['linear_8.tmp_0.subprog_2']} = share_buffer(inputs={X=['linear_8.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_6.tmp_0.subprog_2'], XShape=['reshape2_6.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_7.tmp_0.subprog_2'], XOut=['linear_9.tmp_0.subprog_2']} = share_buffer(inputs={X=['linear_9.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_7.tmp_0.subprog_2'], XShape=['reshape2_7.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_2.tmp_0.subprog_2'], XShape=['squeeze_2.tmp_1.subprog_2']} = squeeze2(inputs={X=['eager_tmp_4']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_3.tmp_0.subprog_2'], XShape=['squeeze_3.tmp_1.subprog_2']} = squeeze2(inputs={X=['eager_tmp_5']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_6.tmp_0.subprog_2'], XShape=['unsqueeze2_6.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_2.tmp_0.subprog_2']} = gather_nd(inputs={Index=['unsqueeze2_6.tmp_0.subprog_2'], X=['squeeze_2.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_7.tmp_0.subprog_2'], XOut=['gather_nd_2.tmp_0.subprog_2']} = share_buffer(inputs={X=['gather_nd_2.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_7.tmp_0.subprog_2'], XShape=['unsqueeze2_7.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_2.tmp_0.subprog_2']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_8.tmp_0.subprog_2'], XShape=['unsqueeze2_8.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_3.tmp_0.subprog_2']} = gather_nd(inputs={Index=['unsqueeze2_8.tmp_0.subprog_2'], X=['squeeze_3.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_9.tmp_0.subprog_2'], XOut=['gather_nd_3.tmp_0.subprog_2']} = share_buffer(inputs={X=['gather_nd_3.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_9.tmp_0.subprog_2'], XShape=['unsqueeze2_9.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_3.tmp_0.subprog_2']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_28.subprog_2']} = elementwise_mul(inputs={X=['reshape2_5.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_0.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_1.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_29.subprog_2'], XOut=['reshape2_5.tmp_0_slice_1.subprog_2']} = share_buffer(inputs={X=['reshape2_5.tmp_0_slice_1.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_29.subprog_2']} = scale(inputs={ScaleTensor=[], X=['reshape2_5.tmp_0_slice_1.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_2.tmp_0.subprog_2']} = concat(inputs={AxisTensor=[], X=['tmp_29.subprog_2', 'reshape2_5.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_30.subprog_2']} = elementwise_mul(inputs={X=['concat_2.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_31.subprog_2'], XOut=['tmp_28.subprog_2']} = share_buffer(inputs={X=['tmp_28.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_31.subprog_2']} = elementwise_add(inputs={X=['tmp_28.subprog_2'], Y=['tmp_30.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_32.subprog_2']} = elementwise_mul(inputs={X=['reshape2_6.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_0.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_1.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_33.subprog_2'], XOut=['reshape2_6.tmp_0_slice_1.subprog_2']} = share_buffer(inputs={X=['reshape2_6.tmp_0_slice_1.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_33.subprog_2']} = scale(inputs={ScaleTensor=[], X=['reshape2_6.tmp_0_slice_1.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_3.tmp_0.subprog_2']} = concat(inputs={AxisTensor=[], X=['tmp_33.subprog_2', 'reshape2_6.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_34.subprog_2']} = elementwise_mul(inputs={X=['concat_3.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_35.subprog_2'], XOut=['tmp_32.subprog_2']} = share_buffer(inputs={X=['tmp_32.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_35.subprog_2']} = elementwise_add(inputs={X=['tmp_32.subprog_2'], Y=['tmp_34.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0.subprog_2'], XShape=['transpose_5.tmp_1.subprog_2']} = transpose2(inputs={X=['tmp_31.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_6.tmp_0.subprog_2'], XShape=['transpose_6.tmp_1.subprog_2']} = transpose2(inputs={X=['tmp_35.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_7.tmp_0.subprog_2'], XShape=['transpose_7.tmp_1.subprog_2']} = transpose2(inputs={X=['reshape2_7.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_36.subprog_2'], XOut=['transpose_5.tmp_0.subprog_2']} = share_buffer(inputs={X=['transpose_5.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_36.subprog_2']} = scale(inputs={ScaleTensor=[], X=['transpose_5.tmp_0.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_8.tmp_0.subprog_2'], XShape=['transpose_8.tmp_1.subprog_2']} = transpose2(inputs={X=['transpose_6.tmp_0.subprog_2']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_36.subprog_2'], Y=['transpose_8.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_8.tmp_0.subprog_2'], XShape=['reshape2_8.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [4, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_37.subprog_2'], XOut=['matmul_v2_2.tmp_0.subprog_2']} = share_buffer(inputs={X=['matmul_v2_2.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_37.subprog_2']} = elementwise_add(inputs={X=['matmul_v2_2.tmp_0.subprog_2'], Y=['reshape2_8.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_1.tmp_0.subprog_2']} = cast(inputs={X=['tmp_37.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_1.tmp_1.subprog_2'], XOut=['softmax_1.tmp_0.subprog_2']} = share_buffer(inputs={X=['softmax_1.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['softmax_1.tmp_1.subprog_2']} = softmax(inputs={X=['softmax_1.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_38.subprog_2']} = cast(inputs={X=['softmax_1.tmp_1.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_3.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_38.subprog_2'], Y=['transpose_7.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_9.tmp_0.subprog_2'], XShape=['transpose_9.tmp_1.subprog_2']} = transpose2(inputs={X=['matmul_v2_3.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_9.tmp_0.subprog_2'], XOut=['transpose_9.tmp_0.subprog_2']} = share_buffer(inputs={X=['transpose_9.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_9.tmp_0.subprog_2'], XShape=['reshape2_9.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_9.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [4, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_10.tmp_0.subprog_2']} = matmul_v2(inputs={X=['reshape2_9.tmp_0.subprog_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_39.subprog_2']} = elementwise_add(inputs={X=['tmp_23'], Y=['linear_10.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_40.subprog_2']} = cast(inputs={X=['tmp_39.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_3.tmp_0.subprog_2']} = pow(inputs={FactorTensor=[], X=['tmp_40.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_3.tmp_0.subprog_2']} = reduce_mean(inputs={X=['pow_3.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_41.subprog_2'], XOut=['mean_3.tmp_0.subprog_2']} = share_buffer(inputs={X=['mean_3.tmp_0.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_41.subprog_2']} = scale(inputs={ScaleTensor=[], X=['mean_3.tmp_0.subprog_2']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_3.tmp_0.subprog_2'], XOut=['tmp_41.subprog_2']} = share_buffer(inputs={X=['tmp_41.subprog_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['rsqrt_3.tmp_0.subprog_2']} = rsqrt(inputs={X=['tmp_41.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_42.subprog_2']} = elementwise_mul(inputs={X=['rsqrt_3.tmp_0.subprog_2'], Y=['tmp_39.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43.subprog_2']} = elementwise_mul(inputs={X=['tmp_42.subprog_2'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_11.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_43.subprog_2'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_1.tmp_0.subprog_2']} = silu(inputs={X=['linear_11.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_12.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_43.subprog_2'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_44.subprog_2']} = elementwise_mul(inputs={X=['silu_1.tmp_0.subprog_2'], Y=['linear_12.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_13.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_44.subprog_2'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_45@GRAD@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = False, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [4, 2048, 4096], peer = 0, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_45@GRAD@RESHARD_0'], XOut=['tmp_45@GRAD@recv_0']} = share_buffer(inputs={X=['tmp_45@GRAD@recv_0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_45@GRAD@RESHARD_0']} = assign(inputs={X=['tmp_45@GRAD@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_39@GRAD@RENAME@block0@0'], XOut=['tmp_45@GRAD@RESHARD_0']} = share_buffer(inputs={X=['tmp_45@GRAD@RESHARD_0']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_39@GRAD@RENAME@block0@0'], Y@GRAD=['linear_13.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_45@GRAD@RESHARD_0'], X=['tmp_39.subprog_2'], Y=['linear_13.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_44@GRAD'], Y@GRAD=['linear_13.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_13.tmp_0@GRAD'], X=['tmp_44.subprog_2'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_13.w_0@GRAD@MERGE'], Y=['linear_13.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['silu_1.tmp_0@GRAD'], Y@GRAD=['linear_12.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_44@GRAD'], X=['silu_1.tmp_0.subprog_2'], Y=['linear_12.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_43@GRAD@RENAME@block0@0'], Y@GRAD=['linear_12.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_12.tmp_0@GRAD'], X=['tmp_43.subprog_2'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_11.tmp_0@GRAD'], XOut=['silu_1.tmp_0@GRAD']} = share_buffer(inputs={X=['silu_1.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_11.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_1.tmp_0.subprog_2'], Out@GRAD=['silu_1.tmp_0@GRAD'], X=['linear_11.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_43@GRAD@RENAME@block0@1'], Y@GRAD=['linear_11.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_11.tmp_0@GRAD'], X=['tmp_43.subprog_2'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_11.w_0@GRAD@MERGE'], Y=['linear_11.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43@GRAD'], XOut=['tmp_43@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_43@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_43@GRAD']} = sum(inputs={X=['tmp_43@GRAD@RENAME@block0@0', 'tmp_43@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_42@GRAD'], Y@GRAD=['create_parameter_3.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_43@GRAD'], X=['tmp_42.subprog_2'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['create_parameter_3.w_0', 'create_parameter_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['create_parameter_3.w_0@GRAD@MERGE'], Y=['create_parameter_3.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['rsqrt_3.tmp_0@GRAD'], Y@GRAD=['tmp_39@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_42@GRAD'], X=['rsqrt_3.tmp_0.subprog_2'], Y=['tmp_39.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_41@GRAD'], XOut=['rsqrt_3.tmp_0@GRAD']} = share_buffer(inputs={X=['rsqrt_3.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_41@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_3.tmp_0.subprog_2'], Out@GRAD=['rsqrt_3.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_3.tmp_0@GRAD'], XOut=['tmp_41@GRAD']} = share_buffer(inputs={X=['tmp_41@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['mean_3.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_41@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_3.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_3.tmp_0@GRAD'], X=['pow_3.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_40@GRAD'], XOut=['pow_3.tmp_0@GRAD']} = share_buffer(inputs={X=['pow_3.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_40@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_3.tmp_0@GRAD'], X=['tmp_40.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_39@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_40@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_39@GRAD'], XOut=['tmp_39@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_39@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_39@GRAD']} = sum(inputs={X=['tmp_39@GRAD@RENAME@block0@0', 'tmp_39@GRAD@RENAME@block0@1', 'tmp_39@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_23@GRAD@RENAME@block0@0'], XOut=['tmp_39@GRAD']} = share_buffer(inputs={X=['tmp_39@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_23@GRAD@RENAME@block0@0'], Y@GRAD=['linear_10.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['tmp_23'], Y=['linear_10.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD'], Y@GRAD=['linear_10.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_10.tmp_0@GRAD'], X=['reshape2_9.tmp_0.subprog_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_9.tmp_0@GRAD'], XOut=['reshape2_9.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_9.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['transpose_9.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [8, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_38@GRAD'], Y@GRAD=['transpose_7.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['tmp_38.subprog_2'], Y=['transpose_7.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_1.tmp_1@GRAD']} = cast(inputs={X=['tmp_38@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_1.subprog_2'], Out@GRAD=['softmax_1.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_37@GRAD']} = cast(inputs={X=['softmax_1.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0@GRAD'], XOut=['tmp_37@GRAD']} = share_buffer(inputs={X=['tmp_37@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['matmul_v2_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['matmul_v2_2.tmp_0.subprog_2'], Y=['reshape2_8.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_36@GRAD'], Y@GRAD=['transpose_8.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['tmp_36.subprog_2'], Y=['transpose_8.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1.subprog_2']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0@GRAD'], XOut=['tmp_36@GRAD']} = share_buffer(inputs={X=['tmp_36@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['transpose_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_36@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_35@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_31@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_32@GRAD'], XOut=['tmp_35@GRAD']} = share_buffer(inputs={X=['tmp_35@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_32@GRAD'], Y@GRAD=['tmp_34@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_35@GRAD'], X=['tmp_32.subprog_2'], Y=['tmp_34.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_34@GRAD'], X=['concat_3.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_33@GRAD', 'reshape2_6.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_3.tmp_0@GRAD'], X=['tmp_33.subprog_2', 'reshape2_6.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_1@GRAD'], XOut=['tmp_33@GRAD']} = share_buffer(inputs={X=['tmp_33@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_6.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_33@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], Out@GRAD=['reshape2_6.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], Out@GRAD=['reshape2_6.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['reshape2_6.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_28@GRAD'], XOut=['tmp_31@GRAD']} = share_buffer(inputs={X=['tmp_31@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_28@GRAD'], Y@GRAD=['tmp_30@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_31@GRAD'], X=['tmp_28.subprog_2'], Y=['tmp_30.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['concat_2.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_29@GRAD', 'reshape2_5.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_2.tmp_0@GRAD'], X=['tmp_29.subprog_2', 'reshape2_5.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_1@GRAD'], XOut=['tmp_29@GRAD']} = share_buffer(inputs={X=['tmp_29@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_5.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_29@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], Out@GRAD=['reshape2_5.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], Out@GRAD=['reshape2_5.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['reshape2_5.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_9.tmp_0@GRAD'], XOut=['reshape2_7.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_7.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_9.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@0'], Y@GRAD=['linear_9.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_9.tmp_0@GRAD'], X=['tmp_27.subprog_2'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_9.w_0@GRAD@MERGE'], Y=['linear_9.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0@GRAD'], XOut=['reshape2_6.tmp_0@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['reshape2_6.tmp_0@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_6.tmp_0@GRAD']} = sum(inputs={X=['reshape2_6.tmp_0@GRAD@RENAME@block0@0', 'reshape2_6.tmp_0@GRAD@RENAME@block0@1', 'reshape2_6.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_8.tmp_0@GRAD'], XOut=['reshape2_6.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_6.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_8.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@1'], Y@GRAD=['linear_8.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_8.tmp_0@GRAD'], X=['tmp_27.subprog_2'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_5.tmp_0@GRAD'], XOut=['reshape2_5.tmp_0@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['reshape2_5.tmp_0@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_5.tmp_0@GRAD']} = sum(inputs={X=['reshape2_5.tmp_0@GRAD@RENAME@block0@0', 'reshape2_5.tmp_0@GRAD@RENAME@block0@1', 'reshape2_5.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_7.tmp_0@GRAD'], XOut=['reshape2_5.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_5.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@2'], Y@GRAD=['linear_7.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_7.tmp_0@GRAD'], X=['tmp_27.subprog_2'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_27@GRAD'], XOut=['tmp_27@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_27@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_27@GRAD']} = sum(inputs={X=['tmp_27@GRAD@RENAME@block0@0', 'tmp_27@GRAD@RENAME@block0@1', 'tmp_27@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_26@GRAD'], Y@GRAD=['create_parameter_2.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['tmp_26.subprog_2'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['create_parameter_2.w_0', 'create_parameter_2.w_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['rsqrt_2.tmp_0@GRAD'], Y@GRAD=['tmp_23@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_26@GRAD'], X=['rsqrt_2.tmp_0.subprog_2'], Y=['tmp_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_25@GRAD'], XOut=['rsqrt_2.tmp_0@GRAD']} = share_buffer(inputs={X=['rsqrt_2.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_25@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_2.tmp_0.subprog_2'], Out@GRAD=['rsqrt_2.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_2.tmp_0@GRAD'], XOut=['tmp_25@GRAD']} = share_buffer(inputs={X=['tmp_25@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['mean_2.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_25@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_2.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_2.tmp_0@GRAD'], X=['pow_2.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_24@GRAD'], XOut=['pow_2.tmp_0@GRAD']} = share_buffer(inputs={X=['pow_2.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_24@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_2.tmp_0@GRAD'], X=['tmp_24.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_23@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_24@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_2.subprog_3']} = cast(inputs={X=['embedding_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_0.tmp_0.subprog_3']} = pow(inputs={FactorTensor=[], X=['tmp_2.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0.subprog_3']} = reduce_mean(inputs={X=['pow_0.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_3.subprog_3'], XOut=['mean_0.tmp_0.subprog_3']} = share_buffer(inputs={X=['mean_0.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_3.subprog_3']} = scale(inputs={ScaleTensor=[], X=['mean_0.tmp_0.subprog_3']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_0.tmp_0.subprog_3'], XOut=['tmp_3.subprog_3']} = share_buffer(inputs={X=['tmp_3.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['rsqrt_0.tmp_0.subprog_3']} = rsqrt(inputs={X=['tmp_3.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_4.subprog_3']} = elementwise_mul(inputs={X=['rsqrt_0.tmp_0.subprog_3'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_5.subprog_3']} = elementwise_mul(inputs={X=['tmp_4.subprog_3'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_0.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_0.tmp_0.subprog_3'], XOut=['linear_0.tmp_0.subprog_3']} = share_buffer(inputs={X=['linear_0.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_0.tmp_0.subprog_3'], XShape=['reshape2_0.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_1.tmp_0.subprog_3'], XOut=['linear_1.tmp_0.subprog_3']} = share_buffer(inputs={X=['linear_1.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_1.tmp_0.subprog_3'], XShape=['reshape2_1.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_2.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_2.tmp_0.subprog_3'], XOut=['linear_2.tmp_0.subprog_3']} = share_buffer(inputs={X=['linear_2.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_2.tmp_0.subprog_3'], XShape=['reshape2_2.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_0.tmp_0.subprog_3'], XShape=['squeeze_0.tmp_1.subprog_3']} = squeeze2(inputs={X=['eager_tmp_1']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_1.tmp_0.subprog_3'], XShape=['squeeze_1.tmp_1.subprog_3']} = squeeze2(inputs={X=['eager_tmp_2']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_2.tmp_0.subprog_3'], XShape=['unsqueeze2_2.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_0.tmp_0.subprog_3']} = gather_nd(inputs={Index=['unsqueeze2_2.tmp_0.subprog_3'], X=['squeeze_0.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_3.tmp_0.subprog_3'], XOut=['gather_nd_0.tmp_0.subprog_3']} = share_buffer(inputs={X=['gather_nd_0.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_3.tmp_0.subprog_3'], XShape=['unsqueeze2_3.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_0.tmp_0.subprog_3']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_4.tmp_0.subprog_3'], XOut=['split@RESHARD.tmp_0']} = share_buffer(inputs={X=['split@RESHARD.tmp_0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_4.tmp_0.subprog_3'], XShape=['unsqueeze2_4.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_1.tmp_0.subprog_3']} = gather_nd(inputs={Index=['unsqueeze2_4.tmp_0.subprog_3'], X=['squeeze_1.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_5.tmp_0.subprog_3'], XOut=['gather_nd_1.tmp_0.subprog_3']} = share_buffer(inputs={X=['gather_nd_1.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['unsqueeze2_5.tmp_0.subprog_3'], XShape=['unsqueeze2_5.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_1.tmp_0.subprog_3']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_6.subprog_3']} = elementwise_mul(inputs={X=['reshape2_0.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_0.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_1.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_7.subprog_3'], XOut=['reshape2_0.tmp_0_slice_1.subprog_3']} = share_buffer(inputs={X=['reshape2_0.tmp_0_slice_1.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_7.subprog_3']} = scale(inputs={ScaleTensor=[], X=['reshape2_0.tmp_0_slice_1.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_0.tmp_0.subprog_3']} = concat(inputs={AxisTensor=[], X=['tmp_7.subprog_3', 'reshape2_0.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_8.subprog_3']} = elementwise_mul(inputs={X=['concat_0.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_9.subprog_3'], XOut=['tmp_6.subprog_3']} = share_buffer(inputs={X=['tmp_6.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_9.subprog_3']} = elementwise_add(inputs={X=['tmp_6.subprog_3'], Y=['tmp_8.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_10.subprog_3']} = elementwise_mul(inputs={X=['reshape2_1.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_0.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_1.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_11.subprog_3'], XOut=['reshape2_1.tmp_0_slice_1.subprog_3']} = share_buffer(inputs={X=['reshape2_1.tmp_0_slice_1.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_11.subprog_3']} = scale(inputs={ScaleTensor=[], X=['reshape2_1.tmp_0_slice_1.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_1.tmp_0.subprog_3']} = concat(inputs={AxisTensor=[], X=['tmp_11.subprog_3', 'reshape2_1.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_12.subprog_3']} = elementwise_mul(inputs={X=['concat_1.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_13.subprog_3'], XOut=['tmp_10.subprog_3']} = share_buffer(inputs={X=['tmp_10.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_13.subprog_3']} = elementwise_add(inputs={X=['tmp_10.subprog_3'], Y=['tmp_12.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0.subprog_3'], XShape=['transpose_0.tmp_1.subprog_3']} = transpose2(inputs={X=['tmp_9.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_1.tmp_0.subprog_3'], XShape=['transpose_1.tmp_1.subprog_3']} = transpose2(inputs={X=['tmp_13.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_2.tmp_0.subprog_3'], XShape=['transpose_2.tmp_1.subprog_3']} = transpose2(inputs={X=['reshape2_2.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_14.subprog_3'], XOut=['transpose_0.tmp_0.subprog_3']} = share_buffer(inputs={X=['transpose_0.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_14.subprog_3']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_3.tmp_0.subprog_3'], XShape=['transpose_3.tmp_1.subprog_3']} = transpose2(inputs={X=['transpose_1.tmp_0.subprog_3']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_14.subprog_3'], Y=['transpose_3.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_3.tmp_0.subprog_3'], XOut=['split@RESHARD.tmp_2']} = share_buffer(inputs={X=['split@RESHARD.tmp_2']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_3.tmp_0.subprog_3'], XShape=['reshape2_3.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [4, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_15.subprog_3'], XOut=['matmul_v2_0.tmp_0.subprog_3']} = share_buffer(inputs={X=['matmul_v2_0.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_15.subprog_3']} = elementwise_add(inputs={X=['matmul_v2_0.tmp_0.subprog_3'], Y=['reshape2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_0.tmp_0.subprog_3']} = cast(inputs={X=['tmp_15.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_0.tmp_1.subprog_3'], XOut=['softmax_0.tmp_0.subprog_3']} = share_buffer(inputs={X=['softmax_0.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['softmax_0.tmp_1.subprog_3']} = softmax(inputs={X=['softmax_0.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_16.subprog_3']} = cast(inputs={X=['softmax_0.tmp_1.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_1.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_16.subprog_3'], Y=['transpose_2.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_4.tmp_0.subprog_3'], XShape=['transpose_4.tmp_1.subprog_3']} = transpose2(inputs={X=['matmul_v2_1.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_4.tmp_0.subprog_3'], XOut=['transpose_4.tmp_0.subprog_3']} = share_buffer(inputs={X=['transpose_4.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_4.tmp_0.subprog_3'], XShape=['reshape2_4.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_4.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [4, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_3.tmp_0.subprog_3']} = matmul_v2(inputs={X=['reshape2_4.tmp_0.subprog_3'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_17.subprog_3']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_18.subprog_3']} = cast(inputs={X=['tmp_17.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_1.tmp_0.subprog_3']} = pow(inputs={FactorTensor=[], X=['tmp_18.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_1.tmp_0.subprog_3']} = reduce_mean(inputs={X=['pow_1.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_19.subprog_3'], XOut=['mean_1.tmp_0.subprog_3']} = share_buffer(inputs={X=['mean_1.tmp_0.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_19.subprog_3']} = scale(inputs={ScaleTensor=[], X=['mean_1.tmp_0.subprog_3']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_1.tmp_0.subprog_3'], XOut=['tmp_19.subprog_3']} = share_buffer(inputs={X=['tmp_19.subprog_3']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['rsqrt_1.tmp_0.subprog_3']} = rsqrt(inputs={X=['tmp_19.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_20.subprog_3']} = elementwise_mul(inputs={X=['rsqrt_1.tmp_0.subprog_3'], Y=['tmp_17.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21.subprog_3']} = elementwise_mul(inputs={X=['tmp_20.subprog_3'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_4.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_21.subprog_3'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_0.tmp_0.subprog_3']} = silu(inputs={X=['linear_4.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_5.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_21.subprog_3'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_22.subprog_3']} = elementwise_mul(inputs={X=['silu_0.tmp_0.subprog_3'], Y=['linear_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_6.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_22.subprog_3'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_23@GRAD'], XOut=['tmp_23@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_23@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_23@GRAD']} = sum(inputs={X=['tmp_23@GRAD@RENAME@block0@0', 'tmp_23@GRAD@RENAME@block0@1', 'tmp_23@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_17@GRAD@RENAME@block0@0'], XOut=['tmp_23@GRAD']} = share_buffer(inputs={X=['tmp_23@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_17@GRAD@RENAME@block0@0'], Y@GRAD=['linear_6.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['tmp_17.subprog_3'], Y=['linear_6.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_22@GRAD'], Y@GRAD=['linear_6.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_6.tmp_0@GRAD'], X=['tmp_22.subprog_3'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_6.w_0@GRAD@MERGE'], Y=['linear_6.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['silu_0.tmp_0@GRAD'], Y@GRAD=['linear_5.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_22@GRAD'], X=['silu_0.tmp_0.subprog_3'], Y=['linear_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_21@GRAD@RENAME@block0@0'], Y@GRAD=['linear_5.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_5.tmp_0@GRAD'], X=['tmp_21.subprog_3'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_4.tmp_0@GRAD'], XOut=['silu_0.tmp_0@GRAD']} = share_buffer(inputs={X=['silu_0.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_4.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_0.tmp_0.subprog_3'], Out@GRAD=['silu_0.tmp_0@GRAD'], X=['linear_4.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_21@GRAD@RENAME@block0@1'], Y@GRAD=['linear_4.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_4.tmp_0@GRAD'], X=['tmp_21.subprog_3'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_21@GRAD'], XOut=['tmp_21@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_21@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_21@GRAD']} = sum(inputs={X=['tmp_21@GRAD@RENAME@block0@0', 'tmp_21@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_20@GRAD'], Y@GRAD=['create_parameter_1.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['tmp_20.subprog_3'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['rsqrt_1.tmp_0@GRAD'], Y@GRAD=['tmp_17@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['rsqrt_1.tmp_0.subprog_3'], Y=['tmp_17.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_19@GRAD'], XOut=['rsqrt_1.tmp_0@GRAD']} = share_buffer(inputs={X=['rsqrt_1.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_19@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_1.tmp_0.subprog_3'], Out@GRAD=['rsqrt_1.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_1.tmp_0@GRAD'], XOut=['tmp_19@GRAD']} = share_buffer(inputs={X=['tmp_19@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['mean_1.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_19@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_1.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_1.tmp_0@GRAD'], X=['pow_1.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_18@GRAD'], XOut=['pow_1.tmp_0@GRAD']} = share_buffer(inputs={X=['pow_1.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_18@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_1.tmp_0@GRAD'], X=['tmp_18.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_17@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_18@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_17@GRAD'], XOut=['tmp_17@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_17@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_17@GRAD']} = sum(inputs={X=['tmp_17@GRAD@RENAME@block0@0', 'tmp_17@GRAD@RENAME@block0@1', 'tmp_17@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD@RENAME@block0@0'], XOut=['tmp_17@GRAD']} = share_buffer(inputs={X=['tmp_17@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['embedding_0.tmp_0@GRAD@RENAME@block0@0'], Y@GRAD=['linear_3.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_17@GRAD'], X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD'], Y@GRAD=['linear_3.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_3.tmp_0@GRAD'], X=['reshape2_4.tmp_0.subprog_3'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_4.tmp_0@GRAD'], XOut=['reshape2_4.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_4.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['transpose_4.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [8, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_16@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['tmp_16.subprog_3'], Y=['transpose_2.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_0.tmp_1@GRAD']} = cast(inputs={X=['tmp_16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_1.subprog_3'], Out@GRAD=['softmax_0.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_15@GRAD']} = cast(inputs={X=['softmax_0.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0@GRAD'], XOut=['tmp_15@GRAD']} = share_buffer(inputs={X=['tmp_15@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['matmul_v2_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_15@GRAD'], X=['matmul_v2_0.tmp_0.subprog_3'], Y=['reshape2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_14@GRAD'], Y@GRAD=['transpose_3.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['tmp_14.subprog_3'], Y=['transpose_3.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1.subprog_3']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0@GRAD'], XOut=['tmp_14@GRAD']} = share_buffer(inputs={X=['tmp_14@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['transpose_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_14@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_13@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_9@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_10@GRAD'], XOut=['tmp_13@GRAD']} = share_buffer(inputs={X=['tmp_13@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_10@GRAD'], Y@GRAD=['tmp_12@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['tmp_10.subprog_3'], Y=['tmp_12.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['concat_1.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_11@GRAD', 'reshape2_1.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_1.tmp_0@GRAD'], X=['tmp_11.subprog_3', 'reshape2_1.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_1@GRAD'], XOut=['tmp_11@GRAD']} = share_buffer(inputs={X=['tmp_11@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_1.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_11@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], Out@GRAD=['reshape2_1.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], Out@GRAD=['reshape2_1.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['reshape2_1.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_6@GRAD'], XOut=['tmp_9@GRAD']} = share_buffer(inputs={X=['tmp_9@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_6@GRAD'], Y@GRAD=['tmp_8@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['tmp_6.subprog_3'], Y=['tmp_8.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['concat_0.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_7@GRAD', 'reshape2_0.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_0.tmp_0@GRAD'], X=['tmp_7.subprog_3', 'reshape2_0.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_1@GRAD'], XOut=['tmp_7@GRAD']} = share_buffer(inputs={X=['tmp_7@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_0.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_7@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], Out@GRAD=['reshape2_0.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], Out@GRAD=['reshape2_0.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['reshape2_0.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_2.tmp_0@GRAD'], XOut=['reshape2_2.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_2.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_2.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@0'], Y@GRAD=['linear_2.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_2.tmp_0@GRAD'], X=['tmp_5.subprog_3'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_1.tmp_0@GRAD'], XOut=['reshape2_1.tmp_0@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['reshape2_1.tmp_0@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_1.tmp_0@GRAD']} = sum(inputs={X=['reshape2_1.tmp_0@GRAD@RENAME@block0@0', 'reshape2_1.tmp_0@GRAD@RENAME@block0@1', 'reshape2_1.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_1.tmp_0@GRAD'], XOut=['reshape2_1.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_1.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_1.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@1'], Y@GRAD=['linear_1.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_1.tmp_0@GRAD'], X=['tmp_5.subprog_3'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_0.tmp_0@GRAD'], XOut=['reshape2_0.tmp_0@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['reshape2_0.tmp_0@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['reshape2_0.tmp_0@GRAD']} = sum(inputs={X=['reshape2_0.tmp_0@GRAD@RENAME@block0@0', 'reshape2_0.tmp_0@GRAD@RENAME@block0@1', 'reshape2_0.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_0.tmp_0@GRAD'], XOut=['reshape2_0.tmp_0@GRAD']} = share_buffer(inputs={X=['reshape2_0.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['linear_0.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@2'], Y@GRAD=['linear_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_0.tmp_0@GRAD'], X=['tmp_5.subprog_3'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_5@GRAD'], XOut=['tmp_5@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['tmp_5@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['tmp_5@GRAD']} = sum(inputs={X=['tmp_5@GRAD@RENAME@block0@0', 'tmp_5@GRAD@RENAME@block0@1', 'tmp_5@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_4@GRAD'], Y@GRAD=['create_parameter_0.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['tmp_4.subprog_3'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['create_parameter_0.w_0', 'create_parameter_0.w_0@GRAD'], with_quant_attr = False)
    {X@GRAD=['rsqrt_0.tmp_0@GRAD'], Y@GRAD=['embedding_0.tmp_0@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['rsqrt_0.tmp_0.subprog_3'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_3@GRAD'], XOut=['rsqrt_0.tmp_0@GRAD']} = share_buffer(inputs={X=['rsqrt_0.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_3@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_0.tmp_0.subprog_3'], Out@GRAD=['rsqrt_0.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0@GRAD'], XOut=['tmp_3@GRAD']} = share_buffer(inputs={X=['tmp_3@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['mean_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_3@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_0.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['pow_0.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_2@GRAD'], XOut=['pow_0.tmp_0@GRAD']} = share_buffer(inputs={X=['pow_0.tmp_0@GRAD']}, op_role = 1, share_dims_and_dtype = [False])
    {X@GRAD=['tmp_2@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_0.tmp_0@GRAD'], X=['tmp_2.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_2@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD'], XOut=['embedding_0.tmp_0@GRAD@RENAME@block0@0']} = share_buffer(inputs={X=['embedding_0.tmp_0@GRAD@RENAME@block0@0']}, op_role = 1, share_dims_and_dtype = [False])
    {Out=['embedding_0.tmp_0@GRAD']} = sum(inputs={X=['embedding_0.tmp_0@GRAD@RENAME@block0@0', 'embedding_0.tmp_0@GRAD@RENAME@block0@1', 'embedding_0.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {W@GRAD=['embedding_0.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['input_ids'], Out@GRAD=['embedding_0.tmp_0@GRAD'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], padding_idx = -1, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['embedding_0.w_0@GRAD@MERGE'], Y=['embedding_0.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gradient_merge_step']} = increment(inputs={X=['gradient_merge_step']}, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], step = 1.0, with_quant_attr = False)
    {Out=['gradient_merge_step']} = elementwise_mod(inputs={X=['gradient_merge_step'], Y=['gradient_merge_k']}, axis = -1, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gradient_merge_cond']} = equal(inputs={X=['gradient_merge_step'], Y=['gradient_merge_zero']}, axis = -1, force_cpu = False, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
}

[2024-03-01 08:42:28,845] [    INFO] pipeline_pass_base.py:69 - sub_program type: optimizer, sum_program:
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    persist var gradient_merge_cond : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(True)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var embedding_0.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(32000, 4096).dtype(float32).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(False)
    persist trainable param create_parameter_3.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_13.w_0@GRAD@MERGE : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_0.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_0 : LOD_TENSOR.shape(32000, 4096).dtype(float32).stop_gradient(True)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    persist var create_parameter_3.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var embedding_0.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var create_parameter_2.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(True)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_9.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(True)
    persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(True)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var linear_6.w_0@GRAD@MERGE : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var create_parameter_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(32000, 4096).dtype(float32).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var create_parameter_1.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var embedding_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var opt_opt_sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var opt_tmp_17 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_1 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var opt_opt_squared_l2_norm_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_tmp_8 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_opt_stack_0.tmp_0 : LOD_TENSOR.shape(39, 1).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var opt_tmp_15 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_tmp_19 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_13 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var opt_opt_squared_l2_norm_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var _generated_var_0 : STEP_SCOPES)

    {Out=['linear_12.w_0@GRAD', 'create_parameter_3.w_0_fp32_master_0_beta2_pow_acc_0', 'find_infinite_scale.@fp16_0@cast_int32', 'num_good_steps_0', 'num_bad_steps_0', 'linear_0.w_0', 'opt_opt_sum_0.tmp_0', 'linear_6.w_0_fp32_master_0_beta2_pow_acc_0', 'create_parameter_3.w_0', 'linear_13.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD', 'opt_tmp_17', 'create_parameter_0.w_0', 'linear_13.w_0_fp32_master_0_moment2_0', 'linear_6.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_6.w_0', 'embedding_0.w_0_fp32_master_0', 'loss_scaling_0', 'opt_tmp_0', 'linear_3.w_0@GRAD', 'linear_5.w_0@GRAD', 'opt_opt_sqrt_0.tmp_0', 'create_parameter_2.w_0@GRAD', 'linear_8.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_9.w_0_fp32_master_0', 'opt_opt_squared_l2_norm_12.tmp_0', 'create_parameter_2.w_0', 'opt_opt_squared_l2_norm_16.tmp_0', 'linear_6.w_0@GRAD@MERGE', 'linear_9.w_0_fp32_master_0_moment1_0', 'linear_9.w_0_fp32_master_0_moment2_0', 'linear_11.w_0@GRAD@MERGE', 'opt_tmp_1', 'linear_2.w_0', 'concat.tmp_0', 'linear_2.w_0@GRAD', 'linear_9.w_0', 'embedding_0.w_0', 'linear_11.w_0_fp32_master_0_moment1_0', 'create_parameter_3.w_0@GRAD@MERGE', 'embedding_0.w_0_fp32_master_0_moment2_0', 'linear_4.w_0@GRAD', 'linear_11.w_0', 'embedding_0.w_0@GRAD@MERGE', 'linear_4.w_0', 'linear_3.w_0', 'opt_opt_squared_l2_norm_14.tmp_0', 'opt_elementwise_div_0', 'opt_tmp_8', 'linear_11.w_0_fp32_master_0_moment2_0', 'linear_5.w_0', 'embedding_0.w_0_fp32_master_0_moment1_0', 'linear_10.w_0@GRAD', 'linear_6.w_0_fp32_master_0_moment2_0', 'opt_opt_fill_constant_1.tmp_0', 'opt_opt_stack_0.tmp_0', 'linear_1.w_0', 'linear_10.w_0', 'linear_9.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_7.tmp_0', 'linear_9.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_12.w_0', 'linear_7.w_0@GRAD', 'embedding_0.w_0_fp32_master_0_beta2_pow_acc_0', 'create_parameter_3.w_0_fp32_master_0_moment2_0', 'linear_13.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_6.w_0_fp32_master_0_moment1_0', 'create_parameter_3.w_0_fp32_master_0', 'embedding_0.w_0_fp32_master_0_beta1_pow_acc_0', 'find_infinite_scale.@fp16_0', 'linear_11.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_6.w_0_fp32_master_0', 'linear_7.w_0', 'linear_1.w_0@GRAD', 'opt_tmp_15', 'linear_9.w_0@GRAD@MERGE', 'linear_11.w_0_fp32_master_0', 'linear_13.w_0_fp32_master_0', 'opt_elementwise_max_0', 'linear_8.w_0', 'create_parameter_3.w_0_fp32_master_0_moment1_0', 'linear_11.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0_fp32_master_0_moment1_0', 'opt_tmp_19', 'opt_opt_squared_l2_norm_18.tmp_0', 'opt_tmp_13', 'memcopy__0', 'find_infinite_scale.tmp_0', 'create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD', 'linear_13.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_0.tmp_0', 'create_parameter_3.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0'], Scope=['_generated_var_0']} = conditional_block(inputs={Cond=['gradient_merge_cond'], Input=['linear_12.w_0@GRAD', 'create_parameter_3.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_11.w_0_fp32_master_0_moment2_0', 'linear_5.w_0', 'num_good_steps_0', 'num_bad_steps_0', 'embedding_0.w_0_fp32_master_0_moment1_0', 'linear_0.w_0', 'linear_10.w_0@GRAD', 'linear_6.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_6.w_0_fp32_master_0_moment2_0', 'create_parameter_3.w_0', 'linear_13.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD', 'create_parameter_0.w_0', 'linear_1.w_0', 'linear_10.w_0', 'linear_13.w_0_fp32_master_0_moment2_0', 'linear_9.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_6.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_9.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_6.w_0', 'linear_7.w_0@GRAD', 'linear_12.w_0', 'embedding_0.w_0_fp32_master_0_beta2_pow_acc_0', 'embedding_0.w_0_fp32_master_0', 'loss_scaling_0', 'create_parameter_3.w_0_fp32_master_0_moment2_0', 'linear_13.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_3.w_0@GRAD', 'linear_5.w_0@GRAD', 'linear_6.w_0_fp32_master_0_moment1_0', 'create_parameter_3.w_0_fp32_master_0', 'embedding_0.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_2.w_0@GRAD', 'linear_8.w_0@GRAD', 'linear_11.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_6.w_0_fp32_master_0', 'linear_7.w_0', 'linear_1.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_9.w_0_fp32_master_0', 'linear_9.w_0@GRAD@MERGE', 'linear_11.w_0_fp32_master_0', 'create_parameter_2.w_0', 'linear_13.w_0_fp32_master_0', 'learning_rate_0', 'linear_6.w_0@GRAD@MERGE', 'linear_9.w_0_fp32_master_0_moment1_0', 'linear_9.w_0_fp32_master_0_moment2_0', 'linear_8.w_0', 'linear_11.w_0@GRAD@MERGE', 'linear_2.w_0', 'create_parameter_3.w_0_fp32_master_0_moment1_0', 'linear_2.w_0@GRAD', 'linear_11.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0_fp32_master_0_moment1_0', 'linear_9.w_0', 'embedding_0.w_0', 'linear_11.w_0_fp32_master_0_moment1_0', 'create_parameter_3.w_0@GRAD@MERGE', 'create_parameter_1.w_0', 'embedding_0.w_0_fp32_master_0_moment2_0', 'linear_4.w_0@GRAD', 'create_parameter_1.w_0@GRAD', 'linear_11.w_0', 'embedding_0.w_0@GRAD@MERGE', 'linear_13.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_4.w_0', 'linear_3.w_0', 'create_parameter_3.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 2, op_role_var = [], sub_block = block[1], with_quant_attr = False)
}
{ // block_idx:1  parent_idx:0  forward_idx:-1  backward_idx:-1

    {Out=['embedding_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['embedding_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['create_parameter_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_13.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_13.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_12.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_12.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_11.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_11.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['create_parameter_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['create_parameter_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_10.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_10.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_9.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_9.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_8.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_8.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_7.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_7.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['create_parameter_2.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_2.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['linear_6.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_6.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_5.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_5.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_4.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_4.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['create_parameter_1.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_1.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_3.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_3.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_2.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_2.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_1.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_1.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['linear_0.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_0.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['create_parameter_0.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_0.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = c_allreduce_sum(inputs={Cond=[], X=['embedding_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['embedding_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, scale = 0.5, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['embedding_0.w_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'create_parameter_3.w_0@GRAD@MERGE']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['embedding_0.w_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'create_parameter_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, ring_id = 29, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['embedding_0.w_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'create_parameter_3.w_0@GRAD@MERGE'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['embedding_0.w_0@GRAD@MERGE', 'linear_6.w_0@GRAD@MERGE', 'linear_9.w_0@GRAD@MERGE', 'linear_11.w_0@GRAD@MERGE', 'linear_13.w_0@GRAD@MERGE', 'create_parameter_3.w_0@GRAD@MERGE']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_0.tmp_0']} = squared_l2_norm(inputs={X=['embedding_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_7.tmp_0']} = squared_l2_norm(inputs={X=['linear_6.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_12.tmp_0']} = squared_l2_norm(inputs={X=['linear_9.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_14.tmp_0']} = squared_l2_norm(inputs={X=['linear_11.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_16.tmp_0']} = squared_l2_norm(inputs={X=['linear_13.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_18.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Y=['opt_opt_stack_0.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_0.tmp_0', 'opt_opt_squared_l2_norm_7.tmp_0', 'opt_opt_squared_l2_norm_12.tmp_0', 'opt_opt_squared_l2_norm_14.tmp_0', 'opt_opt_squared_l2_norm_16.tmp_0', 'opt_opt_squared_l2_norm_18.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_sum_0.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_0.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_tmp_0']} = cast(inputs={X=['opt_opt_sum_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 5, with_quant_attr = False)
    {Out=['opt_tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['opt_tmp_0']}, op_device = , op_namescope = /auto_parallel/global_norm_synchronization, op_role = 2, ring_id = 0, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_1']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['embedding_0.w_0@GRAD@MERGE'], Y=['opt_tmp_1']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_8']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_6.w_0@GRAD@MERGE'], Y=['opt_tmp_8']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_13']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_9.w_0@GRAD@MERGE'], Y=['opt_tmp_13']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_15']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_11.w_0@GRAD@MERGE'], Y=['opt_tmp_15']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_17']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_13.w_0@GRAD@MERGE'], Y=['opt_tmp_17']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_19']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['create_parameter_3.w_0@GRAD@MERGE'], Y=['opt_tmp_19']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Beta1PowOut=['embedding_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['embedding_0.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['embedding_0.w_0_fp32_master_0'], Moment1Out=['embedding_0.w_0_fp32_master_0_moment1_0'], Moment2Out=['embedding_0.w_0_fp32_master_0_moment2_0'], ParamOut=['embedding_0.w_0']} = adamw(inputs={Beta1Pow=['embedding_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_0.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['embedding_0.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['embedding_0.w_0_fp32_master_0'], Moment1=['embedding_0.w_0_fp32_master_0_moment1_0'], Moment2=['embedding_0.w_0_fp32_master_0_moment2_0'], Param=['embedding_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_6.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_6.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_6.w_0_fp32_master_0'], Moment1Out=['linear_6.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_6.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_6.w_0']} = adamw(inputs={Beta1Pow=['linear_6.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_6.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_6.w_0_fp32_master_0'], Moment1=['linear_6.w_0_fp32_master_0_moment1_0'], Moment2=['linear_6.w_0_fp32_master_0_moment2_0'], Param=['linear_6.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_7/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_9.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_9.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_9.w_0_fp32_master_0'], Moment1Out=['linear_9.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_9.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_9.w_0']} = adamw(inputs={Beta1Pow=['linear_9.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_9.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_9.w_0_fp32_master_0'], Moment1=['linear_9.w_0_fp32_master_0_moment1_0'], Moment2=['linear_9.w_0_fp32_master_0_moment2_0'], Param=['linear_9.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_12/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_3.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_3.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['create_parameter_3.w_0_fp32_master_0'], Moment1Out=['create_parameter_3.w_0_fp32_master_0_moment1_0'], Moment2Out=['create_parameter_3.w_0_fp32_master_0_moment2_0'], ParamOut=['create_parameter_3.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_3.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_3.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_3.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['create_parameter_3.w_0_fp32_master_0'], Moment1=['create_parameter_3.w_0_fp32_master_0_moment1_0'], Moment2=['create_parameter_3.w_0_fp32_master_0_moment2_0'], Param=['create_parameter_3.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_18/, op_role = 2, use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_11.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_11.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_11.w_0_fp32_master_0'], Moment1Out=['linear_11.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_11.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_11.w_0']} = adamw(inputs={Beta1Pow=['linear_11.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_11.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_11.w_0_fp32_master_0'], Moment1=['linear_11.w_0_fp32_master_0_moment1_0'], Moment2=['linear_11.w_0_fp32_master_0_moment2_0'], Param=['linear_11.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_14/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_13.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_13.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_13.w_0_fp32_master_0'], Moment1Out=['linear_13.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_13.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_13.w_0']} = adamw(inputs={Beta1Pow=['linear_13.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_13.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_13.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_13.w_0_fp32_master_0'], Moment1=['linear_13.w_0_fp32_master_0_moment1_0'], Moment2=['linear_13.w_0_fp32_master_0_moment2_0'], Param=['linear_13.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_16/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Out=['embedding_0.w_0']} = c_broadcast(inputs={X=['embedding_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_0.w_0']} = c_broadcast(inputs={X=['create_parameter_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_0.w_0']} = c_broadcast(inputs={X=['linear_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_1.w_0']} = c_broadcast(inputs={X=['linear_1.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_2.w_0']} = c_broadcast(inputs={X=['linear_2.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_3.w_0']} = c_broadcast(inputs={X=['linear_3.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_1.w_0']} = c_broadcast(inputs={X=['create_parameter_1.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_4.w_0']} = c_broadcast(inputs={X=['linear_4.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_5.w_0']} = c_broadcast(inputs={X=['linear_5.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_6.w_0']} = c_broadcast(inputs={X=['linear_6.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_2.w_0']} = c_broadcast(inputs={X=['create_parameter_2.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_7.w_0']} = c_broadcast(inputs={X=['linear_7.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_8.w_0']} = c_broadcast(inputs={X=['linear_8.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_9.w_0']} = c_broadcast(inputs={X=['linear_9.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_10.w_0']} = c_broadcast(inputs={X=['linear_10.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_3.w_0']} = c_broadcast(inputs={X=['create_parameter_3.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_11.w_0']} = c_broadcast(inputs={X=['linear_11.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_12.w_0']} = c_broadcast(inputs={X=['linear_12.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_13.w_0']} = c_broadcast(inputs={X=['linear_13.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['embedding_0.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_6.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_9.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['create_parameter_3.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_11.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_13.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
}