grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
[33m[2024-02-22 15:20:47,317] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[33m[2024-02-22 15:20:47,318] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[32m[2024-02-22 15:20:47,318] [    INFO][0m - PP configs:{"enable":"True","schedule_mode":"VPP","vpp_degree":"1","vpp_seg_method":"","micro_batch_size":"1","accumulate_steps":"16","generation_batch_size":"1","enable_send_recv_overlap":"False","job_schedule_profiler_start":"-1","job_schedule_profiler_stop":"-1",}, use master_grad: True[0m
[33m[2024-02-22 15:20:47,318] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[2024-02-22 15:20:47,319] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_call_stack_level', current_value=2, default_value=1)
=======================================================================
I0222 15:20:47.320811 24064 tcp_utils.cc:107] Retry to connect to 172.17.0.3:52585 while the server is not yet listening.
I0222 15:20:50.321038 24064 tcp_utils.cc:130] Successfully connected to 172.17.0.3:52585
I0222 15:20:50.344223 24064 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0222 15:20:50.345055 24064 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-02-22 15:20:50,345] [    INFO] topology.py:358 - Total 2 pipe comm group(s) create successfully!
W0222 15:20:50.345986 24064 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.8
W0222 15:20:50.347456 24064 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
[2024-02-22 15:20:52,317] [    INFO] topology.py:358 - Total 4 data comm group(s) create successfully!
I0222 15:20:52.318032 24064 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-02-22 15:20:52,318] [    INFO] topology.py:358 - Total 2 model comm group(s) create successfully!
[2024-02-22 15:20:52,318] [    INFO] topology.py:358 - Total 4 sharding comm group(s) create successfully!
I0222 15:20:52.318629 24064 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0222 15:20:52.318722 24064 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-02-22 15:20:52,318] [    INFO] topology.py:288 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 1, pp_degree: 2, dp_degree: 1, sep_degree: 1, mp_group: [2, 3],  sharding_group: [3], pp_group: [1, 3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-02-22 15:20:52,321] [    INFO][0m - {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 1024.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': [], 'custom_black_list': [], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': True}","sharding":"{'enable': False, 'stage': 1, 'degree': 8, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': False, 'k_steps': 1, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': 'VPP', 'vpp_degree': 1, 'vpp_seg_method': '', 'micro_batch_size': 1, 'accumulate_steps': 16, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': False}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': True}","sp_optimization":"{'enable': False}",}[0m
/home/workspace/PaddleNLP/llm/llama/auto_parallel/run_pretrain_auto_static.py:429: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only 
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(random_seed)
[32m[2024-02-22 15:20:52,324] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 142.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_call_stack_level', current_value=2, default_value=1)
=======================================================================
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - ============================================================[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - paddle commit id              : 557b888e2c07631a347a1f8b0e06d3cec3250eb1[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - paddlenlp commit id           : edd3d74fc091d1b9ad0df9607336c8fd4d28bb90.dirty[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - config_name                   : None[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - continue_training             : False[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - fuse_attention_ffn            : False[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - fuse_attention_qkv            : False[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - hidden_size                   : None[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - intermediate_size             : None[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - model_name_or_path            : facebook/llama-7b[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - model_type                    : llama[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - num_attention_heads           : None[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - num_hidden_layers             : None[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - use_flash_attention           : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - virtual_pp_degree             : 2[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - vocab_size                    : None[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - [0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - ============================================================[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - paddle commit id              : 557b888e2c07631a347a1f8b0e06d3cec3250eb1[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - paddlenlp commit id           : edd3d74fc091d1b9ad0df9607336c8fd4d28bb90.dirty[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - data_cache                    : None[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - data_impl                     : mmap[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - input_dir                     : ../data[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - share_folder                  : False[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - skip_warmup                   : True[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - split                         : 949,50,1[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - [0m
[33m[2024-02-22 15:20:52,331] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-02-22 15:20:52,331] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-02-22 15:20:52,332] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-02-22 15:20:52,346] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-02-22 15:20:52,346] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-02-22 15:20:52,508] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-02-22 15:20:52,511] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-02-22 15:20:52,512] [    INFO][0m - Reset vocab size to 32000 for batter amp peformance.[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 4,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "seq_length": 2048,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": true,
  "virtual_pp_degree": 2,
  "vocab_size": 32000
}

Using old dataet (.npy & .npz)
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-02-22 15:20:53,361] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-02-22 15:20:53,383] [    INFO][0m - . How?? Well thats the magic!! Its a bit complicated to explain here. Get the book and read Chapter 9. But it works! Check this out:Amazing isn't it? And the even more amazing thing is that it is not specific to factorial. See thisWoohoo!! On a roll now..You can take any recursive function, and rewrite it in the above style and the Y combinator will make a recursive version of it. How cool is that?</s> Perception.

Computational models of facial attractiveness judgments.

Strikingly, both models produced estimates of facial attractiveness that were indistinguishable from human ratings.

The image factors that the model discovered correspond to two of the main contemporary hypotheses of averageness judgments: facial attractiveness and sexual dimorphism.

This provides novel evidence for the importance of averageness and sexual dimorphism, but not symmetry, in human judgments of facial attractiveness.

Very related: Computer taught to recognize female attractiveness 2008;37(1):126-42.Related ArticlesBronstad PM, Langlois JH, Russell R.We designed two computational models to replicate human facial attractiveness ratings. The primary model used partial least squares (PLS) to identify image factors associated with facial attractiveness from facial images and attractiveness ratings of those images. For comparison we also made a model similar to previous models of facial attractiveness, in that it used manually derived measurements between features as inputs, though we took the additional step of dimensionality reduction via principal component analysis (PCA) and weighting of PCA dimensions via a perceptron.Because PLS extracts a small number of image factors from the facial images that covary with attractiveness ratings of the images, it is possible to determine the information used by the model.In contrast, facial symmetry was not important to the model, and an explicit feature-based measurement of symmetry was not correlated with human judgments of facial attractiveness.</s> The news comes after the Telegraph launched a campaign calling for a fair deal for motorists. This follows a 50 per cent increase in the amount of car-related tax since Labour came to power.</s> Mengenal Gaya Bermain Poker Online Pada Pemain Profesional

Mengenal Gaya Bermain Poker Online Pada Pemain Profesional – Ada beberapa hal yang harus anda ketahui ketika anda bermain poker online ini. Salah satunya adalah mengenal gaya bermain poker online pada setiap pemain yang melakukan taruhan ini. Gaya bermain ini sering dikatakan sebagai playing styles, dimana terdapat banyak sekali gaya yang harus anda ketahui. Berikut […]</s>U.S. troops working the streets of the capital fear one Iraqi weapon more than others -- a copper-plated explosive that can penetrate armor and has proved devastating to Humvees and even capable of severely damaging tanks.

The power of what the military calls an EFP -- for explosively formed penetrator, or projectile -- to spray molten metal balls that punch through the armor on vehicles has some American troops rethinking their tactics. They are asking whether the U.S. should give up its reliance on making constant improvements to vehicle defenses.

Instead, these troops think, it is time to leave the armor behind -- and get out and walk.

“In our area, the biggest threat for us is EFPs. When you are in the vehicles, you are a big target,” said Army Staff Sgt. Cavin Moskwa, 33, of Hawaii, who patrols Baghdad’s Zafraniya neighborhood with the Bravo Battery of the 2nd Battalion, 17th Field Artillery Regiment. “But when you are dismounted... you are a lot safer.”

Advertisement

In the last three days, 15 U.S. troops have been killed in Iraq, nine of them in two powerful roadside bomb blasts. The military does not publicly identify the kind of weapon used in improvised explosive attacks, but the deadly nature of the blasts Wednesday and Thursday suggested that EFPs may have been used.

The deaths brought to 3,545 the total number of U.S. troops killed in the Iraq theater since the March 2003 American-led invasion, the U.S. military said. Hundreds of these troops have been killed by EFPs and other kinds of improvised explosive devices, or IEDs. The Pentagon’s most recent Iraq status report said EFP attacks were at an all-time high.

Foot patrols, of course, are not a fail-safe method. On city streets, snipers remain a threat. And bombs can still kill dismounted troops. But when blasts occur in the middle of a foot patrol, the number of casualties are generally lower because the troops are more spread out.

Before a foot patrol last week through a neighborhood next to Baghdad’s Sadr City district, a private with Alpha Company of the Army’s 1st Battalion, 8th Cavalry Regiment, began complaining about having to walk. But EFPs have claimed the lives of several soldiers in the unit, and Sgt. Leland Kidd, 28, of Gonzales, Texas, said the private should be thankful they were on foot.

Advertisement

“When I walk on my feet, I don’t have to worry about being blown up,” Kidd told the private. “In the vehicle, I have to.”

Top commanders have been encouraging more such units in Baghdad to take just that tack.

A counterinsurgency guidance memo released last week by Army Lt. Gen Raymond T. Odierno, the commander of day-to-day military operations, urges Iraqi and American troops to “get out and walk.”

The memo argues that although Humvees offer protection, they also make units predictable and “insulate us from the Iraqi people we intend to secure.”

Advertisement

The original draft of the memo, written by counterinsurgency expert David Kilcullen, goes further. It notes that EFP attacks on Humvees damage them heavily. “So we gain little in safety, but sacrifice much in effectiveness,” the draft reads.

One reason for the increased number of troops victimized by roadside bombs is that there are more forces in Iraq now, Marine Gen. Peter Pace, chairman of the Joint Chiefs of Staff, said at a Pentagon news conference Thursday. This month, the final additional American combat units arrived in Baghdad, as part of a counterinsurgency strategy announced by President Bush in January that has increased the U.S. military presence in Iraq by 28,500 troops.

“As we’re taking the fight to the enemy with the additional troops, we can expect that there’s going to be tough fighting ahead,” Pace said. “So it is an expectation that this surge is going to result in more contact and therefore more casualties.”

But another reason for the rising death toll is the ability of Iraq’s militants to adapt to new U.S. military tactics.

Advertisement

During the 2003 invasion, most American Humvees were outfitted with flimsy canvas doors. When the first improvised explosive devices made from artillery shells appeared, the military scrambled to put stronger armor on the vehicles. Since then, the military has repeatedly upgraded Humvee armor as militants have made bigger and bigger bombs.

But the small and easily hidden EFPs, which often are powered by C-4 plastic explosives, are not just a more powerful IED. Military personnel experienced with the projectiles say that what makes the weapons so deadly is that they use the Americans’ own armor against them. As the hot copper slug melts through the armor of a Humvee, it transforms the protective plating into shrapnel that sprays into the passenger cabin, they say.

“We joked about going back to canvas doors. That way, unless it hits you directly, you are OK,” said Army Sgt. William Bowman, 31, of Fort Myers, Fla.

But to Moskwa, the staff sergeant from Hawaii, the question of armor is no joke. Moskwa, who served as an Army recruiter in Pasadena before deploying to Iraq, thinks armor on vehicles and body armor on troops are too restrictive, hampering a service member’s ability to move quickly and agilely.

Advertisement

“I would rather go out without any armor or gear,” he said. “If an EFP hits the vehicle, you are dead anyway no[0m
[32m[2024-02-22 15:20:53,383] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-02-22 15:20:53,404] [    INFO][0m - intolerant of large, polluting industrial plants on their doorsteps. Second, American power companies are fearful that they will soon have to pay for one particular pollutant, carbon dioxide, as is starting to happen in other parts of the rich world. Having invested heavily in gas-fired stations, only to find themselves locked into an increasingly expensive fuel, they do not want to make another mistake.

That has opened up a capacity gap and an opportunity for wind and sunlight. The future price of these resources—zero—is known. That certainty has economic value as a hedge, even if the capital cost of wind and solar power stations is, at the moment, higher than that of coal-fired ones.

The reasons for the boom, then, are tangled, and the way they are perceived may change. Global warming, a long-range phenomenon, may not be uppermost in people's minds during an economic downturn. High fuel prices may fall as new sources of supply are exploited to fill rising demand from Asia. Security of supply may improve if hostile governments are replaced by friendly ones and sources become more diversified. But none of the reasons is likely to go away entirely.

Global warming certainly will not. “Peak oil”, if oil means the traditional sort that comes cheaply out of holes in the ground, probably will arrive soon. There is oil aplenty of other sorts (tar sands, liquefied coal and so on), so the stuff is unlikely to run out for a long time yet. But it will get more expensive to produce, putting a floor on the price that is way above today's. And political risk will always be there—particularly for oil, which is so often associated with bad government for the simple reason that its very presence causes bad government in states that do not have strong institutions to curb their politicians.

A prize beyond the dreams of avarice

The market for energy is huge. At present, the world's population consumes about 15 terawatts of power. (A terawatt is 1,000 gigawatts, and a gigawatt is the capacity of the largest sort of coal-fired power station.) That translates into a business worth $6 trillion a year—about a tenth of the world's economic output—according to John Doerr, a venture capitalist who is heavily involved in the industry. And by 2050, power consumption is likely to have risen to 30 terawatts.

Scale is one of the important differences between the coming energy boom, if it materialises, and its recent predecessors—particularly those that relied on information technology, a market measured in mere hundreds of billions. Another difference is that new information technologies tend to be disruptive, forcing the replacement of existing equipment, whereas, say, building wind farms does not force the closure of coal-fired power stations.

For both of these reasons, any transition from an economy based on fossil fuels to one based on renewable, alternative, green energy—call it what you will—is likely to be slow, as similar changes have been in the past (see chart 1). On the other hand, the scale of the market provides opportunities for alternatives to prove themselves at the margin and then move into the mainstream, as is happening with wind power at the moment. And some energy technologies do have the potential to be disruptive. Plug-in cars, for example, could be fuelled with electricity at a price equivalent to 25 cents a litre of petrol. That could shake up the oil, carmaking and electricity industries all in one go.

The innovation lull of the past few decades also provides opportunities for technological leapfrogging. Indeed, it may be that the field of energy gives the not-quite-booms in biotechnology and nanotechnology the industrial applications they need to grow really big, and that the three aspiring booms will thus merge into one.

The possibility of thus recapturing the good times of their youth has brought many well-known members of the “technorati” out of their homes in places like Woodside, California. Energy has become supercool. Elon Musk, who co-founded PayPal, has developed a battery-powered sports car. Larry Page and Sergey Brin, the founders of Google, have started an outfit called Google.org that is searching for a way to make renewable energy truly cheaper than coal (or RE

Vinod Khosla, one of the founders of Sun Microsystems, is turning his considerable skills as a venture capitalist towards renewable energy, as are Robert Metcalfe, who invented the ethernet system used to connect computers together in local networks, and Mr Doerr, who works at Kleiner Perkins Caufield & Byers, one of Silicon Valley's best-known venture-capital firms. Sir Richard Branson, too, is getting in on the act with his Virgin Green Fund.

This renewed interest in energy is bringing forth a raft of ideas, some bright, some batty, that is indeed reminiscent of the dotcom boom. As happened in that boom, most of these ideas will come to naught. But there could just be a PayPal or a Google or a Sun among them.

More traditional companies are also taking an interest. General Electric (GE), a large American engineering firm, already has a thriving wind-turbine business and is gearing up its solar-energy business. The energy researchers at its laboratories in Schenectady, New York, enjoy much of the intellectual freedom associated with start-up firms, combined with a secure supply of money.

Meanwhile, BP and Shell, two of the world's biggest oil companies, are sponsoring both academic researchers and new, small firms with bright ideas, as is DuPont, one of the biggest chemical companies. Not everyone has joined in. Exxon Mobil, the world's largest oil company not in government hands, is conspicuously absent. But in many boardrooms renewables are no longer seen as just a way of keeping environmentalists off companies' backs.

Some people complain that many existing forms of renewable energy rely on subsidies or other forms of special treatment for their viability. On the surface, that is true. Look beneath, though, and the whole energy sector is riddled with subsidies, both explicit and hidden, and costs that are not properly accounted for. Drawing on the work of people like Boyden Gray, a former White House counsel, Mr Woolsey estimates that American oil companies receive preferential treatment from their government worth more than $250 billion a year. And the Intergovernmental Panel on Climate Change (IPCC), a United Nations-appointed group of scientific experts, reckons that fossil fuels should carry a tax of $20-50 for every tonne of carbon dioxide they generate in order to pay for the environmental effects of burning them (hence the fears of the power-generators).

So the subsidies and mandates offered to renewable sources of power such as wind turbines often just level the playing field. It is true that some subsidies amount to unwarranted market-rigging: examples include those handed by cloudy Germany to its solar-power industry and by America to its maize-based ethanol farmers when Brazilian sugar-based ethanol is far cheaper. Others, though, such as a requirement that a certain proportion of electricity be derived from non-fossil-fuel sources, make no attempt to pick particular technological winners. They merely act to stimulate innovation by guaranteeing a market to things that actually work.

If the world were rational, all of these measures would be swept away and replaced by a proper tax on carbon—as is starting to happen in Europe, where the price arrived at by the cap-and-trade system being introduced is close to the IPCC's recommendation. If that occurred, wind-based electricity would already be competitive with fossil fuels and others would be coming close. Failing that, special treatment for alternatives is probably the least bad option—though such measures need to be crafted in ways that favour neither incumbents nor particular ways of doing things, and need to be withdrawn when they are no longer necessary.

The poor world turns greener too

That, at least, is the view from the rich world. But poorer, rapidly developing countries are also taking more of an interest in renewable energy sources, despite assertions to the contrary by some Western politicians and businessmen. It is true that China is building coal-fired power stations at a blazing rate. But it also has a large wind-generation capacity, which is expected to grow by two-thirds this year, and is the world's second-largest manufacturer of solar panels—not to mention having the largest number of solar-heated rooftop hot-water systems in its buildings.

Brazil, meanwhile, has the world's second-largest (just behind America) and most econom[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - ============================================================[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m -          Configuration Arguments        [0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - paddle commit id              : 557b888e2c07631a347a1f8b0e06d3cec3250eb1[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - paddlenlp commit id           : edd3d74fc091d1b9ad0df9607336c8fd4d28bb90.dirty[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-02-22 15:20:53,405] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - amp_master_grad               : True[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - bf16                          : False[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - current_device                : gpu:3[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-02-22 15:20:53,406] [    INFO][0m - dataloader_num_workers        : 1[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - decay_steps                   : 10[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - device                        : gpu[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - do_eval                       : True[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - do_export                     : False[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - do_predict                    : False[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - do_train                      : True[0m
[32m[2024-02-22 15:20:53,407] [    INFO][0m - enable_auto_parallel          : True[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - eval_batch_size               : 16[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - eval_steps                    : 1000[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - fp16                          : True[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-02-22 15:20:53,408] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - fused_linear_param_grad_add   : False[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - gradient_accumulation_steps   : 16[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - greater_is_better             : None[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - job_schedule_profiler_end     : -1[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - job_schedule_profiler_start   : -1[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - label_names                   : None[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-02-22 15:20:53,409] [    INFO][0m - learning_rate                 : 0.0001[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - log_level                     : -1[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - logging_dir                   : output/llama_auto_static_dp2sharding2mp2pp2_vpp2/runs/Feb22_15-20-47_c4d082bc6a0b[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-02-22 15:20:53,410] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - max_steps                     : 10[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-02-22 15:20:53,411] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - output_dir                    : output/llama_auto_static_dp2sharding2mp2pp2_vpp2[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - past_index                    : -1[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - per_device_eval_batch_size    : 16[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - per_device_train_batch_size   : 1[0m
[32m[2024-02-22 15:20:53,412] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - pipeline_parallel_degree      : 2[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - pipeline_parallel_rank        : 1[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - pipeline_schedule_mode        : VPP[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - power                         : 1.0[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - process_index                 : 3[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - recompute                     : True[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - refined_ops_patterns          : None[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-02-22 15:20:53,413] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - run_name                      : output/llama_auto_static_dp2sharding2mp2pp2_vpp2[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - save_steps                    : 5000[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - save_total_limit              : None[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - scale_loss                    : 1024.0[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - seed                          : 42[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - sharding                      : [][0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - should_log                    : True[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - should_save                   : True[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-02-22 15:20:53,415] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - sr                            : 0[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - strategy                      : {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 1024.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': [], 'custom_black_list': [], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': True}","sharding":"{'enable': False, 'stage': 1, 'degree': 8, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': False, 'k_steps': 1, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': 'VPP', 'vpp_degree': 2, 'vpp_seg_method': 'LlamaDecoderLayerAuto', 'micro_batch_size': 1, 'accumulate_steps': 16, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': False}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': True}","sp_optimization":"{'enable': False}",}[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - tensor_parallel_config        : enable_mp_async_allreduce[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - to_static                     : False[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - train_batch_size              : 1[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - virtual_pipeline_seg_method   : LlamaDecoderLayerAuto[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - wandb_api_key                 : None[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - warmup_ratio                  : 0.01[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - world_size                    : 4[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - [0m
[2024-02-22 15:20:53,419] [    INFO] cluster.py:914 - Node Count: 1, Local Device Size: 4, GPU Model: NVIDIA GeForce GTX 1080 Ti, GPU Memory: 11GB, World size: 4, EndPoint: 172.17.0.3:52589.
[2024-02-22 15:20:53,419] [    INFO] engine.py:202 - Distribute training by paddle.distributed.launch
[2024-02-22 15:20:53,420] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
[2024-02-22 15:20:53,420] [ WARNING] fleet.py:272 - The dygraph parallel environment has been initialized.
[2024-02-22 15:20:53,420] [ WARNING] fleet.py:295 - The dygraph hybrid parallel environment has been initialized.
[2024-02-22 15:20:53,420] [    INFO] engine.py:655 - Building model with 'to_static' method.
INFO 2024-02-22 15:20:53,421 helper.py:245] start to build program for mode = train.
/usr/local/lib/python3.9/dist-packages/paddle/base/framework.py:3123: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
/root/.cache/paddle/to_static_tmp/24064/LlamaPretrainingCriterionAuto_forward0bte5kdw.py:28: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 32000, 32000
  _jst.Call(_jst.Ld(_jst.Ld(warnings).warn))(f'enable_parallel_cross_entropy, the vocab_size should be splited: {_jst.Ld(_jst.Shape(_jst.Ld(prediction_scores))[-1])}, {_jst.Ld(_jst.Ld(_jst.Ld(self).config).vocab_size)}')
WARNING: there are some orphan tensors or ops which are not used in the execution.
Thu Feb 22 15:20:59-INFO: Using Auto VPP
Thu Feb 22 15:20:59-INFO: stage=[0], chunk_id=[0], layer_name=[['LlamaDecoderLayerAuto']]
Thu Feb 22 15:20:59-INFO: start op: [fill_constant]: [[]] [['fill_constant_1.tmp_0']]
Thu Feb 22 15:20:59-INFO: end op: [elementwise_add]: [['tmp_17', 'linear_6.tmp_0']] [['tmp_23']]
Thu Feb 22 15:20:59-INFO: stage=[1], chunk_id=[0], layer_name=[['LlamaDecoderLayerAuto_1']]
Thu Feb 22 15:20:59-INFO: start op: [cast]: [['tmp_23']] [['tmp_24']]
Thu Feb 22 15:20:59-INFO: end op: [elementwise_add]: [['tmp_39', 'linear_13.tmp_0']] [['tmp_45']]
Thu Feb 22 15:20:59-INFO: stage=[0], chunk_id=[1], layer_name=[['LlamaDecoderLayerAuto_2']]
Thu Feb 22 15:20:59-INFO: start op: [cast]: [['tmp_45']] [['tmp_46']]
Thu Feb 22 15:20:59-INFO: end op: [elementwise_add]: [['tmp_61', 'linear_20.tmp_0']] [['tmp_67']]
Thu Feb 22 15:20:59-INFO: stage=[1], chunk_id=[1], layer_name=[['LlamaDecoderLayerAuto_3']]
Thu Feb 22 15:20:59-INFO: start op: [cast]: [['tmp_67']] [['tmp_68']]
Thu Feb 22 15:20:59-INFO: end op: [reduce_mean]: [['tmp_97']] [['mean_9.tmp_0']]
[2024-02-22 15:20:59,395] [    INFO] parallelizer_v2.py:283 - Applying AMP-float16-o2 ...
[2024-02-22 15:20:59,637] [    INFO] auto_parallel_recompute.py:392 - The excluded ops in recompute segments are:
[[], [], []]
[2024-02-22 15:21:02,872] [ WARNING] parallelizer_v2.py:437 - You set mp_optimization.allreduce_matmul_grad_overlapping=True, but you did not set environment variable CUDA_DEVICE_MAX_CONNECTIONS=1, which may leads to performance loss. Try to export CUDA_DEVICE_MAX_CONNECTIONS=1 for better performance.
INFO 2024-02-22 15:21:02,876 allreduce_matmul_grad_overlapping.py:59] overlap matmul_grad and allreduce: OrderedDict([(196, 198), (277, 279), (281, 283), (325, 327), (330, 332), (335, 337), (419, 421), (423, 425), (467, 469), (472, 474), (477, 479)])
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var input_ids : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(1, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var range_0.tmp_0 : LOD_TENSOR.shape(2048,).dtype(int64).stop_gradient(True)
    var expand_0.tmp_0 : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(1, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(bool).stop_gradient(True)
    var tmp_0 : LOD_TENSOR.shape(1, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var expand_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_13.tmp_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var tril_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_1 : LOD_TENSOR.shape(0, 2048, 2048).dtype(bool).stop_gradient(True)
    var expand_2.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var bitwise_and_0.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var fill_constant_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_2.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var cast_0.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var elementwise_add_1 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_2 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_3 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_4 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_2.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var where_0.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var tmp_1 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_24 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist var eager_tmp_4 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_5 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_8.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_28 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_37 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_42 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_3.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_68 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_69 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_70 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_6.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_71 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_21.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_22.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_23.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist var eager_tmp_10 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_6.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_11 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_14.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_16.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_16.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_72 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_73 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_74 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_75 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_76 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_77 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_78 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_79 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_80 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_18.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_81 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_82 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_24.w_0 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_84 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_85 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_86 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_7.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_87 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_25.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_26.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_88 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_27.w_0 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_89 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_90 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_91 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_92 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_8.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_93 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param llama_lm_head_auto_0.w_0 : LOD_TENSOR.shape(4096, 16000).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 16000).dtype(float16).stop_gradient(False)
    var tmp_94 : LOD_TENSOR.shape(1, 2048, 16000).dtype(float16).stop_gradient(False)
    var labels : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_18.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_18.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var tmp_95 : LOD_TENSOR.shape().dtype(float16).stop_gradient(True)
    var tmp_96 : LOD_TENSOR.shape(1, 2048, 1).dtype(bool).stop_gradient(False)
    var masked_select_0.tmp_0 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var tmp_97 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var mean_9.tmp_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var mean_9.tmp_0.cast_fp32_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var scaled_loss_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var scaled_loss_1@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var mean_9.tmp_0.cast_fp32_0@GRAD_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var mean_9.tmp_0@GRAD : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var tmp_97@GRAD : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var masked_select_0.tmp_0@GRAD : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_94@GRAD : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16000).dtype(float16).stop_gradient(False)
    var tmp_93@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var llama_lm_head_auto_0.w_0@GRAD : LOD_TENSOR.shape(4096, 16000).dtype(float32).stop_gradient(False)
    var tmp_92@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_8.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_89@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_91@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_90@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_89@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_68.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_69.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_70.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_71.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var squeeze_6.tmp_0.subprog_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_6.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_0.subprog_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_14.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_14.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_16.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_16.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_72.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_1.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_73.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_74.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_75.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_76.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_1.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_77.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_78.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_79.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_80.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_18.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_81.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_82.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_84.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_85.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_86.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_87.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_3.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_88.subprog_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_89@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_88@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_27.w_0@GRAD : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    var silu_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_87@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_26.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var linear_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_87@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_25.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var tmp_87@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_86@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_7.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_85@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_84@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_24.w_0@GRAD : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    var transpose_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_82@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_1@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_81@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_80@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_79@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_75@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_76@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_78@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_77@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_72@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_74@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_73@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_23.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_22.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_21.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var tmp_71@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_70@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_6.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_69@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_68@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_25.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_26.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var squeeze_2.tmp_0.subprog_2 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_0.subprog_2 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_6.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_28.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_29.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_30.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_31.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_32.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_33.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_34.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_35.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_36.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_37.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_38.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_41.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_42.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_43.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_44.subprog_2 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_44@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_13.w_0@GRAD : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    var silu_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var linear_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var tmp_43@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_42@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_3.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_2.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var tmp_23@recv_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@RESHARD_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@recv_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@RESHARD_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var c_concat@RESHARD_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_0 : LOD_TENSOR.shape(1, 2048, 16000).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_1 : LOD_TENSOR.shape(1, 2048, 16000).dtype(float16).stop_gradient(False)
    var create_parameter_2.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_10.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_3.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_13.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_6.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_21.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_22.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_23.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_24.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_7.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_25.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_26.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_27.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_8.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 16000).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_19.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_stack_0.tmp_0 : LOD_TENSOR.shape(20, 1).dtype(float32).stop_gradient(False)
    var opt_opt_sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var learning_rate_1 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var linear_7.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_7.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_8.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_9.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_9.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_1 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(True)
    persist var linear_10.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_11.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_11.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_12.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_1 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(True)
    persist var linear_13.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_13.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_2.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_3.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_3.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_21.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_21.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_22.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_22.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_23.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_23.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_1 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(True)
    persist var linear_24.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_25.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_25.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_26.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_1 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(True)
    persist var linear_27.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_27.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_6.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_6.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_6.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_6.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_6.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_7.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_7.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_7.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_7.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_7.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_8.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 16000).dtype(float32).stop_gradient(True)
    persist var llama_lm_head_auto_0.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 16000).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 16000).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_27.subprog_2@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27.subprog_2@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var tmp_43.subprog_2@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_43.subprog_2@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var tmp_71.subprog_0@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_71.subprog_0@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_21.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_21.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_22.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_22.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_23.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_23.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var tmp_87.subprog_0@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_87.subprog_0@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_25.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_25.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_26.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_26.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var tmp_93@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_93@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 16000).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 16000).dtype(float16).stop_gradient(False)
    var llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 16000).dtype(float16).stop_gradient(False)
    var llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 16000).dtype(float16).stop_gradient(False)

    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 1, op original_id: 1, process_mesh (annotated): {shape: [1,2,2], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 3, op original_id: 3, process_mesh (annotated): {shape: [1,2,2], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_9.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 2048, value = 2048.0, with_quant_attr = False)
    {Out=['fill_constant_11.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1, value = 1.0, with_quant_attr = False)
    {Out=['range_0.tmp_0']} = range(inputs={End=['fill_constant_9.tmp_0'], Start=['fill_constant_7.tmp_0'], Step=['fill_constant_11.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_0.tmp_0']} = expand_v2(inputs={Shape=[], X=['range_0.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [1, 2048], with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['fill_constant_5.tmp_0']}, axes = [1, 2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_0']} = cast(inputs={X=['unsqueeze2_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['expand_1.tmp_0']} = expand_v2(inputs={Shape=[], X=['tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], with_quant_attr = False)
    {Out=['fill_constant_13.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2048, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['tril_0']} = tril_triu(inputs={X=['fill_constant_13.tmp_0']}, diagonal = 0, lower = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_1.tmp_0'], XShape=['unsqueeze2_1.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['tril_0']}, axes = [0, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_2.tmp_0']} = expand_v2(inputs={Shape=[], X=['unsqueeze2_1.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], with_quant_attr = False)
    {Out=['bitwise_and_0.tmp_0']} = bitwise_and(inputs={X=['expand_1.tmp_0'], Y=['expand_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['fill_constant_15.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_17.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = -3.4028234663852886e+38, value = -3.4028234663852886e+38, with_quant_attr = False)
    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['fill_constant_15.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_1.tmp_0']} = fill_any_like(inputs={X=['fill_constant_17.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_2.tmp_0']} = fill_any_like(inputs={X=['bitwise_and_0.tmp_0']}, dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['cast_0.tmp_0']} = cast(inputs={X=['full_like_2.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['cast_1.tmp_0']} = cast(inputs={X=['bitwise_and_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['elementwise_add_0']} = elementwise_add(inputs={X=['full_like_0.tmp_0'], Y=['full_like_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_1']} = elementwise_add(inputs={X=['elementwise_add_0'], Y=['cast_0.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_2']} = elementwise_add(inputs={X=['fill_constant_15.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_3']} = elementwise_add(inputs={X=['fill_constant_17.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_4']} = elementwise_add(inputs={X=['cast_1.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['elementwise_add_4']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['where_0.tmp_0']} = where(inputs={Condition=['cast_2.tmp_0'], X=['elementwise_add_2'], Y=['elementwise_add_3']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1']} = cast(inputs={X=['where_0.tmp_0']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_23@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_23@RESHARD_0']} = assign(inputs={X=['tmp_23@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_24']} = cast(inputs={X=['tmp_23@RESHARD_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 97, op original_id: 97, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_24's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_2.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_24']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 98, op original_id: 98, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_24's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_2.tmp_0']} = reduce_mean(inputs={X=['pow_2.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 99, op original_id: 99, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; pow_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_25']} = scale(inputs={ScaleTensor=[], X=['mean_2.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 100, op original_id: 100, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; mean_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_25's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_2.tmp_0']} = rsqrt(inputs={X=['tmp_25']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 101, op original_id: 101, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_25's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_26']} = elementwise_mul(inputs={X=['rsqrt_2.tmp_0'], Y=['tmp_23@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 102, op original_id: 102, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; rsqrt_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_26's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_27']} = elementwise_mul(inputs={X=['tmp_26'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 103, op original_id: 103, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_26's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_2.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_27's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_7.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 104, op original_id: 104, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_7.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 105, op original_id: 105, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_8.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 106, op original_id: 106, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_8.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 107, op original_id: 107, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_9.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 108, op original_id: 108, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_9.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 109, op original_id: 109, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_2.tmp_0'], XShape=['squeeze_2.tmp_1']} = squeeze2(inputs={X=['eager_tmp_4']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 110, op original_id: 110, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; eager_tmp_4's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_3.tmp_0'], XShape=['squeeze_3.tmp_1']} = squeeze2(inputs={X=['eager_tmp_5']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 111, op original_id: 111, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; eager_tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_6.tmp_0'], XShape=['unsqueeze2_6.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 112, op original_id: 112, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_2.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_6.tmp_0'], X=['squeeze_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 113, op original_id: 113, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; unsqueeze2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_7.tmp_0'], XShape=['unsqueeze2_7.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_2.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 114, op original_id: 114, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; gather_nd_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_8.tmp_0'], XShape=['unsqueeze2_8.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 115, op original_id: 115, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_3.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_8.tmp_0'], X=['squeeze_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 116, op original_id: 116, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; unsqueeze2_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_9.tmp_0'], XShape=['unsqueeze2_9.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_3.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 117, op original_id: 117, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; gather_nd_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_28']} = elementwise_mul(inputs={X=['reshape2_5.tmp_0'], Y=['unsqueeze2_7.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 118, op original_id: 118, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_28's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 119, op original_id: 119, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 120, op original_id: 120, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_29']} = scale(inputs={ScaleTensor=[], X=['reshape2_5.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 121, op original_id: 121, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_5.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_29's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_2.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_29', 'reshape2_5.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 122, op original_id: 122, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_29's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_30']} = elementwise_mul(inputs={X=['concat_2.tmp_0'], Y=['unsqueeze2_9.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 123, op original_id: 123, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; concat_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_30's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_31']} = elementwise_add(inputs={X=['tmp_28'], Y=['tmp_30']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 124, op original_id: 124, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_28's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_30's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_31's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_32']} = elementwise_mul(inputs={X=['reshape2_6.tmp_0'], Y=['unsqueeze2_7.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 125, op original_id: 125, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_32's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 126, op original_id: 126, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 127, op original_id: 127, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_33']} = scale(inputs={ScaleTensor=[], X=['reshape2_6.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 128, op original_id: 128, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_6.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_33's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_3.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_33', 'reshape2_6.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 129, op original_id: 129, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_33's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_34']} = elementwise_mul(inputs={X=['concat_3.tmp_0'], Y=['unsqueeze2_9.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 130, op original_id: 130, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; concat_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_34's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_35']} = elementwise_add(inputs={X=['tmp_32'], Y=['tmp_34']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 131, op original_id: 131, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_32's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_34's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_35's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['tmp_31']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 132, op original_id: 132, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_31's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['tmp_35']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 133, op original_id: 133, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_35's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['reshape2_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 134, op original_id: 134, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_36']} = scale(inputs={ScaleTensor=[], X=['transpose_5.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 135, op original_id: 135, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; transpose_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_36's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['transpose_6.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 136, op original_id: 136, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; transpose_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['tmp_36'], Y=['transpose_8.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 137, op original_id: 137, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_36's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 138, op original_id: 138, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_37']} = elementwise_add(inputs={X=['matmul_v2_2.tmp_0'], Y=['reshape2_8.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 139, op original_id: 139, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; matmul_v2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_37's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_1.tmp_0']} = cast(inputs={X=['tmp_37']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 140, op original_id: 140, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_37's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_1.tmp_1']} = softmax(inputs={X=['softmax_1.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 141, op original_id: 141, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; softmax_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_38']} = cast(inputs={X=['softmax_1.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 142, op original_id: 142, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; softmax_1.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_38's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['tmp_38'], Y=['transpose_7.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 143, op original_id: 143, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_38's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['matmul_v2_3.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 144, op original_id: 144, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; matmul_v2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 145, op original_id: 145, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; transpose_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_10.tmp_0']} = matmul_v2(inputs={X=['reshape2_9.tmp_0'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 146, op original_id: 146, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_10.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_10.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_10.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_39']} = elementwise_add(inputs={X=['tmp_23@RESHARD_0'], Y=['linear_10.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 147, op original_id: 147, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_39's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_40']} = cast(inputs={X=['tmp_39']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 148, op original_id: 148, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_40's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_3.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_40']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 149, op original_id: 149, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_40's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_3.tmp_0']} = reduce_mean(inputs={X=['pow_3.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 150, op original_id: 150, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; pow_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_41']} = scale(inputs={ScaleTensor=[], X=['mean_3.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 151, op original_id: 151, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; mean_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_41's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_3.tmp_0']} = rsqrt(inputs={X=['tmp_41']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 152, op original_id: 152, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_41's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_42']} = elementwise_mul(inputs={X=['rsqrt_3.tmp_0'], Y=['tmp_39']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 153, op original_id: 153, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; rsqrt_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_42's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_43']} = elementwise_mul(inputs={X=['tmp_42'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 154, op original_id: 154, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_42's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_3.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_43's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_11.tmp_0']} = matmul_v2(inputs={X=['tmp_43'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 155, op original_id: 155, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_43's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_11.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_1.tmp_0']} = silu(inputs={X=['linear_11.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 156, op original_id: 156, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_12.tmp_0']} = matmul_v2(inputs={X=['tmp_43'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 157, op original_id: 157, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_43's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_12.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_44']} = elementwise_mul(inputs={X=['silu_1.tmp_0'], Y=['linear_12.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 158, op original_id: 158, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; silu_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_44's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_13.tmp_0']} = matmul_v2(inputs={X=['tmp_44'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 159, op original_id: 159, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_44's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_13.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_13.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_13.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_45']} = elementwise_add(inputs={X=['tmp_39'], Y=['linear_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 160, op original_id: 160, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_45's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    send_v2(inputs={X=['tmp_45']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_67@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_67@RESHARD_0']} = assign(inputs={X=['tmp_67@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_68']} = cast(inputs={X=['tmp_67@RESHARD_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 225, op original_id: 225, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_67's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_68's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_6.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_68']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 226, op original_id: 226, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_68's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_6.tmp_0']} = reduce_mean(inputs={X=['pow_6.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 227, op original_id: 227, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; pow_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_69']} = scale(inputs={ScaleTensor=[], X=['mean_6.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 228, op original_id: 228, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; mean_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_69's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_6.tmp_0']} = rsqrt(inputs={X=['tmp_69']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 229, op original_id: 229, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_69's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_70']} = elementwise_mul(inputs={X=['rsqrt_6.tmp_0'], Y=['tmp_67@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 230, op original_id: 230, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; rsqrt_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_67's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_70's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_71']} = elementwise_mul(inputs={X=['tmp_70'], Y=['create_parameter_6.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 231, op original_id: 231, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_70's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_6.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_71's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_21.tmp_0']} = matmul_v2(inputs={X=['tmp_71'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 232, op original_id: 232, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_71's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_21.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_21.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_15.tmp_0'], XShape=['reshape2_15.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_21.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 233, op original_id: 233, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_21.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_22.tmp_0']} = matmul_v2(inputs={X=['tmp_71'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 234, op original_id: 234, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_71's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_22.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_22.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_16.tmp_0'], XShape=['reshape2_16.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_22.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 235, op original_id: 235, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_22.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_23.tmp_0']} = matmul_v2(inputs={X=['tmp_71'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 236, op original_id: 236, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_71's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_23.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_23.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_17.tmp_0'], XShape=['reshape2_17.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 237, op original_id: 237, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_23.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_17.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_6.tmp_0'], XShape=['squeeze_6.tmp_1']} = squeeze2(inputs={X=['eager_tmp_10']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 238, op original_id: 238, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; eager_tmp_10's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_7.tmp_0'], XShape=['squeeze_7.tmp_1']} = squeeze2(inputs={X=['eager_tmp_11']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 239, op original_id: 239, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; eager_tmp_11's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_14.tmp_0'], XShape=['unsqueeze2_14.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 240, op original_id: 240, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_14.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_6.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_14.tmp_0'], X=['squeeze_6.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 241, op original_id: 241, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; unsqueeze2_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_15.tmp_0'], XShape=['unsqueeze2_15.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_6.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 242, op original_id: 242, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; gather_nd_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_16.tmp_0'], XShape=['unsqueeze2_16.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 243, op original_id: 243, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_16.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_7.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_16.tmp_0'], X=['squeeze_7.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 244, op original_id: 244, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; unsqueeze2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_17.tmp_0'], XShape=['unsqueeze2_17.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_7.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 245, op original_id: 245, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; gather_nd_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_72']} = elementwise_mul(inputs={X=['reshape2_15.tmp_0'], Y=['unsqueeze2_15.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 246, op original_id: 246, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_72's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_15.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 247, op original_id: 247, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_15.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 248, op original_id: 248, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_73']} = scale(inputs={ScaleTensor=[], X=['reshape2_15.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 249, op original_id: 249, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_15.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_73's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_6.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_73', 'reshape2_15.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 250, op original_id: 250, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_73's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_74']} = elementwise_mul(inputs={X=['concat_6.tmp_0'], Y=['unsqueeze2_17.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 251, op original_id: 251, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; concat_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_74's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_75']} = elementwise_add(inputs={X=['tmp_72'], Y=['tmp_74']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 252, op original_id: 252, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_72's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_74's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_75's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_76']} = elementwise_mul(inputs={X=['reshape2_16.tmp_0'], Y=['unsqueeze2_15.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 253, op original_id: 253, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_76's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_16.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 254, op original_id: 254, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_16.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 255, op original_id: 255, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_77']} = scale(inputs={ScaleTensor=[], X=['reshape2_16.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 256, op original_id: 256, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_16.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_77's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_7.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_77', 'reshape2_16.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 257, op original_id: 257, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_77's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_78']} = elementwise_mul(inputs={X=['concat_7.tmp_0'], Y=['unsqueeze2_17.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 258, op original_id: 258, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; concat_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_78's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_79']} = elementwise_add(inputs={X=['tmp_76'], Y=['tmp_78']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 259, op original_id: 259, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_76's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_78's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_79's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_15.tmp_0'], XShape=['transpose_15.tmp_1']} = transpose2(inputs={X=['tmp_75']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 260, op original_id: 260, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_75's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_15.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_16.tmp_0'], XShape=['transpose_16.tmp_1']} = transpose2(inputs={X=['tmp_79']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 261, op original_id: 261, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_79's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_16.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_17.tmp_0'], XShape=['transpose_17.tmp_1']} = transpose2(inputs={X=['reshape2_17.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 262, op original_id: 262, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_17.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_80']} = scale(inputs={ScaleTensor=[], X=['transpose_15.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 263, op original_id: 263, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; transpose_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_80's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_18.tmp_0'], XShape=['transpose_18.tmp_1']} = transpose2(inputs={X=['transpose_16.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 264, op original_id: 264, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; transpose_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_18.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_18.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_6.tmp_0']} = matmul_v2(inputs={X=['tmp_80'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 265, op original_id: 265, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_80's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_18.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_18.tmp_0'], XShape=['reshape2_18.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 266, op original_id: 266, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_18.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_18.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_81']} = elementwise_add(inputs={X=['matmul_v2_6.tmp_0'], Y=['reshape2_18.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 267, op original_id: 267, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; matmul_v2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_18.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_81's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_3.tmp_0']} = cast(inputs={X=['tmp_81']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 268, op original_id: 268, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_81's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_3.tmp_1']} = softmax(inputs={X=['softmax_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 269, op original_id: 269, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; softmax_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_82']} = cast(inputs={X=['softmax_3.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 270, op original_id: 270, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; softmax_3.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_82's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_7.tmp_0']} = matmul_v2(inputs={X=['tmp_82'], Y=['transpose_17.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 271, op original_id: 271, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_82's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_19.tmp_0'], XShape=['transpose_19.tmp_1']} = transpose2(inputs={X=['matmul_v2_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 272, op original_id: 272, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; matmul_v2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_19.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_19.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_19.tmp_0'], XShape=['reshape2_19.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 273, op original_id: 273, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; transpose_19.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_19.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_19.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_24.tmp_0']} = matmul_v2(inputs={X=['reshape2_19.tmp_0'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 274, op original_id: 274, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_19.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_24.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_24.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_24.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_24.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_83']} = elementwise_add(inputs={X=['tmp_67@RESHARD_0'], Y=['linear_24.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 275, op original_id: 275, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_67's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_24.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_83's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_84']} = cast(inputs={X=['tmp_83']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 276, op original_id: 276, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_83's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_84's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_7.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_84']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 277, op original_id: 277, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_84's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_7.tmp_0']} = reduce_mean(inputs={X=['pow_7.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 278, op original_id: 278, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; pow_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_85']} = scale(inputs={ScaleTensor=[], X=['mean_7.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 279, op original_id: 279, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; mean_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_85's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_7.tmp_0']} = rsqrt(inputs={X=['tmp_85']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 280, op original_id: 280, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_85's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_86']} = elementwise_mul(inputs={X=['rsqrt_7.tmp_0'], Y=['tmp_83']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 281, op original_id: 281, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; rsqrt_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_83's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_86's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_87']} = elementwise_mul(inputs={X=['tmp_86'], Y=['create_parameter_7.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 282, op original_id: 282, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_86's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_7.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_87's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_25.tmp_0']} = matmul_v2(inputs={X=['tmp_87'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 283, op original_id: 283, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_87's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_25.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_25.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_3.tmp_0']} = silu(inputs={X=['linear_25.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 284, op original_id: 284, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_25.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_26.tmp_0']} = matmul_v2(inputs={X=['tmp_87'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 285, op original_id: 285, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_87's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_26.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_26.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_88']} = elementwise_mul(inputs={X=['silu_3.tmp_0'], Y=['linear_26.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 286, op original_id: 286, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; silu_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_26.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_88's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_27.tmp_0']} = matmul_v2(inputs={X=['tmp_88'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 287, op original_id: 287, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_88's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_27.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_27.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_27.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_27.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_89']} = elementwise_add(inputs={X=['tmp_83'], Y=['linear_27.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 288, op original_id: 288, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_83's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_27.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_89's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_90']} = cast(inputs={X=['tmp_89']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_8.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_90']}, factor = 2.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['mean_8.tmp_0']} = reduce_mean(inputs={X=['pow_8.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_91']} = scale(inputs={ScaleTensor=[], X=['mean_8.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_8.tmp_0']} = rsqrt(inputs={X=['tmp_91']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_92']} = elementwise_mul(inputs={X=['rsqrt_8.tmp_0'], Y=['tmp_89']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_93']} = elementwise_mul(inputs={X=['tmp_92'], Y=['create_parameter_8.w_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0']} = matmul_v2(inputs={X=['tmp_93'], Y=['llama_lm_head_auto_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_94']} = cast(inputs={X=['matmul_v2_8.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['unsqueeze2_18.tmp_0'], XShape=['unsqueeze2_18.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['labels']}, axes = [2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['c_concat@RESHARD_0.tmp_0']} = c_concat(inputs={X=['tmp_94']}, nranks = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], rank = 1, ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['unsqueeze2_18.tmp_0'], Logits=['c_concat@RESHARD_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {Out=['tmp_95']} = fill_constant(inputs={}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [], str_value = , value = 0.0, with_quant_attr = False)
    {Out=['tmp_96']} = greater_than(inputs={X=['softmax_with_cross_entropy_0.tmp_1'], Y=['tmp_95']}, axis = -1, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Y=['masked_select_0.tmp_0']} = masked_select(inputs={Mask=['tmp_96'], X=['softmax_with_cross_entropy_0.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_97']} = cast(inputs={X=['masked_select_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['mean_9.tmp_0']} = reduce_mean(inputs={X=['tmp_97']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['mean_9.tmp_0.cast_fp32_0']} = cast(inputs={X=['mean_9.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['scaled_loss_0']} = elementwise_mul(inputs={X=['mean_9.tmp_0.cast_fp32_0'], Y=['loss_scaling_0']}, axis = -1, op_device = , op_namescope = /, op_role = 256, op_role_var = [], with_quant_attr = False)
    {Out=['scaled_loss_1@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = , op_role = 257, op_role_var = [], place_type = -1, shape = [], str_value = , value = 1.0, with_quant_attr = False)
    {X@GRAD=['mean_9.tmp_0.cast_fp32_0@GRAD_0'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['scaled_loss_1@GRAD'], X=['mean_9.tmp_0.cast_fp32_0'], Y=['loss_scaling_0']}, axis = -1, op_role = 1)
    {Out=['mean_9.tmp_0@GRAD']} = cast(inputs={X=['mean_9.tmp_0.cast_fp32_0@GRAD_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['tmp_97@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_9.tmp_0@GRAD'], X=['tmp_97']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['masked_select_0.tmp_0@GRAD']} = cast(inputs={X=['tmp_97@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD']} = masked_select_grad(inputs={Mask=['tmp_96'], X=['softmax_with_cross_entropy_0.tmp_1'], Y@GRAD=['masked_select_0.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Logits@GRAD=['tmp_94@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['unsqueeze2_18.tmp_0'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {Out=['split@RESHARD.tmp_0', 'split@RESHARD.tmp_1']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['tmp_94@GRAD']}, axis = 2, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0@GRAD']} = cast(inputs={X=['split@RESHARD.tmp_1']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_93@GRAD']} = matmul_v2(inputs={X=['matmul_v2_8.tmp_0@GRAD'], Y=['llama_lm_head_auto_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_93@GRAD']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_93@GRAD']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_93@reshape.out'], XShape=['tmp_93@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_93']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0@GRAD@reshape.out'], XShape=['matmul_v2_8.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['matmul_v2_8.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 16000], use_quantizer = False, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_93@reshape.out'], Y=['matmul_v2_8.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16'], XShape=['llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 16000], use_quantizer = False, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD']} = cast(inputs={X=['llama_lm_head_auto_0.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['tmp_92@GRAD'], Y@GRAD=['create_parameter_8.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_93@GRAD'], X=['tmp_92'], Y=['create_parameter_8.w_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_8.w_0@GRAD']} = cast(inputs={X=['create_parameter_8.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_8.tmp_0@GRAD'], Y@GRAD=['tmp_89@GRAD@RENAME@block0@0']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_92@GRAD'], X=['rsqrt_8.tmp_0'], Y=['tmp_89']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_91@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_8.tmp_0'], Out@GRAD=['rsqrt_8.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_91@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_8.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_8.tmp_0@GRAD'], X=['pow_8.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_90@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_8.tmp_0@GRAD'], X=['tmp_90']}, factor = 2.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_89@GRAD@RENAME@block0@1']} = cast(inputs={X=['tmp_90@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_68.subprog_0']} = cast(inputs={X=['tmp_67@RESHARD_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_6.tmp_0.subprog_0']} = pow(inputs={FactorTensor=[], X=['tmp_68.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_6.tmp_0.subprog_0']} = reduce_mean(inputs={X=['pow_6.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_69.subprog_0']} = scale(inputs={ScaleTensor=[], X=['mean_6.tmp_0.subprog_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_6.tmp_0.subprog_0']} = rsqrt(inputs={X=['tmp_69.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_70.subprog_0']} = elementwise_mul(inputs={X=['rsqrt_6.tmp_0.subprog_0'], Y=['tmp_67@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_71.subprog_0']} = elementwise_mul(inputs={X=['tmp_70.subprog_0'], Y=['create_parameter_6.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_21.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_71.subprog_0'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_15.tmp_0.subprog_0'], XShape=['reshape2_15.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_21.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_22.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_71.subprog_0'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_16.tmp_0.subprog_0'], XShape=['reshape2_16.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_22.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_23.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_71.subprog_0'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_17.tmp_0.subprog_0'], XShape=['reshape2_17.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_23.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_6.tmp_0.subprog_0'], XShape=['squeeze_6.tmp_1.subprog_0']} = squeeze2(inputs={X=['eager_tmp_10']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_7.tmp_0.subprog_0'], XShape=['squeeze_7.tmp_1.subprog_0']} = squeeze2(inputs={X=['eager_tmp_11']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_14.tmp_0.subprog_0'], XShape=['unsqueeze2_14.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_6.tmp_0.subprog_0']} = gather_nd(inputs={Index=['unsqueeze2_14.tmp_0.subprog_0'], X=['squeeze_6.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_15.tmp_0.subprog_0'], XShape=['unsqueeze2_15.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_6.tmp_0.subprog_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_16.tmp_0.subprog_0'], XShape=['unsqueeze2_16.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_7.tmp_0.subprog_0']} = gather_nd(inputs={Index=['unsqueeze2_16.tmp_0.subprog_0'], X=['squeeze_7.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_17.tmp_0.subprog_0'], XShape=['unsqueeze2_17.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_7.tmp_0.subprog_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_72.subprog_0']} = elementwise_mul(inputs={X=['reshape2_15.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0_slice_0.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0_slice_1.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_73.subprog_0']} = scale(inputs={ScaleTensor=[], X=['reshape2_15.tmp_0_slice_1.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_6.tmp_0.subprog_0']} = concat(inputs={AxisTensor=[], X=['tmp_73.subprog_0', 'reshape2_15.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_74.subprog_0']} = elementwise_mul(inputs={X=['concat_6.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_75.subprog_0']} = elementwise_add(inputs={X=['tmp_72.subprog_0'], Y=['tmp_74.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_76.subprog_0']} = elementwise_mul(inputs={X=['reshape2_16.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0_slice_0.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0_slice_1.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_77.subprog_0']} = scale(inputs={ScaleTensor=[], X=['reshape2_16.tmp_0_slice_1.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_7.tmp_0.subprog_0']} = concat(inputs={AxisTensor=[], X=['tmp_77.subprog_0', 'reshape2_16.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_78.subprog_0']} = elementwise_mul(inputs={X=['concat_7.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_79.subprog_0']} = elementwise_add(inputs={X=['tmp_76.subprog_0'], Y=['tmp_78.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_15.tmp_0.subprog_0'], XShape=['transpose_15.tmp_1.subprog_0']} = transpose2(inputs={X=['tmp_75.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_16.tmp_0.subprog_0'], XShape=['transpose_16.tmp_1.subprog_0']} = transpose2(inputs={X=['tmp_79.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_17.tmp_0.subprog_0'], XShape=['transpose_17.tmp_1.subprog_0']} = transpose2(inputs={X=['reshape2_17.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_80.subprog_0']} = scale(inputs={ScaleTensor=[], X=['transpose_15.tmp_0.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_18.tmp_0.subprog_0'], XShape=['transpose_18.tmp_1.subprog_0']} = transpose2(inputs={X=['transpose_16.tmp_0.subprog_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_6.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_80.subprog_0'], Y=['transpose_18.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_18.tmp_0.subprog_0'], XShape=['reshape2_18.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_81.subprog_0']} = elementwise_add(inputs={X=['matmul_v2_6.tmp_0.subprog_0'], Y=['reshape2_18.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_3.tmp_0.subprog_0']} = cast(inputs={X=['tmp_81.subprog_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_3.tmp_1.subprog_0']} = softmax(inputs={X=['softmax_3.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_82.subprog_0']} = cast(inputs={X=['softmax_3.tmp_1.subprog_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_7.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_82.subprog_0'], Y=['transpose_17.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_19.tmp_0.subprog_0'], XShape=['transpose_19.tmp_1.subprog_0']} = transpose2(inputs={X=['matmul_v2_7.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_19.tmp_0.subprog_0'], XShape=['reshape2_19.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_24.tmp_0.subprog_0']} = matmul_v2(inputs={X=['reshape2_19.tmp_0.subprog_0'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_24.tmp_0.subprog_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_24.tmp_0.subprog_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_83.subprog_0']} = elementwise_add(inputs={X=['tmp_67@RESHARD_0'], Y=['linear_24.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_84.subprog_0']} = cast(inputs={X=['tmp_83.subprog_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_7.tmp_0.subprog_0']} = pow(inputs={FactorTensor=[], X=['tmp_84.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_7.tmp_0.subprog_0']} = reduce_mean(inputs={X=['pow_7.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_85.subprog_0']} = scale(inputs={ScaleTensor=[], X=['mean_7.tmp_0.subprog_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_7.tmp_0.subprog_0']} = rsqrt(inputs={X=['tmp_85.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_86.subprog_0']} = elementwise_mul(inputs={X=['rsqrt_7.tmp_0.subprog_0'], Y=['tmp_83.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_87.subprog_0']} = elementwise_mul(inputs={X=['tmp_86.subprog_0'], Y=['create_parameter_7.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_25.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_87.subprog_0'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_3.tmp_0.subprog_0']} = silu(inputs={X=['linear_25.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_26.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_87.subprog_0'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_88.subprog_0']} = elementwise_mul(inputs={X=['silu_3.tmp_0.subprog_0'], Y=['linear_26.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_27.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_88.subprog_0'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_27.tmp_0.subprog_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_27.tmp_0.subprog_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_89@GRAD']} = sum(inputs={X=['tmp_89@GRAD@RENAME@block0@0', 'tmp_89@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_83@GRAD@RENAME@block0@0'], Y@GRAD=['linear_27.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_89@GRAD'], X=['tmp_83.subprog_0'], Y=['linear_27.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_88@GRAD'], Y@GRAD=['linear_27.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_27.tmp_0@GRAD'], X=['tmp_88.subprog_0'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD']} = cast(inputs={X=['linear_27.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['silu_3.tmp_0@GRAD'], Y@GRAD=['linear_26.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_88@GRAD'], X=['silu_3.tmp_0.subprog_0'], Y=['linear_26.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_87@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_26.tmp_0@GRAD'], Y=['linear_26.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_87@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_87@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_87.subprog_0@reshape.out'], XShape=['tmp_87.subprog_0@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_87.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_26.tmp_0@GRAD@reshape.out'], XShape=['linear_26.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_26.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_87.subprog_0@reshape.out'], Y=['linear_26.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD@master_grad_fp16'], XShape=['linear_26.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_26.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD']} = cast(inputs={X=['linear_26.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['linear_25.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_3.tmp_0.subprog_0'], Out@GRAD=['silu_3.tmp_0@GRAD'], X=['linear_25.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_87@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_25.tmp_0@GRAD'], Y=['linear_25.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_87@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_87@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_87.subprog_0@reshape.out'], XShape=['tmp_87.subprog_0@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_87.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_25.tmp_0@GRAD@reshape.out'], XShape=['linear_25.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_25.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_87.subprog_0@reshape.out'], Y=['linear_25.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD@master_grad_fp16'], XShape=['linear_25.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_25.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD']} = cast(inputs={X=['linear_25.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_87@GRAD']} = sum(inputs={X=['tmp_87@GRAD@RENAME@block0@0', 'tmp_87@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_86@GRAD'], Y@GRAD=['create_parameter_7.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_87@GRAD'], X=['tmp_86.subprog_0'], Y=['create_parameter_7.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['create_parameter_7.w_0', 'create_parameter_7.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_7.w_0@GRAD']} = cast(inputs={X=['create_parameter_7.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_7.tmp_0@GRAD'], Y@GRAD=['tmp_83@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_86@GRAD'], X=['rsqrt_7.tmp_0.subprog_0'], Y=['tmp_83.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_85@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_7.tmp_0.subprog_0'], Out@GRAD=['rsqrt_7.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_7.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_85@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_7.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_7.tmp_0@GRAD'], X=['pow_7.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_84@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_7.tmp_0@GRAD'], X=['tmp_84.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_83@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_84@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_83@GRAD']} = sum(inputs={X=['tmp_83@GRAD@RENAME@block0@0', 'tmp_83@GRAD@RENAME@block0@1', 'tmp_83@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_67@GRAD@RENAME@block0@0'], Y@GRAD=['linear_24.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_83@GRAD'], X=['tmp_67@RESHARD_0'], Y=['linear_24.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_19.tmp_0@GRAD'], Y@GRAD=['linear_24.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_24.tmp_0@GRAD'], X=['reshape2_19.tmp_0.subprog_0'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD']} = cast(inputs={X=['linear_24.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['transpose_19.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_19.tmp_0@GRAD'], XShape=['reshape2_19.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['transpose_19.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_82@GRAD'], Y@GRAD=['transpose_17.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_7.tmp_0@GRAD'], X=['tmp_82.subprog_0'], Y=['transpose_17.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_3.tmp_1@GRAD']} = cast(inputs={X=['tmp_82@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_3.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_3.tmp_1.subprog_0'], Out@GRAD=['softmax_3.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_81@GRAD']} = cast(inputs={X=['softmax_3.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_6.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_81@GRAD'], X=['matmul_v2_6.tmp_0.subprog_0'], Y=['reshape2_18.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_80@GRAD'], Y@GRAD=['transpose_18.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_6.tmp_0@GRAD'], X=['tmp_80.subprog_0'], Y=['transpose_18.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_16.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_18.tmp_0@GRAD'], XShape=['transpose_18.tmp_1.subprog_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_15.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_80@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_17.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_17.tmp_0@GRAD'], XShape=['transpose_17.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_79@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_16.tmp_0@GRAD'], XShape=['transpose_16.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_75@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['transpose_15.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_76@GRAD'], Y@GRAD=['tmp_78@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_79@GRAD'], X=['tmp_76.subprog_0'], Y=['tmp_78.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_7.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_78@GRAD'], X=['concat_7.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_77@GRAD', 'reshape2_16.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_7.tmp_0@GRAD'], X=['tmp_77.subprog_0', 'reshape2_16.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_77@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_16.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], Out@GRAD=['reshape2_16.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_16.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], Out@GRAD=['reshape2_16.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_16.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_76@GRAD'], X=['reshape2_16.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_72@GRAD'], Y@GRAD=['tmp_74@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_75@GRAD'], X=['tmp_72.subprog_0'], Y=['tmp_74.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_6.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_74@GRAD'], X=['concat_6.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_73@GRAD', 'reshape2_15.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_6.tmp_0@GRAD'], X=['tmp_73.subprog_0', 'reshape2_15.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_73@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_15.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], Out@GRAD=['reshape2_15.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_15.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], Out@GRAD=['reshape2_15.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_15.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_72@GRAD'], X=['reshape2_15.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_23.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_17.tmp_0@GRAD'], XShape=['reshape2_17.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_71@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_23.tmp_0@GRAD'], Y=['linear_23.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_71@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_71@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_71.subprog_0@reshape.out'], XShape=['tmp_71.subprog_0@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_71.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_23.tmp_0@GRAD@reshape.out'], XShape=['linear_23.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_23.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_71.subprog_0@reshape.out'], Y=['linear_23.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD@master_grad_fp16'], XShape=['linear_23.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_23.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD']} = cast(inputs={X=['linear_23.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_16.tmp_0@GRAD']} = sum(inputs={X=['reshape2_16.tmp_0@GRAD@RENAME@block0@0', 'reshape2_16.tmp_0@GRAD@RENAME@block0@1', 'reshape2_16.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_22.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_16.tmp_0@GRAD'], XShape=['reshape2_16.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_71@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_22.tmp_0@GRAD'], Y=['linear_22.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_71@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_71@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_71.subprog_0@reshape.out'], XShape=['tmp_71.subprog_0@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_71.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_22.tmp_0@GRAD@reshape.out'], XShape=['linear_22.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_22.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_71.subprog_0@reshape.out'], Y=['linear_22.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD@master_grad_fp16'], XShape=['linear_22.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_22.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD']} = cast(inputs={X=['linear_22.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_15.tmp_0@GRAD']} = sum(inputs={X=['reshape2_15.tmp_0@GRAD@RENAME@block0@0', 'reshape2_15.tmp_0@GRAD@RENAME@block0@1', 'reshape2_15.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_21.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_15.tmp_0@GRAD'], XShape=['reshape2_15.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_71@GRAD@RENAME@block0@2']} = matmul_v2(inputs={X=['linear_21.tmp_0@GRAD'], Y=['linear_21.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_71@GRAD@RENAME@block0@2']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_71@GRAD@RENAME@block0@2']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_71.subprog_0@reshape.out'], XShape=['tmp_71.subprog_0@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_71.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_21.tmp_0@GRAD@reshape.out'], XShape=['linear_21.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_21.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_71.subprog_0@reshape.out'], Y=['linear_21.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD@master_grad_fp16'], XShape=['linear_21.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_21.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD']} = cast(inputs={X=['linear_21.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_71@GRAD']} = sum(inputs={X=['tmp_71@GRAD@RENAME@block0@0', 'tmp_71@GRAD@RENAME@block0@1', 'tmp_71@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_70@GRAD'], Y@GRAD=['create_parameter_6.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_71@GRAD'], X=['tmp_70.subprog_0'], Y=['create_parameter_6.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['create_parameter_6.w_0', 'create_parameter_6.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_6.w_0@GRAD']} = cast(inputs={X=['create_parameter_6.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_6.tmp_0@GRAD'], Y@GRAD=['tmp_67@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_70@GRAD'], X=['rsqrt_6.tmp_0.subprog_0'], Y=['tmp_67@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_69@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_6.tmp_0.subprog_0'], Out@GRAD=['rsqrt_6.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_6.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_69@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_6.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_6.tmp_0@GRAD'], X=['pow_6.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_68@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_6.tmp_0@GRAD'], X=['tmp_68.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_67@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_68@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_67@GRAD']} = sum(inputs={X=['tmp_67@GRAD@RENAME@block0@0', 'tmp_67@GRAD@RENAME@block0@1', 'tmp_67@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_67@GRAD']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], peer = 1, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_24.subprog_2']} = cast(inputs={X=['tmp_23@RESHARD_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_2.tmp_0.subprog_2']} = pow(inputs={FactorTensor=[], X=['tmp_24.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_2.tmp_0.subprog_2']} = reduce_mean(inputs={X=['pow_2.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_25.subprog_2']} = scale(inputs={ScaleTensor=[], X=['mean_2.tmp_0.subprog_2']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_2.tmp_0.subprog_2']} = rsqrt(inputs={X=['tmp_25.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_26.subprog_2']} = elementwise_mul(inputs={X=['rsqrt_2.tmp_0.subprog_2'], Y=['tmp_23@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_27.subprog_2']} = elementwise_mul(inputs={X=['tmp_26.subprog_2'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_7.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_5.tmp_0.subprog_2'], XShape=['reshape2_5.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_8.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_6.tmp_0.subprog_2'], XShape=['reshape2_6.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_7.tmp_0.subprog_2'], XShape=['reshape2_7.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_2.tmp_0.subprog_2'], XShape=['squeeze_2.tmp_1.subprog_2']} = squeeze2(inputs={X=['eager_tmp_4']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_3.tmp_0.subprog_2'], XShape=['squeeze_3.tmp_1.subprog_2']} = squeeze2(inputs={X=['eager_tmp_5']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_6.tmp_0.subprog_2'], XShape=['unsqueeze2_6.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_2.tmp_0.subprog_2']} = gather_nd(inputs={Index=['unsqueeze2_6.tmp_0.subprog_2'], X=['squeeze_2.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_7.tmp_0.subprog_2'], XShape=['unsqueeze2_7.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_2.tmp_0.subprog_2']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_8.tmp_0.subprog_2'], XShape=['unsqueeze2_8.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_3.tmp_0.subprog_2']} = gather_nd(inputs={Index=['unsqueeze2_8.tmp_0.subprog_2'], X=['squeeze_3.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_9.tmp_0.subprog_2'], XShape=['unsqueeze2_9.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_3.tmp_0.subprog_2']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_28.subprog_2']} = elementwise_mul(inputs={X=['reshape2_5.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_0.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_1.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_29.subprog_2']} = scale(inputs={ScaleTensor=[], X=['reshape2_5.tmp_0_slice_1.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_2.tmp_0.subprog_2']} = concat(inputs={AxisTensor=[], X=['tmp_29.subprog_2', 'reshape2_5.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_30.subprog_2']} = elementwise_mul(inputs={X=['concat_2.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_31.subprog_2']} = elementwise_add(inputs={X=['tmp_28.subprog_2'], Y=['tmp_30.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_32.subprog_2']} = elementwise_mul(inputs={X=['reshape2_6.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_0.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_1.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_33.subprog_2']} = scale(inputs={ScaleTensor=[], X=['reshape2_6.tmp_0_slice_1.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_3.tmp_0.subprog_2']} = concat(inputs={AxisTensor=[], X=['tmp_33.subprog_2', 'reshape2_6.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_34.subprog_2']} = elementwise_mul(inputs={X=['concat_3.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_35.subprog_2']} = elementwise_add(inputs={X=['tmp_32.subprog_2'], Y=['tmp_34.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0.subprog_2'], XShape=['transpose_5.tmp_1.subprog_2']} = transpose2(inputs={X=['tmp_31.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_6.tmp_0.subprog_2'], XShape=['transpose_6.tmp_1.subprog_2']} = transpose2(inputs={X=['tmp_35.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_7.tmp_0.subprog_2'], XShape=['transpose_7.tmp_1.subprog_2']} = transpose2(inputs={X=['reshape2_7.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_36.subprog_2']} = scale(inputs={ScaleTensor=[], X=['transpose_5.tmp_0.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_8.tmp_0.subprog_2'], XShape=['transpose_8.tmp_1.subprog_2']} = transpose2(inputs={X=['transpose_6.tmp_0.subprog_2']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_36.subprog_2'], Y=['transpose_8.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_8.tmp_0.subprog_2'], XShape=['reshape2_8.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_37.subprog_2']} = elementwise_add(inputs={X=['matmul_v2_2.tmp_0.subprog_2'], Y=['reshape2_8.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_1.tmp_0.subprog_2']} = cast(inputs={X=['tmp_37.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_1.tmp_1.subprog_2']} = softmax(inputs={X=['softmax_1.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_38.subprog_2']} = cast(inputs={X=['softmax_1.tmp_1.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_3.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_38.subprog_2'], Y=['transpose_7.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_9.tmp_0.subprog_2'], XShape=['transpose_9.tmp_1.subprog_2']} = transpose2(inputs={X=['matmul_v2_3.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_9.tmp_0.subprog_2'], XShape=['reshape2_9.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_9.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_10.tmp_0.subprog_2']} = matmul_v2(inputs={X=['reshape2_9.tmp_0.subprog_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_10.tmp_0.subprog_2']} = c_allreduce_sum(inputs={Cond=[], X=['linear_10.tmp_0.subprog_2']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_39.subprog_2']} = elementwise_add(inputs={X=['tmp_23@RESHARD_0'], Y=['linear_10.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_40.subprog_2']} = cast(inputs={X=['tmp_39.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_3.tmp_0.subprog_2']} = pow(inputs={FactorTensor=[], X=['tmp_40.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_3.tmp_0.subprog_2']} = reduce_mean(inputs={X=['pow_3.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_41.subprog_2']} = scale(inputs={ScaleTensor=[], X=['mean_3.tmp_0.subprog_2']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_3.tmp_0.subprog_2']} = rsqrt(inputs={X=['tmp_41.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_42.subprog_2']} = elementwise_mul(inputs={X=['rsqrt_3.tmp_0.subprog_2'], Y=['tmp_39.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43.subprog_2']} = elementwise_mul(inputs={X=['tmp_42.subprog_2'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_11.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_43.subprog_2'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_1.tmp_0.subprog_2']} = silu(inputs={X=['linear_11.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_12.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_43.subprog_2'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_44.subprog_2']} = elementwise_mul(inputs={X=['silu_1.tmp_0.subprog_2'], Y=['linear_12.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_13.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_44.subprog_2'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_13.tmp_0.subprog_2']} = c_allreduce_sum(inputs={Cond=[], X=['linear_13.tmp_0.subprog_2']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_45@GRAD']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {X@GRAD=['tmp_39@GRAD@RENAME@block0@0'], Y@GRAD=['linear_13.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_45@GRAD'], X=['tmp_39.subprog_2'], Y=['linear_13.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_44@GRAD'], Y@GRAD=['linear_13.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_13.tmp_0@GRAD'], X=['tmp_44.subprog_2'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD']} = cast(inputs={X=['linear_13.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['silu_1.tmp_0@GRAD'], Y@GRAD=['linear_12.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_44@GRAD'], X=['silu_1.tmp_0.subprog_2'], Y=['linear_12.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_12.tmp_0@GRAD'], Y=['linear_12.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_43@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_43@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_43.subprog_2@reshape.out'], XShape=['tmp_43.subprog_2@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_43.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_12.tmp_0@GRAD@reshape.out'], XShape=['linear_12.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_12.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_43.subprog_2@reshape.out'], Y=['linear_12.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@master_grad_fp16'], XShape=['linear_12.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_12.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD']} = cast(inputs={X=['linear_12.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['linear_11.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_1.tmp_0.subprog_2'], Out@GRAD=['silu_1.tmp_0@GRAD'], X=['linear_11.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_11.tmp_0@GRAD'], Y=['linear_11.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_43@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_43@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_43.subprog_2@reshape.out'], XShape=['tmp_43.subprog_2@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_43.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_11.tmp_0@GRAD@reshape.out'], XShape=['linear_11.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_11.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_43.subprog_2@reshape.out'], Y=['linear_11.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD@master_grad_fp16'], XShape=['linear_11.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_11.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD']} = cast(inputs={X=['linear_11.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_43@GRAD']} = sum(inputs={X=['tmp_43@GRAD@RENAME@block0@0', 'tmp_43@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_42@GRAD'], Y@GRAD=['create_parameter_3.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_43@GRAD'], X=['tmp_42.subprog_2'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['create_parameter_3.w_0', 'create_parameter_3.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD']} = cast(inputs={X=['create_parameter_3.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_3.tmp_0@GRAD'], Y@GRAD=['tmp_39@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_42@GRAD'], X=['rsqrt_3.tmp_0.subprog_2'], Y=['tmp_39.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_41@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_3.tmp_0.subprog_2'], Out@GRAD=['rsqrt_3.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_3.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_41@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_3.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_3.tmp_0@GRAD'], X=['pow_3.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_40@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_3.tmp_0@GRAD'], X=['tmp_40.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_39@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_40@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_39@GRAD']} = sum(inputs={X=['tmp_39@GRAD@RENAME@block0@0', 'tmp_39@GRAD@RENAME@block0@1', 'tmp_39@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_23@GRAD@RENAME@block0@0'], Y@GRAD=['linear_10.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['tmp_23@RESHARD_0'], Y=['linear_10.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD'], Y@GRAD=['linear_10.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_10.tmp_0@GRAD'], X=['reshape2_9.tmp_0.subprog_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD']} = cast(inputs={X=['linear_10.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['transpose_9.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_38@GRAD'], Y@GRAD=['transpose_7.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['tmp_38.subprog_2'], Y=['transpose_7.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_1.tmp_1@GRAD']} = cast(inputs={X=['tmp_38@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_1.subprog_2'], Out@GRAD=['softmax_1.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_37@GRAD']} = cast(inputs={X=['softmax_1.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['matmul_v2_2.tmp_0.subprog_2'], Y=['reshape2_8.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_36@GRAD'], Y@GRAD=['transpose_8.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['tmp_36.subprog_2'], Y=['transpose_8.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1.subprog_2']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_36@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_35@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_31@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_32@GRAD'], Y@GRAD=['tmp_34@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_35@GRAD'], X=['tmp_32.subprog_2'], Y=['tmp_34.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_34@GRAD'], X=['concat_3.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_33@GRAD', 'reshape2_6.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_3.tmp_0@GRAD'], X=['tmp_33.subprog_2', 'reshape2_6.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_33@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], Out@GRAD=['reshape2_6.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], Out@GRAD=['reshape2_6.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['reshape2_6.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_28@GRAD'], Y@GRAD=['tmp_30@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_31@GRAD'], X=['tmp_28.subprog_2'], Y=['tmp_30.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['concat_2.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_29@GRAD', 'reshape2_5.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_2.tmp_0@GRAD'], X=['tmp_29.subprog_2', 'reshape2_5.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_29@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], Out@GRAD=['reshape2_5.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], Out@GRAD=['reshape2_5.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['reshape2_5.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_9.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_27@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_9.tmp_0@GRAD'], Y=['linear_9.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_27@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_27@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_27.subprog_2@reshape.out'], XShape=['tmp_27.subprog_2@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_27.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.tmp_0@GRAD@reshape.out'], XShape=['linear_9.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_27.subprog_2@reshape.out'], Y=['linear_9.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD@master_grad_fp16'], XShape=['linear_9.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD']} = cast(inputs={X=['linear_9.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_6.tmp_0@GRAD']} = sum(inputs={X=['reshape2_6.tmp_0@GRAD@RENAME@block0@0', 'reshape2_6.tmp_0@GRAD@RENAME@block0@1', 'reshape2_6.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_8.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_27@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_8.tmp_0@GRAD'], Y=['linear_8.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_27@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_27@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_27.subprog_2@reshape.out'], XShape=['tmp_27.subprog_2@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_27.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_8.tmp_0@GRAD@reshape.out'], XShape=['linear_8.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_27.subprog_2@reshape.out'], Y=['linear_8.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@master_grad_fp16'], XShape=['linear_8.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD']} = cast(inputs={X=['linear_8.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_5.tmp_0@GRAD']} = sum(inputs={X=['reshape2_5.tmp_0@GRAD@RENAME@block0@0', 'reshape2_5.tmp_0@GRAD@RENAME@block0@1', 'reshape2_5.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_27@GRAD@RENAME@block0@2']} = matmul_v2(inputs={X=['linear_7.tmp_0@GRAD'], Y=['linear_7.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_27@GRAD@RENAME@block0@2']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_27@GRAD@RENAME@block0@2']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 27, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_27.subprog_2@reshape.out'], XShape=['tmp_27.subprog_2@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_27.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_7.tmp_0@GRAD@reshape.out'], XShape=['linear_7.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_27.subprog_2@reshape.out'], Y=['linear_7.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@master_grad_fp16'], XShape=['linear_7.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD']} = cast(inputs={X=['linear_7.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_27@GRAD']} = sum(inputs={X=['tmp_27@GRAD@RENAME@block0@0', 'tmp_27@GRAD@RENAME@block0@1', 'tmp_27@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_26@GRAD'], Y@GRAD=['create_parameter_2.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['tmp_26.subprog_2'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['create_parameter_2.w_0', 'create_parameter_2.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD']} = cast(inputs={X=['create_parameter_2.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_2.tmp_0@GRAD'], Y@GRAD=['tmp_23@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_26@GRAD'], X=['rsqrt_2.tmp_0.subprog_2'], Y=['tmp_23@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_25@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_2.tmp_0.subprog_2'], Out@GRAD=['rsqrt_2.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_2.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_25@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_2.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_2.tmp_0@GRAD'], X=['pow_2.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_24@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_2.tmp_0@GRAD'], X=['tmp_24.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_23@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_24@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_23@GRAD']} = sum(inputs={X=['tmp_23@GRAD@RENAME@block0@0', 'tmp_23@GRAD@RENAME@block0@1', 'tmp_23@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_23@GRAD']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], peer = 1, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['linear_7.w_0@GRAD', 'linear_8.w_0@GRAD', 'linear_9.w_0@GRAD', 'linear_10.w_0@GRAD', 'linear_11.w_0@GRAD', 'linear_12.w_0@GRAD', 'linear_13.w_0@GRAD', 'create_parameter_2.w_0@GRAD', 'create_parameter_3.w_0@GRAD', 'linear_21.w_0@GRAD', 'linear_22.w_0@GRAD', 'linear_23.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_25.w_0@GRAD', 'linear_26.w_0@GRAD', 'linear_27.w_0@GRAD', 'create_parameter_6.w_0@GRAD', 'create_parameter_7.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['linear_7.w_0@GRAD', 'linear_8.w_0@GRAD', 'linear_9.w_0@GRAD', 'linear_10.w_0@GRAD', 'linear_11.w_0@GRAD', 'linear_12.w_0@GRAD', 'linear_13.w_0@GRAD', 'create_parameter_2.w_0@GRAD', 'create_parameter_3.w_0@GRAD', 'linear_21.w_0@GRAD', 'linear_22.w_0@GRAD', 'linear_23.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_25.w_0@GRAD', 'linear_26.w_0@GRAD', 'linear_27.w_0@GRAD', 'create_parameter_6.w_0@GRAD', 'create_parameter_7.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchorization, op_role = 2, op_role_var = [], ring_id = 28, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['linear_7.w_0@GRAD', 'linear_8.w_0@GRAD', 'linear_9.w_0@GRAD', 'linear_10.w_0@GRAD', 'linear_11.w_0@GRAD', 'linear_12.w_0@GRAD', 'linear_13.w_0@GRAD', 'create_parameter_2.w_0@GRAD', 'create_parameter_3.w_0@GRAD', 'linear_21.w_0@GRAD', 'linear_22.w_0@GRAD', 'linear_23.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_25.w_0@GRAD', 'linear_26.w_0@GRAD', 'linear_27.w_0@GRAD', 'create_parameter_6.w_0@GRAD', 'create_parameter_7.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['linear_7.w_0@GRAD', 'linear_8.w_0@GRAD', 'linear_9.w_0@GRAD', 'linear_10.w_0@GRAD', 'linear_11.w_0@GRAD', 'linear_12.w_0@GRAD', 'linear_13.w_0@GRAD', 'create_parameter_2.w_0@GRAD', 'create_parameter_3.w_0@GRAD', 'linear_21.w_0@GRAD', 'linear_22.w_0@GRAD', 'linear_23.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_25.w_0@GRAD', 'linear_26.w_0@GRAD', 'linear_27.w_0@GRAD', 'create_parameter_6.w_0@GRAD', 'create_parameter_7.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_0.tmp_0']} = squared_l2_norm(inputs={X=['linear_7.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_1.tmp_0']} = squared_l2_norm(inputs={X=['linear_8.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_2.tmp_0']} = squared_l2_norm(inputs={X=['linear_9.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_3.tmp_0']} = squared_l2_norm(inputs={X=['linear_10.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_4.tmp_0']} = squared_l2_norm(inputs={X=['linear_11.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_5.tmp_0']} = squared_l2_norm(inputs={X=['linear_12.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_6.tmp_0']} = squared_l2_norm(inputs={X=['linear_13.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_7.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_2.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_2.w_0', 'create_parameter_2.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_8.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_3.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_3.w_0', 'create_parameter_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_9.tmp_0']} = squared_l2_norm(inputs={X=['linear_21.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_10.tmp_0']} = squared_l2_norm(inputs={X=['linear_22.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_11.tmp_0']} = squared_l2_norm(inputs={X=['linear_23.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_12.tmp_0']} = squared_l2_norm(inputs={X=['linear_24.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_13.tmp_0']} = squared_l2_norm(inputs={X=['linear_25.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_14.tmp_0']} = squared_l2_norm(inputs={X=['linear_26.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_15.tmp_0']} = squared_l2_norm(inputs={X=['linear_27.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_16.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_6.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_6.w_0', 'create_parameter_6.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_17.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_7.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_7.w_0', 'create_parameter_7.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_18.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_8.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_19.tmp_0']} = squared_l2_norm(inputs={X=['llama_lm_head_auto_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Y=['opt_opt_stack_0.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_0.tmp_0', 'opt_opt_squared_l2_norm_1.tmp_0', 'opt_opt_squared_l2_norm_2.tmp_0', 'opt_opt_squared_l2_norm_3.tmp_0', 'opt_opt_squared_l2_norm_4.tmp_0', 'opt_opt_squared_l2_norm_5.tmp_0', 'opt_opt_squared_l2_norm_6.tmp_0', 'opt_opt_squared_l2_norm_7.tmp_0', 'opt_opt_squared_l2_norm_8.tmp_0', 'opt_opt_squared_l2_norm_9.tmp_0', 'opt_opt_squared_l2_norm_10.tmp_0', 'opt_opt_squared_l2_norm_11.tmp_0', 'opt_opt_squared_l2_norm_12.tmp_0', 'opt_opt_squared_l2_norm_13.tmp_0', 'opt_opt_squared_l2_norm_14.tmp_0', 'opt_opt_squared_l2_norm_15.tmp_0', 'opt_opt_squared_l2_norm_16.tmp_0', 'opt_opt_squared_l2_norm_17.tmp_0', 'opt_opt_squared_l2_norm_18.tmp_0', 'opt_opt_squared_l2_norm_19.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_sum_0.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_0.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_opt_sum_0.tmp_0']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_7.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_8.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_9.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_10.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_11.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_12.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_13.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_2.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_2.w_0', 'create_parameter_2.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_3.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_3.w_0', 'create_parameter_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_21.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_22.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_23.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_24.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_25.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_26.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_27.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_6.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_6.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_6.w_0', 'create_parameter_6.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_7.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_7.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_7.w_0', 'create_parameter_7.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_8.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_8.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD']} = elementwise_mul(inputs={X=['llama_lm_head_auto_0.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Beta1PowOut=['linear_7.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_7.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_7.w_0_fp32_master_1'], Moment1Out=['linear_7.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_7.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_7.w_0']} = adamw(inputs={Beta1Pow=['linear_7.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_7.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_7.w_0_fp32_master_1'], Moment1=['linear_7.w_0_fp32_master_1_moment1_0'], Moment2=['linear_7.w_0_fp32_master_1_moment2_0'], Param=['linear_7.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_39/, op_role = 2, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_8.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_8.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_8.w_0_fp32_master_1'], Moment1Out=['linear_8.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_8.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_8.w_0']} = adamw(inputs={Beta1Pow=['linear_8.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_8.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_8.w_0_fp32_master_1'], Moment1=['linear_8.w_0_fp32_master_1_moment1_0'], Moment2=['linear_8.w_0_fp32_master_1_moment2_0'], Param=['linear_8.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_40/, op_role = 2, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_9.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_9.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_9.w_0_fp32_master_1'], Moment1Out=['linear_9.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_9.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_9.w_0']} = adamw(inputs={Beta1Pow=['linear_9.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_9.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_9.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_9.w_0_fp32_master_1'], Moment1=['linear_9.w_0_fp32_master_1_moment1_0'], Moment2=['linear_9.w_0_fp32_master_1_moment2_0'], Param=['linear_9.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_41/, op_role = 2, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_10.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_10.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_10.w_0_fp32_master_1'], Moment1Out=['linear_10.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_10.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_10.w_0']} = adamw(inputs={Beta1Pow=['linear_10.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_10.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_10.w_0_fp32_master_1'], Moment1=['linear_10.w_0_fp32_master_1_moment1_0'], Moment2=['linear_10.w_0_fp32_master_1_moment2_0'], Param=['linear_10.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_42/, op_role = 2, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_11.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_11.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_11.w_0_fp32_master_1'], Moment1Out=['linear_11.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_11.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_11.w_0']} = adamw(inputs={Beta1Pow=['linear_11.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_11.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_11.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_11.w_0_fp32_master_1'], Moment1=['linear_11.w_0_fp32_master_1_moment1_0'], Moment2=['linear_11.w_0_fp32_master_1_moment2_0'], Param=['linear_11.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_43/, op_role = 2, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_12.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_12.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_12.w_0_fp32_master_1'], Moment1Out=['linear_12.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_12.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_12.w_0']} = adamw(inputs={Beta1Pow=['linear_12.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_12.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_12.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_12.w_0_fp32_master_1'], Moment1=['linear_12.w_0_fp32_master_1_moment1_0'], Moment2=['linear_12.w_0_fp32_master_1_moment2_0'], Param=['linear_12.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_44/, op_role = 2, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_13.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_13.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_13.w_0_fp32_master_1'], Moment1Out=['linear_13.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_13.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_13.w_0']} = adamw(inputs={Beta1Pow=['linear_13.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_13.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_13.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_13.w_0_fp32_master_1'], Moment1=['linear_13.w_0_fp32_master_1_moment1_0'], Moment2=['linear_13.w_0_fp32_master_1_moment2_0'], Param=['linear_13.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_45/, op_role = 2, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_2.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_2.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_2.w_0_fp32_master_1'], Moment1Out=['create_parameter_2.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_2.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_2.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_2.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_2.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_2.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_2.w_0_fp32_master_1'], Moment1=['create_parameter_2.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_2.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_2.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_46/, op_role = 2, op_role_var = ['create_parameter_2.w_0', 'create_parameter_2.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_3.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_3.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_3.w_0_fp32_master_1'], Moment1Out=['create_parameter_3.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_3.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_3.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_3.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_3.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_3.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_3.w_0_fp32_master_1'], Moment1=['create_parameter_3.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_3.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_3.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_47/, op_role = 2, op_role_var = ['create_parameter_3.w_0', 'create_parameter_3.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_21.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_21.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_21.w_0_fp32_master_1'], Moment1Out=['linear_21.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_21.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_21.w_0']} = adamw(inputs={Beta1Pow=['linear_21.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_21.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_21.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_21.w_0_fp32_master_1'], Moment1=['linear_21.w_0_fp32_master_1_moment1_0'], Moment2=['linear_21.w_0_fp32_master_1_moment2_0'], Param=['linear_21.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_48/, op_role = 2, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_22.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_22.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_22.w_0_fp32_master_1'], Moment1Out=['linear_22.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_22.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_22.w_0']} = adamw(inputs={Beta1Pow=['linear_22.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_22.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_22.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_22.w_0_fp32_master_1'], Moment1=['linear_22.w_0_fp32_master_1_moment1_0'], Moment2=['linear_22.w_0_fp32_master_1_moment2_0'], Param=['linear_22.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_49/, op_role = 2, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_23.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_23.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_23.w_0_fp32_master_1'], Moment1Out=['linear_23.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_23.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_23.w_0']} = adamw(inputs={Beta1Pow=['linear_23.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_23.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_23.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_23.w_0_fp32_master_1'], Moment1=['linear_23.w_0_fp32_master_1_moment1_0'], Moment2=['linear_23.w_0_fp32_master_1_moment2_0'], Param=['linear_23.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_50/, op_role = 2, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_24.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_24.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_24.w_0_fp32_master_1'], Moment1Out=['linear_24.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_24.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_24.w_0']} = adamw(inputs={Beta1Pow=['linear_24.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_24.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_24.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_24.w_0_fp32_master_1'], Moment1=['linear_24.w_0_fp32_master_1_moment1_0'], Moment2=['linear_24.w_0_fp32_master_1_moment2_0'], Param=['linear_24.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_51/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_25.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_25.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_25.w_0_fp32_master_1'], Moment1Out=['linear_25.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_25.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_25.w_0']} = adamw(inputs={Beta1Pow=['linear_25.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_25.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_25.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_25.w_0_fp32_master_1'], Moment1=['linear_25.w_0_fp32_master_1_moment1_0'], Moment2=['linear_25.w_0_fp32_master_1_moment2_0'], Param=['linear_25.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_52/, op_role = 2, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_26.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_26.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_26.w_0_fp32_master_1'], Moment1Out=['linear_26.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_26.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_26.w_0']} = adamw(inputs={Beta1Pow=['linear_26.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_26.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_26.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_26.w_0_fp32_master_1'], Moment1=['linear_26.w_0_fp32_master_1_moment1_0'], Moment2=['linear_26.w_0_fp32_master_1_moment2_0'], Param=['linear_26.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_53/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_27.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_27.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_27.w_0_fp32_master_1'], Moment1Out=['linear_27.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_27.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_27.w_0']} = adamw(inputs={Beta1Pow=['linear_27.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_27.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_27.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_27.w_0_fp32_master_1'], Moment1=['linear_27.w_0_fp32_master_1_moment1_0'], Moment2=['linear_27.w_0_fp32_master_1_moment2_0'], Param=['linear_27.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_54/, op_role = 2, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_6.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_6.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_6.w_0_fp32_master_1'], Moment1Out=['create_parameter_6.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_6.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_6.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_6.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_6.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_6.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_6.w_0_fp32_master_1'], Moment1=['create_parameter_6.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_6.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_6.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_55/, op_role = 2, op_role_var = ['create_parameter_6.w_0', 'create_parameter_6.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_7.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_7.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_7.w_0_fp32_master_1'], Moment1Out=['create_parameter_7.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_7.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_7.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_7.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_7.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_7.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_7.w_0_fp32_master_1'], Moment1=['create_parameter_7.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_7.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_7.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_56/, op_role = 2, op_role_var = ['create_parameter_7.w_0', 'create_parameter_7.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_8.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_8.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_8.w_0_fp32_master_1'], Moment1Out=['create_parameter_8.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_8.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_8.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_8.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_8.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_8.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_8.w_0_fp32_master_1'], Moment1=['create_parameter_8.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_8.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_8.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_57/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['llama_lm_head_auto_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['llama_lm_head_auto_0.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['llama_lm_head_auto_0.w_0_fp32_master_1'], Moment1Out=['llama_lm_head_auto_0.w_0_fp32_master_1_moment1_0'], Moment2Out=['llama_lm_head_auto_0.w_0_fp32_master_1_moment2_0'], ParamOut=['llama_lm_head_auto_0.w_0']} = adamw(inputs={Beta1Pow=['llama_lm_head_auto_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['llama_lm_head_auto_0.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['llama_lm_head_auto_0.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['llama_lm_head_auto_0.w_0_fp32_master_1'], Moment1=['llama_lm_head_auto_0.w_0_fp32_master_1_moment1_0'], Moment2=['llama_lm_head_auto_0.w_0_fp32_master_1_moment2_0'], Param=['llama_lm_head_auto_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_58/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
}

[2024-02-22 15:21:03,225] [    INFO] process_group.py:150 - group_id: 0, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:52589
[2024-02-22 15:21:03,938] [    INFO] process_group.py:150 - group_id: 27, ranks: [2, 3], nranks: 2, trainer_endpoints: 172.17.0.3:52589
[2024-02-22 15:21:04,152] [    INFO] process_group.py:150 - group_id: 28, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:52589
[2024-02-22 15:21:04,443] [    INFO] process_group.py:150 - group_id: 30, ranks: [1, 3], nranks: 2, trainer_endpoints: 172.17.0.3:52589
/usr/local/lib/python3.9/dist-packages/paddle/distributed/auto_parallel/static/process_group.py:255: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-02-22 15:21:04,760] [    INFO] process_group.py:150 - group_id: 32, ranks: [3, 1], nranks: 2, trainer_endpoints: 172.17.0.3:52589
I0222 15:21:05.060089 24064 program_interpreter.cc:220] New Executor is Running.
I0222 15:21:07.710903 24064 interpreter_util.cc:652] Standalone Executor is Used.
I0222 15:21:18.329779 24508 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
[32m[2024-02-22 15:21:18,464] [    INFO][0m - loss: 10.46875, learning_rate: 8.273e-05, global_step: 1, interval_runtime: 13.3338, interval_samples_per_second: 1.1999565783954416, interval_steps_per_second: 0.0749972861497151[0m
[32m[2024-02-22 15:21:28,038] [    INFO][0m - loss: 10.4453125, learning_rate: 7.364e-05, global_step: 2, interval_runtime: 9.5732, interval_samples_per_second: 1.6713355938295367, interval_steps_per_second: 0.10445847461434604[0m
[32m[2024-02-22 15:21:37,647] [    INFO][0m - loss: 10.4609375, learning_rate: 6.455e-05, global_step: 3, interval_runtime: 9.608, interval_samples_per_second: 1.665273803216492, interval_steps_per_second: 0.10407961270103075[0m
[32m[2024-02-22 15:21:47,297] [    INFO][0m - loss: 10.453125, learning_rate: 5.545e-05, global_step: 4, interval_runtime: 9.6491, interval_samples_per_second: 1.6581869964509108, interval_steps_per_second: 0.10363668727818193[0m
[32m[2024-02-22 15:21:57,000] [    INFO][0m - loss: 10.484375, learning_rate: 4.636e-05, global_step: 5, interval_runtime: 9.7012, interval_samples_per_second: 1.6492769330548365, interval_steps_per_second: 0.10307980831592728[0m
