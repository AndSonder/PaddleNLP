grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
[33m[2024-02-22 15:20:47,557] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[33m[2024-02-22 15:20:47,557] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[32m[2024-02-22 15:20:47,558] [    INFO][0m - PP configs:{"enable":"True","schedule_mode":"VPP","vpp_degree":"1","vpp_seg_method":"","micro_batch_size":"1","accumulate_steps":"16","generation_batch_size":"1","enable_send_recv_overlap":"False","job_schedule_profiler_start":"-1","job_schedule_profiler_stop":"-1",}, use master_grad: True[0m
[33m[2024-02-22 15:20:47,558] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[2024-02-22 15:20:47,559] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_call_stack_level', current_value=2, default_value=1)
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0222 15:20:47.561693 24060 tcp_utils.cc:130] Successfully connected to 172.17.0.3:52585
I0222 15:20:47.561961 24060 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0222 15:20:47.562486 24060 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-02-22 15:20:47,562] [    INFO] topology.py:358 - Total 2 pipe comm group(s) create successfully!
W0222 15:20:47.563482 24060 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.8
W0222 15:20:47.564742 24060 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
[2024-02-22 15:20:52,318] [    INFO] topology.py:358 - Total 4 data comm group(s) create successfully!
I0222 15:20:52.319092 24060 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-02-22 15:20:52,319] [    INFO] topology.py:358 - Total 2 model comm group(s) create successfully!
[2024-02-22 15:20:52,319] [    INFO] topology.py:358 - Total 4 sharding comm group(s) create successfully!
I0222 15:20:52.319793 24060 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0222 15:20:52.319936 24060 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-02-22 15:20:52,320] [    INFO] topology.py:288 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 2, dp_degree: 1, sep_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1, 3], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-02-22 15:20:52,321] [    INFO][0m - {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 1024.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': [], 'custom_black_list': [], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': True}","sharding":"{'enable': False, 'stage': 1, 'degree': 8, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': False, 'k_steps': 1, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': 'VPP', 'vpp_degree': 1, 'vpp_seg_method': '', 'micro_batch_size': 1, 'accumulate_steps': 16, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': False}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': True}","sp_optimization":"{'enable': False}",}[0m
/home/workspace/PaddleNLP/llm/llama/auto_parallel/run_pretrain_auto_static.py:429: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only 
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(random_seed)
[32m[2024-02-22 15:20:52,324] [    INFO][0m - The global seed is set to 42, local seed is set to 47 and random seed is set to 42.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_call_stack_level', current_value=2, default_value=1)
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
[35m[2024-02-22 15:20:52,326] [   DEBUG][0m - ============================================================[0m
[35m[2024-02-22 15:20:52,326] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-02-22 15:20:52,326] [   DEBUG][0m - paddle commit id              : 557b888e2c07631a347a1f8b0e06d3cec3250eb1[0m
[35m[2024-02-22 15:20:52,326] [   DEBUG][0m - paddlenlp commit id           : edd3d74fc091d1b9ad0df9607336c8fd4d28bb90.dirty[0m
[35m[2024-02-22 15:20:52,326] [   DEBUG][0m - config_name                   : None[0m
[35m[2024-02-22 15:20:52,326] [   DEBUG][0m - continue_training             : False[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - fuse_attention_ffn            : False[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - fuse_attention_qkv            : False[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - hidden_size                   : None[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - intermediate_size             : None[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - model_name_or_path            : facebook/llama-7b[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - model_type                    : llama[0m
[35m[2024-02-22 15:20:52,327] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - num_attention_heads           : None[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - num_hidden_layers             : None[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-02-22 15:20:52,328] [   DEBUG][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - use_flash_attention           : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - virtual_pp_degree             : 2[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - vocab_size                    : None[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - [0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m - ============================================================[0m
[35m[2024-02-22 15:20:52,329] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - paddle commit id              : 557b888e2c07631a347a1f8b0e06d3cec3250eb1[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - paddlenlp commit id           : edd3d74fc091d1b9ad0df9607336c8fd4d28bb90.dirty[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - data_cache                    : None[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - data_impl                     : mmap[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - input_dir                     : ../data[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-02-22 15:20:52,330] [   DEBUG][0m - share_folder                  : False[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - skip_warmup                   : True[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - split                         : 949,50,1[0m
[35m[2024-02-22 15:20:52,331] [   DEBUG][0m - [0m
[33m[2024-02-22 15:20:52,331] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-02-22 15:20:52,332] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-02-22 15:20:52,332] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-02-22 15:20:52,353] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-02-22 15:20:52,353] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-02-22 15:20:52,544] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-02-22 15:20:52,547] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-02-22 15:20:52,548] [    INFO][0m - Reset vocab size to 32000 for batter amp peformance.[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 4,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "seq_length": 2048,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": true,
  "virtual_pp_degree": 2,
  "vocab_size": 32000
}

Using old dataet (.npy & .npz)
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-02-22 15:20:53,361] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-02-22 15:20:53,391] [    INFO][0m - . How?? Well thats the magic!! Its a bit complicated to explain here. Get the book and read Chapter 9. But it works! Check this out:Amazing isn't it? And the even more amazing thing is that it is not specific to factorial. See thisWoohoo!! On a roll now..You can take any recursive function, and rewrite it in the above style and the Y combinator will make a recursive version of it. How cool is that?</s> Perception.

Computational models of facial attractiveness judgments.

Strikingly, both models produced estimates of facial attractiveness that were indistinguishable from human ratings.

The image factors that the model discovered correspond to two of the main contemporary hypotheses of averageness judgments: facial attractiveness and sexual dimorphism.

This provides novel evidence for the importance of averageness and sexual dimorphism, but not symmetry, in human judgments of facial attractiveness.

Very related: Computer taught to recognize female attractiveness 2008;37(1):126-42.Related ArticlesBronstad PM, Langlois JH, Russell R.We designed two computational models to replicate human facial attractiveness ratings. The primary model used partial least squares (PLS) to identify image factors associated with facial attractiveness from facial images and attractiveness ratings of those images. For comparison we also made a model similar to previous models of facial attractiveness, in that it used manually derived measurements between features as inputs, though we took the additional step of dimensionality reduction via principal component analysis (PCA) and weighting of PCA dimensions via a perceptron.Because PLS extracts a small number of image factors from the facial images that covary with attractiveness ratings of the images, it is possible to determine the information used by the model.In contrast, facial symmetry was not important to the model, and an explicit feature-based measurement of symmetry was not correlated with human judgments of facial attractiveness.</s> The news comes after the Telegraph launched a campaign calling for a fair deal for motorists. This follows a 50 per cent increase in the amount of car-related tax since Labour came to power.</s> Mengenal Gaya Bermain Poker Online Pada Pemain Profesional

Mengenal Gaya Bermain Poker Online Pada Pemain Profesional – Ada beberapa hal yang harus anda ketahui ketika anda bermain poker online ini. Salah satunya adalah mengenal gaya bermain poker online pada setiap pemain yang melakukan taruhan ini. Gaya bermain ini sering dikatakan sebagai playing styles, dimana terdapat banyak sekali gaya yang harus anda ketahui. Berikut […]</s>U.S. troops working the streets of the capital fear one Iraqi weapon more than others -- a copper-plated explosive that can penetrate armor and has proved devastating to Humvees and even capable of severely damaging tanks.

The power of what the military calls an EFP -- for explosively formed penetrator, or projectile -- to spray molten metal balls that punch through the armor on vehicles has some American troops rethinking their tactics. They are asking whether the U.S. should give up its reliance on making constant improvements to vehicle defenses.

Instead, these troops think, it is time to leave the armor behind -- and get out and walk.

“In our area, the biggest threat for us is EFPs. When you are in the vehicles, you are a big target,” said Army Staff Sgt. Cavin Moskwa, 33, of Hawaii, who patrols Baghdad’s Zafraniya neighborhood with the Bravo Battery of the 2nd Battalion, 17th Field Artillery Regiment. “But when you are dismounted... you are a lot safer.”

Advertisement

In the last three days, 15 U.S. troops have been killed in Iraq, nine of them in two powerful roadside bomb blasts. The military does not publicly identify the kind of weapon used in improvised explosive attacks, but the deadly nature of the blasts Wednesday and Thursday suggested that EFPs may have been used.

The deaths brought to 3,545 the total number of U.S. troops killed in the Iraq theater since the March 2003 American-led invasion, the U.S. military said. Hundreds of these troops have been killed by EFPs and other kinds of improvised explosive devices, or IEDs. The Pentagon’s most recent Iraq status report said EFP attacks were at an all-time high.

Foot patrols, of course, are not a fail-safe method. On city streets, snipers remain a threat. And bombs can still kill dismounted troops. But when blasts occur in the middle of a foot patrol, the number of casualties are generally lower because the troops are more spread out.

Before a foot patrol last week through a neighborhood next to Baghdad’s Sadr City district, a private with Alpha Company of the Army’s 1st Battalion, 8th Cavalry Regiment, began complaining about having to walk. But EFPs have claimed the lives of several soldiers in the unit, and Sgt. Leland Kidd, 28, of Gonzales, Texas, said the private should be thankful they were on foot.

Advertisement

“When I walk on my feet, I don’t have to worry about being blown up,” Kidd told the private. “In the vehicle, I have to.”

Top commanders have been encouraging more such units in Baghdad to take just that tack.

A counterinsurgency guidance memo released last week by Army Lt. Gen Raymond T. Odierno, the commander of day-to-day military operations, urges Iraqi and American troops to “get out and walk.”

The memo argues that although Humvees offer protection, they also make units predictable and “insulate us from the Iraqi people we intend to secure.”

Advertisement

The original draft of the memo, written by counterinsurgency expert David Kilcullen, goes further. It notes that EFP attacks on Humvees damage them heavily. “So we gain little in safety, but sacrifice much in effectiveness,” the draft reads.

One reason for the increased number of troops victimized by roadside bombs is that there are more forces in Iraq now, Marine Gen. Peter Pace, chairman of the Joint Chiefs of Staff, said at a Pentagon news conference Thursday. This month, the final additional American combat units arrived in Baghdad, as part of a counterinsurgency strategy announced by President Bush in January that has increased the U.S. military presence in Iraq by 28,500 troops.

“As we’re taking the fight to the enemy with the additional troops, we can expect that there’s going to be tough fighting ahead,” Pace said. “So it is an expectation that this surge is going to result in more contact and therefore more casualties.”

But another reason for the rising death toll is the ability of Iraq’s militants to adapt to new U.S. military tactics.

Advertisement

During the 2003 invasion, most American Humvees were outfitted with flimsy canvas doors. When the first improvised explosive devices made from artillery shells appeared, the military scrambled to put stronger armor on the vehicles. Since then, the military has repeatedly upgraded Humvee armor as militants have made bigger and bigger bombs.

But the small and easily hidden EFPs, which often are powered by C-4 plastic explosives, are not just a more powerful IED. Military personnel experienced with the projectiles say that what makes the weapons so deadly is that they use the Americans’ own armor against them. As the hot copper slug melts through the armor of a Humvee, it transforms the protective plating into shrapnel that sprays into the passenger cabin, they say.

“We joked about going back to canvas doors. That way, unless it hits you directly, you are OK,” said Army Sgt. William Bowman, 31, of Fort Myers, Fla.

But to Moskwa, the staff sergeant from Hawaii, the question of armor is no joke. Moskwa, who served as an Army recruiter in Pasadena before deploying to Iraq, thinks armor on vehicles and body armor on troops are too restrictive, hampering a service member’s ability to move quickly and agilely.

Advertisement

“I would rather go out without any armor or gear,” he said. “If an EFP hits the vehicle, you are dead anyway no[0m
[32m[2024-02-22 15:20:53,391] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-02-22 15:20:53,414] [    INFO][0m - intolerant of large, polluting industrial plants on their doorsteps. Second, American power companies are fearful that they will soon have to pay for one particular pollutant, carbon dioxide, as is starting to happen in other parts of the rich world. Having invested heavily in gas-fired stations, only to find themselves locked into an increasingly expensive fuel, they do not want to make another mistake.

That has opened up a capacity gap and an opportunity for wind and sunlight. The future price of these resources—zero—is known. That certainty has economic value as a hedge, even if the capital cost of wind and solar power stations is, at the moment, higher than that of coal-fired ones.

The reasons for the boom, then, are tangled, and the way they are perceived may change. Global warming, a long-range phenomenon, may not be uppermost in people's minds during an economic downturn. High fuel prices may fall as new sources of supply are exploited to fill rising demand from Asia. Security of supply may improve if hostile governments are replaced by friendly ones and sources become more diversified. But none of the reasons is likely to go away entirely.

Global warming certainly will not. “Peak oil”, if oil means the traditional sort that comes cheaply out of holes in the ground, probably will arrive soon. There is oil aplenty of other sorts (tar sands, liquefied coal and so on), so the stuff is unlikely to run out for a long time yet. But it will get more expensive to produce, putting a floor on the price that is way above today's. And political risk will always be there—particularly for oil, which is so often associated with bad government for the simple reason that its very presence causes bad government in states that do not have strong institutions to curb their politicians.

A prize beyond the dreams of avarice

The market for energy is huge. At present, the world's population consumes about 15 terawatts of power. (A terawatt is 1,000 gigawatts, and a gigawatt is the capacity of the largest sort of coal-fired power station.) That translates into a business worth $6 trillion a year—about a tenth of the world's economic output—according to John Doerr, a venture capitalist who is heavily involved in the industry. And by 2050, power consumption is likely to have risen to 30 terawatts.

Scale is one of the important differences between the coming energy boom, if it materialises, and its recent predecessors—particularly those that relied on information technology, a market measured in mere hundreds of billions. Another difference is that new information technologies tend to be disruptive, forcing the replacement of existing equipment, whereas, say, building wind farms does not force the closure of coal-fired power stations.

For both of these reasons, any transition from an economy based on fossil fuels to one based on renewable, alternative, green energy—call it what you will—is likely to be slow, as similar changes have been in the past (see chart 1). On the other hand, the scale of the market provides opportunities for alternatives to prove themselves at the margin and then move into the mainstream, as is happening with wind power at the moment. And some energy technologies do have the potential to be disruptive. Plug-in cars, for example, could be fuelled with electricity at a price equivalent to 25 cents a litre of petrol. That could shake up the oil, carmaking and electricity industries all in one go.

The innovation lull of the past few decades also provides opportunities for technological leapfrogging. Indeed, it may be that the field of energy gives the not-quite-booms in biotechnology and nanotechnology the industrial applications they need to grow really big, and that the three aspiring booms will thus merge into one.

The possibility of thus recapturing the good times of their youth has brought many well-known members of the “technorati” out of their homes in places like Woodside, California. Energy has become supercool. Elon Musk, who co-founded PayPal, has developed a battery-powered sports car. Larry Page and Sergey Brin, the founders of Google, have started an outfit called Google.org that is searching for a way to make renewable energy truly cheaper than coal (or RE

Vinod Khosla, one of the founders of Sun Microsystems, is turning his considerable skills as a venture capitalist towards renewable energy, as are Robert Metcalfe, who invented the ethernet system used to connect computers together in local networks, and Mr Doerr, who works at Kleiner Perkins Caufield & Byers, one of Silicon Valley's best-known venture-capital firms. Sir Richard Branson, too, is getting in on the act with his Virgin Green Fund.

This renewed interest in energy is bringing forth a raft of ideas, some bright, some batty, that is indeed reminiscent of the dotcom boom. As happened in that boom, most of these ideas will come to naught. But there could just be a PayPal or a Google or a Sun among them.

More traditional companies are also taking an interest. General Electric (GE), a large American engineering firm, already has a thriving wind-turbine business and is gearing up its solar-energy business. The energy researchers at its laboratories in Schenectady, New York, enjoy much of the intellectual freedom associated with start-up firms, combined with a secure supply of money.

Meanwhile, BP and Shell, two of the world's biggest oil companies, are sponsoring both academic researchers and new, small firms with bright ideas, as is DuPont, one of the biggest chemical companies. Not everyone has joined in. Exxon Mobil, the world's largest oil company not in government hands, is conspicuously absent. But in many boardrooms renewables are no longer seen as just a way of keeping environmentalists off companies' backs.

Some people complain that many existing forms of renewable energy rely on subsidies or other forms of special treatment for their viability. On the surface, that is true. Look beneath, though, and the whole energy sector is riddled with subsidies, both explicit and hidden, and costs that are not properly accounted for. Drawing on the work of people like Boyden Gray, a former White House counsel, Mr Woolsey estimates that American oil companies receive preferential treatment from their government worth more than $250 billion a year. And the Intergovernmental Panel on Climate Change (IPCC), a United Nations-appointed group of scientific experts, reckons that fossil fuels should carry a tax of $20-50 for every tonne of carbon dioxide they generate in order to pay for the environmental effects of burning them (hence the fears of the power-generators).

So the subsidies and mandates offered to renewable sources of power such as wind turbines often just level the playing field. It is true that some subsidies amount to unwarranted market-rigging: examples include those handed by cloudy Germany to its solar-power industry and by America to its maize-based ethanol farmers when Brazilian sugar-based ethanol is far cheaper. Others, though, such as a requirement that a certain proportion of electricity be derived from non-fossil-fuel sources, make no attempt to pick particular technological winners. They merely act to stimulate innovation by guaranteeing a market to things that actually work.

If the world were rational, all of these measures would be swept away and replaced by a proper tax on carbon—as is starting to happen in Europe, where the price arrived at by the cap-and-trade system being introduced is close to the IPCC's recommendation. If that occurred, wind-based electricity would already be competitive with fossil fuels and others would be coming close. Failing that, special treatment for alternatives is probably the least bad option—though such measures need to be crafted in ways that favour neither incumbents nor particular ways of doing things, and need to be withdrawn when they are no longer necessary.

The poor world turns greener too

That, at least, is the view from the rich world. But poorer, rapidly developing countries are also taking more of an interest in renewable energy sources, despite assertions to the contrary by some Western politicians and businessmen. It is true that China is building coal-fired power stations at a blazing rate. But it also has a large wind-generation capacity, which is expected to grow by two-thirds this year, and is the world's second-largest manufacturer of solar panels—not to mention having the largest number of solar-heated rooftop hot-water systems in its buildings.

Brazil, meanwhile, has the world's second-largest (just behind America) and most econom[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - ============================================================[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m -          Configuration Arguments        [0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - paddle commit id              : 557b888e2c07631a347a1f8b0e06d3cec3250eb1[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - paddlenlp commit id           : edd3d74fc091d1b9ad0df9607336c8fd4d28bb90.dirty[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-02-22 15:20:53,416] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - amp_master_grad               : True[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - bf16                          : False[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - current_device                : gpu:1[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-02-22 15:20:53,417] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - dataloader_num_workers        : 1[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - decay_steps                   : 10[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - device                        : gpu[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - do_eval                       : True[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - do_export                     : False[0m
[32m[2024-02-22 15:20:53,418] [    INFO][0m - do_predict                    : False[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - do_train                      : True[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - enable_auto_parallel          : True[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - eval_batch_size               : 16[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - eval_steps                    : 1000[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-02-22 15:20:53,419] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - fp16                          : True[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - fused_linear_param_grad_add   : False[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - gradient_accumulation_steps   : 16[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - greater_is_better             : None[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-02-22 15:20:53,420] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - job_schedule_profiler_end     : -1[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - job_schedule_profiler_start   : -1[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - label_names                   : None[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - learning_rate                 : 0.0001[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - local_process_index           : 1[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - local_rank                    : 1[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - log_level                     : -1[0m
[32m[2024-02-22 15:20:53,421] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - logging_dir                   : output/llama_auto_static_dp2sharding2mp2pp2_vpp2/runs/Feb22_15-20-47_c4d082bc6a0b[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - logical_process_index         : 1[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2024-02-22 15:20:53,422] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - max_steps                     : 10[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-02-22 15:20:53,423] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - output_dir                    : output/llama_auto_static_dp2sharding2mp2pp2_vpp2[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - past_index                    : -1[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - per_device_eval_batch_size    : 16[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - per_device_train_batch_size   : 1[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - pipeline_parallel_degree      : 2[0m
[32m[2024-02-22 15:20:53,424] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - pipeline_schedule_mode        : VPP[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - power                         : 1.0[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - process_index                 : 1[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - recompute                     : True[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - refined_ops_patterns          : None[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - run_name                      : output/llama_auto_static_dp2sharding2mp2pp2_vpp2[0m
[32m[2024-02-22 15:20:53,425] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - save_steps                    : 5000[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - save_total_limit              : None[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - scale_loss                    : 1024.0[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - seed                          : 42[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - sharding                      : [][0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-02-22 15:20:53,426] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - should_log                    : True[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - should_save                   : True[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-02-22 15:20:53,427] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - sr                            : 0[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - strategy                      : {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 1024.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': [], 'custom_black_list': [], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': True}","sharding":"{'enable': False, 'stage': 1, 'degree': 8, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': False, 'k_steps': 1, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': 'VPP', 'vpp_degree': 2, 'vpp_seg_method': 'LlamaDecoderLayerAuto', 'micro_batch_size': 1, 'accumulate_steps': 16, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': False}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': True}","sp_optimization":"{'enable': False}",}[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - tensor_parallel_config        : enable_mp_async_allreduce[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-02-22 15:20:53,428] [    INFO][0m - to_static                     : False[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - train_batch_size              : 1[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - virtual_pipeline_seg_method   : LlamaDecoderLayerAuto[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - wandb_api_key                 : None[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - warmup_ratio                  : 0.01[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-02-22 15:20:53,429] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2024-02-22 15:20:53,430] [    INFO][0m - world_size                    : 4[0m
[32m[2024-02-22 15:20:53,430] [    INFO][0m - [0m
[2024-02-22 15:20:53,431] [    INFO] cluster.py:914 - Node Count: 1, Local Device Size: 4, GPU Model: NVIDIA GeForce GTX 1080 Ti, GPU Memory: 11GB, World size: 4, EndPoint: 172.17.0.3:52587.
[2024-02-22 15:20:53,432] [    INFO] engine.py:202 - Distribute training by paddle.distributed.launch
[2024-02-22 15:20:53,432] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
[2024-02-22 15:20:53,432] [ WARNING] fleet.py:272 - The dygraph parallel environment has been initialized.
[2024-02-22 15:20:53,432] [ WARNING] fleet.py:295 - The dygraph hybrid parallel environment has been initialized.
[2024-02-22 15:20:53,433] [    INFO] engine.py:655 - Building model with 'to_static' method.
INFO 2024-02-22 15:20:53,433 helper.py:245] start to build program for mode = train.
/usr/local/lib/python3.9/dist-packages/paddle/base/framework.py:3123: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
/root/.cache/paddle/to_static_tmp/24060/LlamaPretrainingCriterionAuto_forwardkew4n7rd.py:28: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 32000, 32000
  _jst.Call(_jst.Ld(_jst.Ld(warnings).warn))(f'enable_parallel_cross_entropy, the vocab_size should be splited: {_jst.Ld(_jst.Shape(_jst.Ld(prediction_scores))[-1])}, {_jst.Ld(_jst.Ld(_jst.Ld(self).config).vocab_size)}')
WARNING: there are some orphan tensors or ops which are not used in the execution.
Thu Feb 22 15:20:59-INFO: Using Auto VPP
Thu Feb 22 15:20:59-INFO: stage=[0], chunk_id=[0], layer_name=[['LlamaDecoderLayerAuto']]
Thu Feb 22 15:20:59-INFO: start op: [fill_constant]: [[]] [['fill_constant_1.tmp_0']]
Thu Feb 22 15:20:59-INFO: end op: [elementwise_add]: [['tmp_17', 'linear_6.tmp_0']] [['tmp_23']]
Thu Feb 22 15:20:59-INFO: stage=[1], chunk_id=[0], layer_name=[['LlamaDecoderLayerAuto_1']]
Thu Feb 22 15:20:59-INFO: start op: [cast]: [['tmp_23']] [['tmp_24']]
Thu Feb 22 15:20:59-INFO: end op: [elementwise_add]: [['tmp_39', 'linear_13.tmp_0']] [['tmp_45']]
Thu Feb 22 15:20:59-INFO: stage=[0], chunk_id=[1], layer_name=[['LlamaDecoderLayerAuto_2']]
Thu Feb 22 15:20:59-INFO: start op: [cast]: [['tmp_45']] [['tmp_46']]
Thu Feb 22 15:20:59-INFO: end op: [elementwise_add]: [['tmp_61', 'linear_20.tmp_0']] [['tmp_67']]
Thu Feb 22 15:20:59-INFO: stage=[1], chunk_id=[1], layer_name=[['LlamaDecoderLayerAuto_3']]
Thu Feb 22 15:20:59-INFO: start op: [cast]: [['tmp_67']] [['tmp_68']]
Thu Feb 22 15:20:59-INFO: end op: [reduce_mean]: [['tmp_97']] [['mean_9.tmp_0']]
[2024-02-22 15:20:59,904] [    INFO] parallelizer_v2.py:283 - Applying AMP-float16-o2 ...
[2024-02-22 15:21:00,145] [    INFO] auto_parallel_recompute.py:392 - The excluded ops in recompute segments are:
[[], [], []]
[2024-02-22 15:21:03,208] [ WARNING] parallelizer_v2.py:437 - You set mp_optimization.allreduce_matmul_grad_overlapping=True, but you did not set environment variable CUDA_DEVICE_MAX_CONNECTIONS=1, which may leads to performance loss. Try to export CUDA_DEVICE_MAX_CONNECTIONS=1 for better performance.
INFO 2024-02-22 15:21:03,213 allreduce_matmul_grad_overlapping.py:59] overlap matmul_grad and allreduce: OrderedDict([(239, 241), (243, 245), (287, 289), (292, 294), (297, 299), (381, 383), (385, 387), (429, 431), (434, 436), (439, 441)])
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var input_ids : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(16000, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(1, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var range_0.tmp_0 : LOD_TENSOR.shape(2048,).dtype(int64).stop_gradient(True)
    var expand_0.tmp_0 : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(1, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(bool).stop_gradient(True)
    var tmp_0 : LOD_TENSOR.shape(1, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var expand_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_13.tmp_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var tril_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_1 : LOD_TENSOR.shape(0, 2048, 2048).dtype(bool).stop_gradient(True)
    var expand_2.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var bitwise_and_0.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var fill_constant_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_2.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var cast_0.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var elementwise_add_1 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_2 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_3 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_4 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_2.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var where_0.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var tmp_1 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist var eager_tmp_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_4.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_6 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_15 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_46 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_47 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_48 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_4.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_49 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_14.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_15.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist trainable param linear_16.w_0 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    persist var eager_tmp_7 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_4.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_4.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_8 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_0 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_10.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_10.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_12.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_12.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_50 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_51 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_52 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_53 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_54 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_55 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_56 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_57 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_58 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_13.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_59 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_60 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_17.w_0 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_62 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_63 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_64 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_5.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_65 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_18.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_19.w_0 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_66 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    persist trainable param linear_20.w_0 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var labels : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_46.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_47.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_48.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_49.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var squeeze_4.tmp_0.subprog_1 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_4.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_0.subprog_1 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_10.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_10.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_12.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_12.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_50.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_1.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_51.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_52.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_53.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_54.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_1.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_55.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_56.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_57.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_58.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_13.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_59.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_60.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_62.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_63.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_64.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_65.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_2.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_66.subprog_1 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_66@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_20.w_0@GRAD : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    var silu_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_65@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_19.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var linear_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_65@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_18.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var tmp_65@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_64@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_5.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_63@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_62@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_17.w_0@GRAD : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    var transpose_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_60@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_1@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_59@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_58@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_57@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_53@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_54@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_56@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_55@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_50@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_52@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_51@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_16.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_15.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_14.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var tmp_49@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_48@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_4.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_47@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_46@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_2.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_3.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_4.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var squeeze_0.tmp_0.subprog_3 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_0.subprog_3 : LOD_TENSOR.shape(2048, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_6.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_7.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_8.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_9.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_10.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var tmp_11.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_12.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_13.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_14.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_15.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_16.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_19.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_20.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_21.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_22.subprog_3 : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    var silu_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var linear_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 5504).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_1.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 16, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 16, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 16, 128).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_0.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    var rsqrt_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD : LOD_TENSOR.shape(16000, 4096).dtype(float32).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var tmp_45@recv_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@RESHARD_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(16000, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_0.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_3.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_1.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_6.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_4.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_14.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_15.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_16.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_17.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_5.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var linear_18.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_19.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_20.w_0@GRAD@master_grad_fp16 : LOD_TENSOR.shape(5504, 4096).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_squared_l2_norm_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_opt_stack_0.tmp_0 : LOD_TENSOR.shape(19, 1).dtype(float32).stop_gradient(False)
    var opt_opt_sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var learning_rate_1 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var embedding_0.w_0_fp32_master_1 : LOD_TENSOR.shape(16000, 4096).dtype(float32).stop_gradient(True)
    persist var embedding_0.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(16000, 4096).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(16000, 4096).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var embedding_0.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_0.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_1.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_2.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_1 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(True)
    persist var linear_3.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_4.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_5.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_1 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(True)
    persist var linear_6.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_6.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_0.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_1.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_14.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_14.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_15.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(True)
    persist var linear_16.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 2048).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_16.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_1 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(True)
    persist var linear_17.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(2048, 4096).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_18.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_1 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(True)
    persist var linear_19.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096, 5504).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_19.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_1 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(True)
    persist var linear_20.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(5504, 4096).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_4.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_5.w_0_fp32_master_1 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_5.w_0_fp32_master_1_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_5.w_0_fp32_master_1_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_5.w_0_fp32_master_1_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_5.w_0_fp32_master_1_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_5.subprog_3@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5.subprog_3@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var tmp_21.subprog_3@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_21.subprog_3@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var tmp_49.subprog_1@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_49.subprog_1@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_14.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_14.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_15.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_15.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 2048).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var linear_16.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 2048).dtype(float16).stop_gradient(False)
    var linear_16.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 2048).dtype(float16).stop_gradient(False)
    var tmp_65.subprog_1@reshape.out : LOD_TENSOR.shape(2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_65.subprog_1@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_18.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_18.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0@GRAD@reshape.out : LOD_TENSOR.shape(2048, 5504).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0@GRAD@reshape.xshape : LOD_TENSOR.shape(0, 1, 2048, 5504).dtype(float16).stop_gradient(False)
    var linear_19.w_0@GRAD@master_grad_fp16@reshape.out : LOD_TENSOR.shape(4096, 5504).dtype(float16).stop_gradient(False)
    var linear_19.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape : LOD_TENSOR.shape(0, 4096, 5504).dtype(float16).stop_gradient(False)

    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 1, op original_id: 1, process_mesh (annotated): {shape: [1,2,2], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['embedding_0.tmp_0']} = c_embedding(inputs={Ids=['input_ids'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], start_index = 16000, with_quant_attr = False)
    {Out=['embedding_0.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['embedding_0.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 3, op original_id: 3, process_mesh (annotated): {shape: [1,2,2], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_9.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 2048, value = 2048.0, with_quant_attr = False)
    {Out=['fill_constant_11.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1, value = 1.0, with_quant_attr = False)
    {Out=['range_0.tmp_0']} = range(inputs={End=['fill_constant_9.tmp_0'], Start=['fill_constant_7.tmp_0'], Step=['fill_constant_11.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_0.tmp_0']} = expand_v2(inputs={Shape=[], X=['range_0.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [1, 2048], with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['fill_constant_5.tmp_0']}, axes = [1, 2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_0']} = cast(inputs={X=['unsqueeze2_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['expand_1.tmp_0']} = expand_v2(inputs={Shape=[], X=['tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], with_quant_attr = False)
    {Out=['fill_constant_13.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2048, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['tril_0']} = tril_triu(inputs={X=['fill_constant_13.tmp_0']}, diagonal = 0, lower = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_1.tmp_0'], XShape=['unsqueeze2_1.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['tril_0']}, axes = [0, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_2.tmp_0']} = expand_v2(inputs={Shape=[], X=['unsqueeze2_1.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], with_quant_attr = False)
    {Out=['bitwise_and_0.tmp_0']} = bitwise_and(inputs={X=['expand_1.tmp_0'], Y=['expand_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['fill_constant_15.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_17.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = -3.4028234663852886e+38, value = -3.4028234663852886e+38, with_quant_attr = False)
    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['fill_constant_15.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_1.tmp_0']} = fill_any_like(inputs={X=['fill_constant_17.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_2.tmp_0']} = fill_any_like(inputs={X=['bitwise_and_0.tmp_0']}, dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['cast_0.tmp_0']} = cast(inputs={X=['full_like_2.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['cast_1.tmp_0']} = cast(inputs={X=['bitwise_and_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['elementwise_add_0']} = elementwise_add(inputs={X=['full_like_0.tmp_0'], Y=['full_like_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_1']} = elementwise_add(inputs={X=['elementwise_add_0'], Y=['cast_0.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_2']} = elementwise_add(inputs={X=['fill_constant_15.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_3']} = elementwise_add(inputs={X=['fill_constant_17.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_4']} = elementwise_add(inputs={X=['cast_1.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['elementwise_add_4']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['where_0.tmp_0']} = where(inputs={Condition=['cast_2.tmp_0'], X=['elementwise_add_2'], Y=['elementwise_add_3']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1']} = cast(inputs={X=['where_0.tmp_0']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_2']} = cast(inputs={X=['embedding_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 33, op original_id: 33, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_2's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_0.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 34, op original_id: 34, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_2's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['pow_0.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 35, op original_id: 35, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; pow_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_3']} = scale(inputs={ScaleTensor=[], X=['mean_0.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 36, op original_id: 36, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; mean_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_3's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_0.tmp_0']} = rsqrt(inputs={X=['tmp_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 37, op original_id: 37, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_3's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_4']} = elementwise_mul(inputs={X=['rsqrt_0.tmp_0'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 38, op original_id: 38, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; rsqrt_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_4's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_5']} = elementwise_mul(inputs={X=['tmp_4'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 39, op original_id: 39, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_4's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_0.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_5's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_0.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 40, op original_id: 40, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_0.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 41, op original_id: 41, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_1.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 42, op original_id: 42, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_1.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 43, op original_id: 43, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_2.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 44, op original_id: 44, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_2.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 45, op original_id: 45, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_0.tmp_0'], XShape=['squeeze_0.tmp_1']} = squeeze2(inputs={X=['eager_tmp_1']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 46, op original_id: 46, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; eager_tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_1.tmp_0'], XShape=['squeeze_1.tmp_1']} = squeeze2(inputs={X=['eager_tmp_2']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 47, op original_id: 47, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; eager_tmp_2's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_2.tmp_0'], XShape=['unsqueeze2_2.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 48, op original_id: 48, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_0.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_2.tmp_0'], X=['squeeze_0.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 49, op original_id: 49, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; unsqueeze2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_3.tmp_0'], XShape=['unsqueeze2_3.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_0.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 50, op original_id: 50, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; gather_nd_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_4.tmp_0'], XShape=['unsqueeze2_4.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 51, op original_id: 51, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_1.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_4.tmp_0'], X=['squeeze_1.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 52, op original_id: 52, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; unsqueeze2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_5.tmp_0'], XShape=['unsqueeze2_5.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_1.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 53, op original_id: 53, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; gather_nd_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_6']} = elementwise_mul(inputs={X=['reshape2_0.tmp_0'], Y=['unsqueeze2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 54, op original_id: 54, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_6's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 55, op original_id: 55, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 56, op original_id: 56, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_7']} = scale(inputs={ScaleTensor=[], X=['reshape2_0.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 57, op original_id: 57, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_0.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_7's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_0.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_7', 'reshape2_0.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 58, op original_id: 58, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_7's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_8']} = elementwise_mul(inputs={X=['concat_0.tmp_0'], Y=['unsqueeze2_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 59, op original_id: 59, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; concat_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_8's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_9']} = elementwise_add(inputs={X=['tmp_6'], Y=['tmp_8']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 60, op original_id: 60, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_6's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_8's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_9's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_10']} = elementwise_mul(inputs={X=['reshape2_1.tmp_0'], Y=['unsqueeze2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 61, op original_id: 61, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_10's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 62, op original_id: 62, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 63, op original_id: 63, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_11']} = scale(inputs={ScaleTensor=[], X=['reshape2_1.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 64, op original_id: 64, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_1.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_11's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_1.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_11', 'reshape2_1.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 65, op original_id: 65, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_11's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_12']} = elementwise_mul(inputs={X=['concat_1.tmp_0'], Y=['unsqueeze2_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 66, op original_id: 66, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; concat_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_12's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_13']} = elementwise_add(inputs={X=['tmp_10'], Y=['tmp_12']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 67, op original_id: 67, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_10's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_12's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_13's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['tmp_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 68, op original_id: 68, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_9's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['tmp_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 69, op original_id: 69, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_13's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 70, op original_id: 70, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_14']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 71, op original_id: 71, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; transpose_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_14's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['transpose_1.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 72, op original_id: 72, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; transpose_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['tmp_14'], Y=['transpose_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 73, op original_id: 73, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_14's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 74, op original_id: 74, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_15']} = elementwise_add(inputs={X=['matmul_v2_0.tmp_0'], Y=['reshape2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 75, op original_id: 75, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; matmul_v2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_15's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_0.tmp_0']} = cast(inputs={X=['tmp_15']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 76, op original_id: 76, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_15's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_0.tmp_1']} = softmax(inputs={X=['softmax_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 77, op original_id: 77, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; softmax_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_16']} = cast(inputs={X=['softmax_0.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 78, op original_id: 78, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; softmax_0.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_16's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['tmp_16'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 79, op original_id: 79, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_16's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 80, op original_id: 80, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; matmul_v2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_4.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 81, op original_id: 81, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; transpose_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_3.tmp_0']} = matmul_v2(inputs={X=['reshape2_4.tmp_0'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 82, op original_id: 82, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; reshape2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_3.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_3.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_3.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_17']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 83, op original_id: 83, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_17's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_18']} = cast(inputs={X=['tmp_17']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 84, op original_id: 84, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_18's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_1.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_18']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 85, op original_id: 85, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_18's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_1.tmp_0']} = reduce_mean(inputs={X=['pow_1.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 86, op original_id: 86, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; pow_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_19']} = scale(inputs={ScaleTensor=[], X=['mean_1.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 87, op original_id: 87, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; mean_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_19's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_1.tmp_0']} = rsqrt(inputs={X=['tmp_19']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 88, op original_id: 88, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_19's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_20']} = elementwise_mul(inputs={X=['rsqrt_1.tmp_0'], Y=['tmp_17']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 89, op original_id: 89, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; rsqrt_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_20's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_21']} = elementwise_mul(inputs={X=['tmp_20'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 90, op original_id: 90, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_20's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_1.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_21's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_4.tmp_0']} = matmul_v2(inputs={X=['tmp_21'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 91, op original_id: 91, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_21's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_4.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_0.tmp_0']} = silu(inputs={X=['linear_4.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 92, op original_id: 92, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; linear_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_5.tmp_0']} = matmul_v2(inputs={X=['tmp_21'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 93, op original_id: 93, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_21's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_5.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_22']} = elementwise_mul(inputs={X=['silu_0.tmp_0'], Y=['linear_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 94, op original_id: 94, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; silu_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_22's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_6.tmp_0']} = matmul_v2(inputs={X=['tmp_22'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 95, op original_id: 95, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_22's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_6.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_6.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_6.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_23']} = elementwise_add(inputs={X=['tmp_17'], Y=['linear_6.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 96, op original_id: 96, process_mesh (annotated): {shape: [1,2], process_ids: [0,1], dim_names: [dp,mp]}; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_23's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    send_v2(inputs={X=['tmp_23']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_45@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_45@RESHARD_0']} = assign(inputs={X=['tmp_45@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_46']} = cast(inputs={X=['tmp_45@RESHARD_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 161, op original_id: 161, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_45's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_46's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_4.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_46']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 162, op original_id: 162, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_46's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_4.tmp_0']} = reduce_mean(inputs={X=['pow_4.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 163, op original_id: 163, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; pow_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_47']} = scale(inputs={ScaleTensor=[], X=['mean_4.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 164, op original_id: 164, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; mean_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_47's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_4.tmp_0']} = rsqrt(inputs={X=['tmp_47']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 165, op original_id: 165, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_47's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_48']} = elementwise_mul(inputs={X=['rsqrt_4.tmp_0'], Y=['tmp_45@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 166, op original_id: 166, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; rsqrt_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_45's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_48's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_49']} = elementwise_mul(inputs={X=['tmp_48'], Y=['create_parameter_4.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 167, op original_id: 167, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_48's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_4.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_49's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_14.tmp_0']} = matmul_v2(inputs={X=['tmp_49'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 168, op original_id: 168, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_49's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_14.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_10.tmp_0'], XShape=['reshape2_10.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 169, op original_id: 169, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_15.tmp_0']} = matmul_v2(inputs={X=['tmp_49'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 170, op original_id: 170, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_49's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_15.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_11.tmp_0'], XShape=['reshape2_11.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 171, op original_id: 171, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_16.tmp_0']} = matmul_v2(inputs={X=['tmp_49'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 172, op original_id: 172, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_49's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_16.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_12.tmp_0'], XShape=['reshape2_12.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 173, op original_id: 173, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_12.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_4.tmp_0'], XShape=['squeeze_4.tmp_1']} = squeeze2(inputs={X=['eager_tmp_7']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 174, op original_id: 174, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; eager_tmp_7's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_5.tmp_0'], XShape=['squeeze_5.tmp_1']} = squeeze2(inputs={X=['eager_tmp_8']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 175, op original_id: 175, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; eager_tmp_8's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_10.tmp_0'], XShape=['unsqueeze2_10.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 176, op original_id: 176, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_10.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_4.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_10.tmp_0'], X=['squeeze_4.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 177, op original_id: 177, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; unsqueeze2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_11.tmp_0'], XShape=['unsqueeze2_11.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_4.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 178, op original_id: 178, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; gather_nd_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_12.tmp_0'], XShape=['unsqueeze2_12.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 179, op original_id: 179, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_12.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_5.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_12.tmp_0'], X=['squeeze_5.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 180, op original_id: 180, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; unsqueeze2_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_13.tmp_0'], XShape=['unsqueeze2_13.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_5.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 181, op original_id: 181, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; gather_nd_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_50']} = elementwise_mul(inputs={X=['reshape2_10.tmp_0'], Y=['unsqueeze2_11.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 182, op original_id: 182, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_50's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_10.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 183, op original_id: 183, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_10.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 184, op original_id: 184, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_51']} = scale(inputs={ScaleTensor=[], X=['reshape2_10.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 185, op original_id: 185, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_10.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_51's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_4.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_51', 'reshape2_10.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 186, op original_id: 186, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_51's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_52']} = elementwise_mul(inputs={X=['concat_4.tmp_0'], Y=['unsqueeze2_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 187, op original_id: 187, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; concat_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_52's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_53']} = elementwise_add(inputs={X=['tmp_50'], Y=['tmp_52']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 188, op original_id: 188, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_50's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_52's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_53's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_54']} = elementwise_mul(inputs={X=['reshape2_11.tmp_0'], Y=['unsqueeze2_11.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 189, op original_id: 189, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_54's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_11.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 190, op original_id: 190, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_11.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 191, op original_id: 191, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_55']} = scale(inputs={ScaleTensor=[], X=['reshape2_11.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 192, op original_id: 192, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_11.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_55's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_5.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_55', 'reshape2_11.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 193, op original_id: 193, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_55's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_56']} = elementwise_mul(inputs={X=['concat_5.tmp_0'], Y=['unsqueeze2_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 194, op original_id: 194, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; concat_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_56's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_57']} = elementwise_add(inputs={X=['tmp_54'], Y=['tmp_56']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 195, op original_id: 195, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_54's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_56's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_57's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['tmp_53']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 196, op original_id: 196, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_53's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_10.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['tmp_57']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 197, op original_id: 197, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_57's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_11.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_12.tmp_0'], XShape=['transpose_12.tmp_1']} = transpose2(inputs={X=['reshape2_12.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 198, op original_id: 198, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_12.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_58']} = scale(inputs={ScaleTensor=[], X=['transpose_10.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 199, op original_id: 199, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; transpose_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_58's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_13.tmp_0'], XShape=['transpose_13.tmp_1']} = transpose2(inputs={X=['transpose_11.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 200, op original_id: 200, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; transpose_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_13.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_4.tmp_0']} = matmul_v2(inputs={X=['tmp_58'], Y=['transpose_13.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 201, op original_id: 201, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_58's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_13.tmp_0'], XShape=['reshape2_13.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 202, op original_id: 202, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_13.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_59']} = elementwise_add(inputs={X=['matmul_v2_4.tmp_0'], Y=['reshape2_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 203, op original_id: 203, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; matmul_v2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_59's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_2.tmp_0']} = cast(inputs={X=['tmp_59']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 204, op original_id: 204, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_59's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_2.tmp_1']} = softmax(inputs={X=['softmax_2.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 205, op original_id: 205, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; softmax_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_60']} = cast(inputs={X=['softmax_2.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 206, op original_id: 206, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; softmax_2.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_60's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_5.tmp_0']} = matmul_v2(inputs={X=['tmp_60'], Y=['transpose_12.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 207, op original_id: 207, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_60's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_14.tmp_0'], XShape=['transpose_14.tmp_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 208, op original_id: 208, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; matmul_v2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_14.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_14.tmp_0'], XShape=['reshape2_14.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 209, op original_id: 209, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; transpose_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_14.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_17.tmp_0']} = matmul_v2(inputs={X=['reshape2_14.tmp_0'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 210, op original_id: 210, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; reshape2_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_17.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_17.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_17.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_61']} = elementwise_add(inputs={X=['tmp_45@RESHARD_0'], Y=['linear_17.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 211, op original_id: 211, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_45's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_61's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_62']} = cast(inputs={X=['tmp_61']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 212, op original_id: 212, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_61's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_62's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_5.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_62']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 213, op original_id: 213, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_62's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_5.tmp_0']} = reduce_mean(inputs={X=['pow_5.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 214, op original_id: 214, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; pow_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_63']} = scale(inputs={ScaleTensor=[], X=['mean_5.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 215, op original_id: 215, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; mean_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_63's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_5.tmp_0']} = rsqrt(inputs={X=['tmp_63']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 216, op original_id: 216, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_63's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_64']} = elementwise_mul(inputs={X=['rsqrt_5.tmp_0'], Y=['tmp_61']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 217, op original_id: 217, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; rsqrt_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_61's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_64's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_65']} = elementwise_mul(inputs={X=['tmp_64'], Y=['create_parameter_5.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 218, op original_id: 218, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_64's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_5.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_65's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_18.tmp_0']} = matmul_v2(inputs={X=['tmp_65'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 219, op original_id: 219, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_65's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_18.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_18.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_2.tmp_0']} = silu(inputs={X=['linear_18.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 220, op original_id: 220, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; linear_18.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_19.tmp_0']} = matmul_v2(inputs={X=['tmp_65'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 221, op original_id: 221, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_65's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_19.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_19.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_66']} = elementwise_mul(inputs={X=['silu_2.tmp_0'], Y=['linear_19.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 222, op original_id: 222, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; silu_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_19.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_66's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_20.tmp_0']} = matmul_v2(inputs={X=['tmp_66'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 223, op original_id: 223, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_66's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_20.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_20.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_20.tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['linear_20.tmp_0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 0, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_67']} = elementwise_add(inputs={X=['tmp_61'], Y=['linear_20.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 224, op original_id: 224, process_mesh (annotated): {shape: [1,2], process_ids: [2,3], dim_names: [dp,mp]}; tmp_61's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_20.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_67's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    send_v2(inputs={X=['tmp_67']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_46.subprog_1']} = cast(inputs={X=['tmp_45@RESHARD_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_4.tmp_0.subprog_1']} = pow(inputs={FactorTensor=[], X=['tmp_46.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_4.tmp_0.subprog_1']} = reduce_mean(inputs={X=['pow_4.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_47.subprog_1']} = scale(inputs={ScaleTensor=[], X=['mean_4.tmp_0.subprog_1']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_4.tmp_0.subprog_1']} = rsqrt(inputs={X=['tmp_47.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_48.subprog_1']} = elementwise_mul(inputs={X=['rsqrt_4.tmp_0.subprog_1'], Y=['tmp_45@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_49.subprog_1']} = elementwise_mul(inputs={X=['tmp_48.subprog_1'], Y=['create_parameter_4.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_14.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_49.subprog_1'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_10.tmp_0.subprog_1'], XShape=['reshape2_10.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_15.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_49.subprog_1'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_11.tmp_0.subprog_1'], XShape=['reshape2_11.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_15.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_16.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_49.subprog_1'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_12.tmp_0.subprog_1'], XShape=['reshape2_12.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_4.tmp_0.subprog_1'], XShape=['squeeze_4.tmp_1.subprog_1']} = squeeze2(inputs={X=['eager_tmp_7']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_5.tmp_0.subprog_1'], XShape=['squeeze_5.tmp_1.subprog_1']} = squeeze2(inputs={X=['eager_tmp_8']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_10.tmp_0.subprog_1'], XShape=['unsqueeze2_10.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_4.tmp_0.subprog_1']} = gather_nd(inputs={Index=['unsqueeze2_10.tmp_0.subprog_1'], X=['squeeze_4.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_11.tmp_0.subprog_1'], XShape=['unsqueeze2_11.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_4.tmp_0.subprog_1']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_12.tmp_0.subprog_1'], XShape=['unsqueeze2_12.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_5.tmp_0.subprog_1']} = gather_nd(inputs={Index=['unsqueeze2_12.tmp_0.subprog_1'], X=['squeeze_5.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_13.tmp_0.subprog_1'], XShape=['unsqueeze2_13.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_5.tmp_0.subprog_1']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_50.subprog_1']} = elementwise_mul(inputs={X=['reshape2_10.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0_slice_0.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0_slice_1.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_51.subprog_1']} = scale(inputs={ScaleTensor=[], X=['reshape2_10.tmp_0_slice_1.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_4.tmp_0.subprog_1']} = concat(inputs={AxisTensor=[], X=['tmp_51.subprog_1', 'reshape2_10.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_52.subprog_1']} = elementwise_mul(inputs={X=['concat_4.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_53.subprog_1']} = elementwise_add(inputs={X=['tmp_50.subprog_1'], Y=['tmp_52.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_54.subprog_1']} = elementwise_mul(inputs={X=['reshape2_11.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0_slice_0.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0_slice_1.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_55.subprog_1']} = scale(inputs={ScaleTensor=[], X=['reshape2_11.tmp_0_slice_1.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_5.tmp_0.subprog_1']} = concat(inputs={AxisTensor=[], X=['tmp_55.subprog_1', 'reshape2_11.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_56.subprog_1']} = elementwise_mul(inputs={X=['concat_5.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_57.subprog_1']} = elementwise_add(inputs={X=['tmp_54.subprog_1'], Y=['tmp_56.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_10.tmp_0.subprog_1'], XShape=['transpose_10.tmp_1.subprog_1']} = transpose2(inputs={X=['tmp_53.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_11.tmp_0.subprog_1'], XShape=['transpose_11.tmp_1.subprog_1']} = transpose2(inputs={X=['tmp_57.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_12.tmp_0.subprog_1'], XShape=['transpose_12.tmp_1.subprog_1']} = transpose2(inputs={X=['reshape2_12.tmp_0.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_58.subprog_1']} = scale(inputs={ScaleTensor=[], X=['transpose_10.tmp_0.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_13.tmp_0.subprog_1'], XShape=['transpose_13.tmp_1.subprog_1']} = transpose2(inputs={X=['transpose_11.tmp_0.subprog_1']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_4.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_58.subprog_1'], Y=['transpose_13.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_13.tmp_0.subprog_1'], XShape=['reshape2_13.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_59.subprog_1']} = elementwise_add(inputs={X=['matmul_v2_4.tmp_0.subprog_1'], Y=['reshape2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_2.tmp_0.subprog_1']} = cast(inputs={X=['tmp_59.subprog_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_2.tmp_1.subprog_1']} = softmax(inputs={X=['softmax_2.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_60.subprog_1']} = cast(inputs={X=['softmax_2.tmp_1.subprog_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_5.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_60.subprog_1'], Y=['transpose_12.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_14.tmp_0.subprog_1'], XShape=['transpose_14.tmp_1.subprog_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_14.tmp_0.subprog_1'], XShape=['reshape2_14.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_14.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_17.tmp_0.subprog_1']} = matmul_v2(inputs={X=['reshape2_14.tmp_0.subprog_1'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_17.tmp_0.subprog_1']} = c_allreduce_sum(inputs={Cond=[], X=['linear_17.tmp_0.subprog_1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_61.subprog_1']} = elementwise_add(inputs={X=['tmp_45@RESHARD_0'], Y=['linear_17.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_62.subprog_1']} = cast(inputs={X=['tmp_61.subprog_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_5.tmp_0.subprog_1']} = pow(inputs={FactorTensor=[], X=['tmp_62.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_5.tmp_0.subprog_1']} = reduce_mean(inputs={X=['pow_5.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_63.subprog_1']} = scale(inputs={ScaleTensor=[], X=['mean_5.tmp_0.subprog_1']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_5.tmp_0.subprog_1']} = rsqrt(inputs={X=['tmp_63.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_64.subprog_1']} = elementwise_mul(inputs={X=['rsqrt_5.tmp_0.subprog_1'], Y=['tmp_61.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_65.subprog_1']} = elementwise_mul(inputs={X=['tmp_64.subprog_1'], Y=['create_parameter_5.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_18.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_65.subprog_1'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_2.tmp_0.subprog_1']} = silu(inputs={X=['linear_18.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_19.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_65.subprog_1'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_66.subprog_1']} = elementwise_mul(inputs={X=['silu_2.tmp_0.subprog_1'], Y=['linear_19.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_20.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_66.subprog_1'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_20.tmp_0.subprog_1']} = c_allreduce_sum(inputs={Cond=[], X=['linear_20.tmp_0.subprog_1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_67@GRAD']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {X@GRAD=['tmp_61@GRAD@RENAME@block0@0'], Y@GRAD=['linear_20.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_67@GRAD'], X=['tmp_61.subprog_1'], Y=['linear_20.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_66@GRAD'], Y@GRAD=['linear_20.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_20.tmp_0@GRAD'], X=['tmp_66.subprog_1'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD']} = cast(inputs={X=['linear_20.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['silu_2.tmp_0@GRAD'], Y@GRAD=['linear_19.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_66@GRAD'], X=['silu_2.tmp_0.subprog_1'], Y=['linear_19.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_65@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_19.tmp_0@GRAD'], Y=['linear_19.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_65@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_65@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_65.subprog_1@reshape.out'], XShape=['tmp_65.subprog_1@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_65.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_19.tmp_0@GRAD@reshape.out'], XShape=['linear_19.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_19.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_65.subprog_1@reshape.out'], Y=['linear_19.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD@master_grad_fp16'], XShape=['linear_19.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_19.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD']} = cast(inputs={X=['linear_19.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['linear_18.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_2.tmp_0.subprog_1'], Out@GRAD=['silu_2.tmp_0@GRAD'], X=['linear_18.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_65@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_18.tmp_0@GRAD'], Y=['linear_18.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_65@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_65@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_65.subprog_1@reshape.out'], XShape=['tmp_65.subprog_1@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_65.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_18.tmp_0@GRAD@reshape.out'], XShape=['linear_18.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_18.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_65.subprog_1@reshape.out'], Y=['linear_18.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD@master_grad_fp16'], XShape=['linear_18.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_18.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD']} = cast(inputs={X=['linear_18.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_65@GRAD']} = sum(inputs={X=['tmp_65@GRAD@RENAME@block0@0', 'tmp_65@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_64@GRAD'], Y@GRAD=['create_parameter_5.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_65@GRAD'], X=['tmp_64.subprog_1'], Y=['create_parameter_5.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_5.w_0@GRAD']} = cast(inputs={X=['create_parameter_5.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_5.tmp_0@GRAD'], Y@GRAD=['tmp_61@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_64@GRAD'], X=['rsqrt_5.tmp_0.subprog_1'], Y=['tmp_61.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_63@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_5.tmp_0.subprog_1'], Out@GRAD=['rsqrt_5.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_63@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_5.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_5.tmp_0@GRAD'], X=['pow_5.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_62@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_5.tmp_0@GRAD'], X=['tmp_62.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_61@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_62@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_61@GRAD']} = sum(inputs={X=['tmp_61@GRAD@RENAME@block0@0', 'tmp_61@GRAD@RENAME@block0@1', 'tmp_61@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_45@GRAD@RENAME@block0@0'], Y@GRAD=['linear_17.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_61@GRAD'], X=['tmp_45@RESHARD_0'], Y=['linear_17.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_14.tmp_0@GRAD'], Y@GRAD=['linear_17.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_17.tmp_0@GRAD'], X=['reshape2_14.tmp_0.subprog_1'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD']} = cast(inputs={X=['linear_17.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['transpose_14.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_14.tmp_0@GRAD'], XShape=['reshape2_14.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_14.tmp_0@GRAD'], XShape=['transpose_14.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_60@GRAD'], Y@GRAD=['transpose_12.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_5.tmp_0@GRAD'], X=['tmp_60.subprog_1'], Y=['transpose_12.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_2.tmp_1@GRAD']} = cast(inputs={X=['tmp_60@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_2.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_2.tmp_1.subprog_1'], Out@GRAD=['softmax_2.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_59@GRAD']} = cast(inputs={X=['softmax_2.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_4.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_59@GRAD'], X=['matmul_v2_4.tmp_0.subprog_1'], Y=['reshape2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_58@GRAD'], Y@GRAD=['transpose_13.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_4.tmp_0@GRAD'], X=['tmp_58.subprog_1'], Y=['transpose_13.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_13.tmp_0@GRAD'], XShape=['transpose_13.tmp_1.subprog_1']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_10.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_58@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_12.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_12.tmp_0@GRAD'], XShape=['transpose_12.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_57@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_53@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_54@GRAD'], Y@GRAD=['tmp_56@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_57@GRAD'], X=['tmp_54.subprog_1'], Y=['tmp_56.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_5.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_56@GRAD'], X=['concat_5.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_55@GRAD', 'reshape2_11.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_5.tmp_0@GRAD'], X=['tmp_55.subprog_1', 'reshape2_11.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_55@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_11.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], Out@GRAD=['reshape2_11.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_11.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], Out@GRAD=['reshape2_11.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_11.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_54@GRAD'], X=['reshape2_11.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_50@GRAD'], Y@GRAD=['tmp_52@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_53@GRAD'], X=['tmp_50.subprog_1'], Y=['tmp_52.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_4.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_52@GRAD'], X=['concat_4.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_51@GRAD', 'reshape2_10.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_4.tmp_0@GRAD'], X=['tmp_51.subprog_1', 'reshape2_10.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_51@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_10.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], Out@GRAD=['reshape2_10.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_10.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], Out@GRAD=['reshape2_10.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_10.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_50@GRAD'], X=['reshape2_10.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_16.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_12.tmp_0@GRAD'], XShape=['reshape2_12.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_49@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_16.tmp_0@GRAD'], Y=['linear_16.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_49@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_49@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_49.subprog_1@reshape.out'], XShape=['tmp_49.subprog_1@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_49.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_16.tmp_0@GRAD@reshape.out'], XShape=['linear_16.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_49.subprog_1@reshape.out'], Y=['linear_16.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD@master_grad_fp16'], XShape=['linear_16.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD']} = cast(inputs={X=['linear_16.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_11.tmp_0@GRAD']} = sum(inputs={X=['reshape2_11.tmp_0@GRAD@RENAME@block0@0', 'reshape2_11.tmp_0@GRAD@RENAME@block0@1', 'reshape2_11.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_15.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_49@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_15.tmp_0@GRAD'], Y=['linear_15.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_49@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_49@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_49.subprog_1@reshape.out'], XShape=['tmp_49.subprog_1@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_49.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_15.tmp_0@GRAD@reshape.out'], XShape=['linear_15.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_15.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_49.subprog_1@reshape.out'], Y=['linear_15.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD@master_grad_fp16'], XShape=['linear_15.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_15.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD']} = cast(inputs={X=['linear_15.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_10.tmp_0@GRAD']} = sum(inputs={X=['reshape2_10.tmp_0@GRAD@RENAME@block0@0', 'reshape2_10.tmp_0@GRAD@RENAME@block0@1', 'reshape2_10.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_14.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_10.tmp_0@GRAD'], XShape=['reshape2_10.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_49@GRAD@RENAME@block0@2']} = matmul_v2(inputs={X=['linear_14.tmp_0@GRAD'], Y=['linear_14.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_49@GRAD@RENAME@block0@2']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_49@GRAD@RENAME@block0@2']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_49.subprog_1@reshape.out'], XShape=['tmp_49.subprog_1@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_49.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_14.tmp_0@GRAD@reshape.out'], XShape=['linear_14.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_49.subprog_1@reshape.out'], Y=['linear_14.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD@master_grad_fp16'], XShape=['linear_14.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD']} = cast(inputs={X=['linear_14.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_49@GRAD']} = sum(inputs={X=['tmp_49@GRAD@RENAME@block0@0', 'tmp_49@GRAD@RENAME@block0@1', 'tmp_49@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_48@GRAD'], Y@GRAD=['create_parameter_4.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_49@GRAD'], X=['tmp_48.subprog_1'], Y=['create_parameter_4.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_4.w_0@GRAD']} = cast(inputs={X=['create_parameter_4.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_4.tmp_0@GRAD'], Y@GRAD=['tmp_45@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_48@GRAD'], X=['rsqrt_4.tmp_0.subprog_1'], Y=['tmp_45@RESHARD_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_47@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_4.tmp_0.subprog_1'], Out@GRAD=['rsqrt_4.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_47@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_4.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_4.tmp_0@GRAD'], X=['pow_4.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_46@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_4.tmp_0@GRAD'], X=['tmp_46.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_45@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_46@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_45@GRAD']} = sum(inputs={X=['tmp_45@GRAD@RENAME@block0@0', 'tmp_45@GRAD@RENAME@block0@1', 'tmp_45@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_45@GRAD']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], peer = 1, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_2.subprog_3']} = cast(inputs={X=['embedding_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_0.tmp_0.subprog_3']} = pow(inputs={FactorTensor=[], X=['tmp_2.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0.subprog_3']} = reduce_mean(inputs={X=['pow_0.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_3.subprog_3']} = scale(inputs={ScaleTensor=[], X=['mean_0.tmp_0.subprog_3']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_0.tmp_0.subprog_3']} = rsqrt(inputs={X=['tmp_3.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_4.subprog_3']} = elementwise_mul(inputs={X=['rsqrt_0.tmp_0.subprog_3'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_5.subprog_3']} = elementwise_mul(inputs={X=['tmp_4.subprog_3'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_0.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_0.tmp_0.subprog_3'], XShape=['reshape2_0.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_1.tmp_0.subprog_3'], XShape=['reshape2_1.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_2.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_2.tmp_0.subprog_3'], XShape=['reshape2_2.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 16, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_0.tmp_0.subprog_3'], XShape=['squeeze_0.tmp_1.subprog_3']} = squeeze2(inputs={X=['eager_tmp_1']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_1.tmp_0.subprog_3'], XShape=['squeeze_1.tmp_1.subprog_3']} = squeeze2(inputs={X=['eager_tmp_2']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_2.tmp_0.subprog_3'], XShape=['unsqueeze2_2.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_0.tmp_0.subprog_3']} = gather_nd(inputs={Index=['unsqueeze2_2.tmp_0.subprog_3'], X=['squeeze_0.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_3.tmp_0.subprog_3'], XShape=['unsqueeze2_3.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_0.tmp_0.subprog_3']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_4.tmp_0.subprog_3'], XShape=['unsqueeze2_4.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['expand_0.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_1.tmp_0.subprog_3']} = gather_nd(inputs={Index=['unsqueeze2_4.tmp_0.subprog_3'], X=['squeeze_1.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_5.tmp_0.subprog_3'], XShape=['unsqueeze2_5.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_1.tmp_0.subprog_3']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_6.subprog_3']} = elementwise_mul(inputs={X=['reshape2_0.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_0.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_1.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_7.subprog_3']} = scale(inputs={ScaleTensor=[], X=['reshape2_0.tmp_0_slice_1.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_0.tmp_0.subprog_3']} = concat(inputs={AxisTensor=[], X=['tmp_7.subprog_3', 'reshape2_0.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_8.subprog_3']} = elementwise_mul(inputs={X=['concat_0.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_9.subprog_3']} = elementwise_add(inputs={X=['tmp_6.subprog_3'], Y=['tmp_8.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_10.subprog_3']} = elementwise_mul(inputs={X=['reshape2_1.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_0.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_1.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_11.subprog_3']} = scale(inputs={ScaleTensor=[], X=['reshape2_1.tmp_0_slice_1.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_1.tmp_0.subprog_3']} = concat(inputs={AxisTensor=[], X=['tmp_11.subprog_3', 'reshape2_1.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_12.subprog_3']} = elementwise_mul(inputs={X=['concat_1.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_13.subprog_3']} = elementwise_add(inputs={X=['tmp_10.subprog_3'], Y=['tmp_12.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0.subprog_3'], XShape=['transpose_0.tmp_1.subprog_3']} = transpose2(inputs={X=['tmp_9.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_1.tmp_0.subprog_3'], XShape=['transpose_1.tmp_1.subprog_3']} = transpose2(inputs={X=['tmp_13.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_2.tmp_0.subprog_3'], XShape=['transpose_2.tmp_1.subprog_3']} = transpose2(inputs={X=['reshape2_2.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_14.subprog_3']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_3.tmp_0.subprog_3'], XShape=['transpose_3.tmp_1.subprog_3']} = transpose2(inputs={X=['transpose_1.tmp_0.subprog_3']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_14.subprog_3'], Y=['transpose_3.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_3.tmp_0.subprog_3'], XShape=['reshape2_3.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_15.subprog_3']} = elementwise_add(inputs={X=['matmul_v2_0.tmp_0.subprog_3'], Y=['reshape2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_0.tmp_0.subprog_3']} = cast(inputs={X=['tmp_15.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_0.tmp_1.subprog_3']} = softmax(inputs={X=['softmax_0.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_16.subprog_3']} = cast(inputs={X=['softmax_0.tmp_1.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_1.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_16.subprog_3'], Y=['transpose_2.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_4.tmp_0.subprog_3'], XShape=['transpose_4.tmp_1.subprog_3']} = transpose2(inputs={X=['matmul_v2_1.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_4.tmp_0.subprog_3'], XShape=['reshape2_4.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_4.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_3.tmp_0.subprog_3']} = matmul_v2(inputs={X=['reshape2_4.tmp_0.subprog_3'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_3.tmp_0.subprog_3']} = c_allreduce_sum(inputs={Cond=[], X=['linear_3.tmp_0.subprog_3']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_17.subprog_3']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_18.subprog_3']} = cast(inputs={X=['tmp_17.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_1.tmp_0.subprog_3']} = pow(inputs={FactorTensor=[], X=['tmp_18.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_1.tmp_0.subprog_3']} = reduce_mean(inputs={X=['pow_1.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_19.subprog_3']} = scale(inputs={ScaleTensor=[], X=['mean_1.tmp_0.subprog_3']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_1.tmp_0.subprog_3']} = rsqrt(inputs={X=['tmp_19.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_20.subprog_3']} = elementwise_mul(inputs={X=['rsqrt_1.tmp_0.subprog_3'], Y=['tmp_17.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21.subprog_3']} = elementwise_mul(inputs={X=['tmp_20.subprog_3'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_4.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_21.subprog_3'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_0.tmp_0.subprog_3']} = silu(inputs={X=['linear_4.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_5.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_21.subprog_3'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_22.subprog_3']} = elementwise_mul(inputs={X=['silu_0.tmp_0.subprog_3'], Y=['linear_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_6.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_22.subprog_3'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_6.tmp_0.subprog_3']} = c_allreduce_sum(inputs={Cond=[], X=['linear_6.tmp_0.subprog_3']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_23@GRAD']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {X@GRAD=['tmp_17@GRAD@RENAME@block0@0'], Y@GRAD=['linear_6.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['tmp_17.subprog_3'], Y=['linear_6.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_22@GRAD'], Y@GRAD=['linear_6.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_6.tmp_0@GRAD'], X=['tmp_22.subprog_3'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD']} = cast(inputs={X=['linear_6.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['silu_0.tmp_0@GRAD'], Y@GRAD=['linear_5.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_22@GRAD'], X=['silu_0.tmp_0.subprog_3'], Y=['linear_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_5.tmp_0@GRAD'], Y=['linear_5.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_21@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_21@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_21.subprog_3@reshape.out'], XShape=['tmp_21.subprog_3@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_21.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_5.tmp_0@GRAD@reshape.out'], XShape=['linear_5.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_5.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_21.subprog_3@reshape.out'], Y=['linear_5.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@master_grad_fp16'], XShape=['linear_5.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_5.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD']} = cast(inputs={X=['linear_5.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['linear_4.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_0.tmp_0.subprog_3'], Out@GRAD=['silu_0.tmp_0@GRAD'], X=['linear_4.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_4.tmp_0@GRAD'], Y=['linear_4.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_21@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_21@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_21.subprog_3@reshape.out'], XShape=['tmp_21.subprog_3@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_21.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_4.tmp_0@GRAD@reshape.out'], XShape=['linear_4.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_4.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_21.subprog_3@reshape.out'], Y=['linear_4.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@master_grad_fp16'], XShape=['linear_4.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_4.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 5504], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD']} = cast(inputs={X=['linear_4.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_21@GRAD']} = sum(inputs={X=['tmp_21@GRAD@RENAME@block0@0', 'tmp_21@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_20@GRAD'], Y@GRAD=['create_parameter_1.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['tmp_20.subprog_3'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD']} = cast(inputs={X=['create_parameter_1.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_1.tmp_0@GRAD'], Y@GRAD=['tmp_17@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['rsqrt_1.tmp_0.subprog_3'], Y=['tmp_17.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_19@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_1.tmp_0.subprog_3'], Out@GRAD=['rsqrt_1.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_1.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_19@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_1.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_1.tmp_0@GRAD'], X=['pow_1.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_18@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_1.tmp_0@GRAD'], X=['tmp_18.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_17@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_18@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_17@GRAD']} = sum(inputs={X=['tmp_17@GRAD@RENAME@block0@0', 'tmp_17@GRAD@RENAME@block0@1', 'tmp_17@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['embedding_0.tmp_0@GRAD@RENAME@block0@0'], Y@GRAD=['linear_3.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_17@GRAD'], X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD'], Y@GRAD=['linear_3.w_0@GRAD@master_grad_fp16']} = matmul_v2_grad(inputs={Out@GRAD=['linear_3.tmp_0@GRAD'], X=['reshape2_4.tmp_0.subprog_3'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD@master_grad_fp16'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD']} = cast(inputs={X=['linear_3.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['transpose_4.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_16@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['tmp_16.subprog_3'], Y=['transpose_2.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_0.tmp_1@GRAD']} = cast(inputs={X=['tmp_16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_1.subprog_3'], Out@GRAD=['softmax_0.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_15@GRAD']} = cast(inputs={X=['softmax_0.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_15@GRAD'], X=['matmul_v2_0.tmp_0.subprog_3'], Y=['reshape2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_14@GRAD'], Y@GRAD=['transpose_3.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['tmp_14.subprog_3'], Y=['transpose_3.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1.subprog_3']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_14@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_13@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_9@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_10@GRAD'], Y@GRAD=['tmp_12@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['tmp_10.subprog_3'], Y=['tmp_12.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['concat_1.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_11@GRAD', 'reshape2_1.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_1.tmp_0@GRAD'], X=['tmp_11.subprog_3', 'reshape2_1.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_11@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], Out@GRAD=['reshape2_1.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], Out@GRAD=['reshape2_1.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['reshape2_1.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_6@GRAD'], Y@GRAD=['tmp_8@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['tmp_6.subprog_3'], Y=['tmp_8.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['concat_0.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_7@GRAD', 'reshape2_0.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_0.tmp_0@GRAD'], X=['tmp_7.subprog_3', 'reshape2_0.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_7@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], Out@GRAD=['reshape2_0.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], Out@GRAD=['reshape2_0.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['reshape2_0.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_2.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_5@GRAD@RENAME@block0@0']} = matmul_v2(inputs={X=['linear_2.tmp_0@GRAD'], Y=['linear_2.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_5@GRAD@RENAME@block0@0']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_5@GRAD@RENAME@block0@0']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_5.subprog_3@reshape.out'], XShape=['tmp_5.subprog_3@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_5.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_2.tmp_0@GRAD@reshape.out'], XShape=['linear_2.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_5.subprog_3@reshape.out'], Y=['linear_2.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@master_grad_fp16'], XShape=['linear_2.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD']} = cast(inputs={X=['linear_2.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_1.tmp_0@GRAD']} = sum(inputs={X=['reshape2_1.tmp_0@GRAD@RENAME@block0@0', 'reshape2_1.tmp_0@GRAD@RENAME@block0@1', 'reshape2_1.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_1.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_5@GRAD@RENAME@block0@1']} = matmul_v2(inputs={X=['linear_1.tmp_0@GRAD'], Y=['linear_1.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_5@GRAD@RENAME@block0@1']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_5@GRAD@RENAME@block0@1']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_5.subprog_3@reshape.out'], XShape=['tmp_5.subprog_3@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_5.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.tmp_0@GRAD@reshape.out'], XShape=['linear_1.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_5.subprog_3@reshape.out'], Y=['linear_1.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@master_grad_fp16'], XShape=['linear_1.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD']} = cast(inputs={X=['linear_1.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['reshape2_0.tmp_0@GRAD']} = sum(inputs={X=['reshape2_0.tmp_0@GRAD@RENAME@block0@0', 'reshape2_0.tmp_0@GRAD@RENAME@block0@1', 'reshape2_0.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_0.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_5@GRAD@RENAME@block0@2']} = matmul_v2(inputs={X=['linear_0.tmp_0@GRAD'], Y=['linear_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, with_quant_attr = False)
    {Out=['tmp_5@GRAD@RENAME@block0@2']} = c_allreduce_sum(inputs={Cond=[], X=['tmp_5@GRAD@RENAME@block0@2']}, op_device = , op_namescope = /auto_parallel/tensor_parallel, op_role = 1, op_role_var = [], ring_id = 26, use_calc_stream = True, use_model_parallel = True, with_quant_attr = False)
    {Out=['tmp_5.subprog_3@reshape.out'], XShape=['tmp_5.subprog_3@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['tmp_5.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_0.tmp_0@GRAD@reshape.out'], XShape=['linear_0.tmp_0@GRAD@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0@GRAD']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@master_grad_fp16@reshape.out']} = matmul_v2(inputs={X=['tmp_5.subprog_3@reshape.out'], Y=['linear_0.tmp_0@GRAD@reshape.out']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = True, trans_y = False, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@master_grad_fp16'], XShape=['linear_0.w_0@GRAD@master_grad_fp16@reshape.out@reshape.xshape']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.w_0@GRAD@master_grad_fp16@reshape.out']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [4096, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD']} = cast(inputs={X=['linear_0.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['tmp_5@GRAD']} = sum(inputs={X=['tmp_5@GRAD@RENAME@block0@0', 'tmp_5@GRAD@RENAME@block0@1', 'tmp_5@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_4@GRAD'], Y@GRAD=['create_parameter_0.w_0@GRAD@master_grad_fp16']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['tmp_4.subprog_3'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['create_parameter_0.w_0', 'create_parameter_0.w_0@GRAD@master_grad_fp16'], with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD']} = cast(inputs={X=['create_parameter_0.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {X@GRAD=['rsqrt_0.tmp_0@GRAD'], Y@GRAD=['embedding_0.tmp_0@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['rsqrt_0.tmp_0.subprog_3'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_3@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_0.tmp_0.subprog_3'], Out@GRAD=['rsqrt_0.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_3@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_0.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['pow_0.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_2@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_0.tmp_0@GRAD'], X=['tmp_2.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_2@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD']} = sum(inputs={X=['embedding_0.tmp_0@GRAD@RENAME@block0@0', 'embedding_0.tmp_0@GRAD@RENAME@block0@1', 'embedding_0.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {W@GRAD=['embedding_0.w_0@GRAD@master_grad_fp16']} = c_embedding_grad(inputs={Ids=['input_ids'], Out@GRAD=['embedding_0.tmp_0@GRAD'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], start_index = 16000, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD']} = cast(inputs={X=['embedding_0.w_0@GRAD@master_grad_fp16']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['embedding_0.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_1.w_0@GRAD', 'linear_2.w_0@GRAD', 'linear_3.w_0@GRAD', 'linear_4.w_0@GRAD', 'linear_5.w_0@GRAD', 'linear_6.w_0@GRAD', 'create_parameter_0.w_0@GRAD', 'create_parameter_1.w_0@GRAD', 'linear_14.w_0@GRAD', 'linear_15.w_0@GRAD', 'linear_16.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_19.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'create_parameter_5.w_0@GRAD']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['embedding_0.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_1.w_0@GRAD', 'linear_2.w_0@GRAD', 'linear_3.w_0@GRAD', 'linear_4.w_0@GRAD', 'linear_5.w_0@GRAD', 'linear_6.w_0@GRAD', 'create_parameter_0.w_0@GRAD', 'create_parameter_1.w_0@GRAD', 'linear_14.w_0@GRAD', 'linear_15.w_0@GRAD', 'linear_16.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_19.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'create_parameter_5.w_0@GRAD']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchorization, op_role = 2, op_role_var = [], ring_id = 28, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['embedding_0.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_1.w_0@GRAD', 'linear_2.w_0@GRAD', 'linear_3.w_0@GRAD', 'linear_4.w_0@GRAD', 'linear_5.w_0@GRAD', 'linear_6.w_0@GRAD', 'create_parameter_0.w_0@GRAD', 'create_parameter_1.w_0@GRAD', 'linear_14.w_0@GRAD', 'linear_15.w_0@GRAD', 'linear_16.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_19.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'create_parameter_5.w_0@GRAD'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['embedding_0.w_0@GRAD', 'linear_0.w_0@GRAD', 'linear_1.w_0@GRAD', 'linear_2.w_0@GRAD', 'linear_3.w_0@GRAD', 'linear_4.w_0@GRAD', 'linear_5.w_0@GRAD', 'linear_6.w_0@GRAD', 'create_parameter_0.w_0@GRAD', 'create_parameter_1.w_0@GRAD', 'linear_14.w_0@GRAD', 'linear_15.w_0@GRAD', 'linear_16.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_19.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'create_parameter_5.w_0@GRAD']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_0.tmp_0']} = squared_l2_norm(inputs={X=['embedding_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_1.tmp_0']} = squared_l2_norm(inputs={X=['linear_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_2.tmp_0']} = squared_l2_norm(inputs={X=['linear_1.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_3.tmp_0']} = squared_l2_norm(inputs={X=['linear_2.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_4.tmp_0']} = squared_l2_norm(inputs={X=['linear_3.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_5.tmp_0']} = squared_l2_norm(inputs={X=['linear_4.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_6.tmp_0']} = squared_l2_norm(inputs={X=['linear_5.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_7.tmp_0']} = squared_l2_norm(inputs={X=['linear_6.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_8.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_0.w_0', 'create_parameter_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_9.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_1.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_10.tmp_0']} = squared_l2_norm(inputs={X=['linear_14.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_11.tmp_0']} = squared_l2_norm(inputs={X=['linear_15.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_12.tmp_0']} = squared_l2_norm(inputs={X=['linear_16.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_13.tmp_0']} = squared_l2_norm(inputs={X=['linear_17.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_14.tmp_0']} = squared_l2_norm(inputs={X=['linear_18.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_15.tmp_0']} = squared_l2_norm(inputs={X=['linear_19.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_16.tmp_0']} = squared_l2_norm(inputs={X=['linear_20.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_17.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_4.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_18.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_5.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Y=['opt_opt_stack_0.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_0.tmp_0', 'opt_opt_squared_l2_norm_1.tmp_0', 'opt_opt_squared_l2_norm_2.tmp_0', 'opt_opt_squared_l2_norm_3.tmp_0', 'opt_opt_squared_l2_norm_4.tmp_0', 'opt_opt_squared_l2_norm_5.tmp_0', 'opt_opt_squared_l2_norm_6.tmp_0', 'opt_opt_squared_l2_norm_7.tmp_0', 'opt_opt_squared_l2_norm_8.tmp_0', 'opt_opt_squared_l2_norm_9.tmp_0', 'opt_opt_squared_l2_norm_10.tmp_0', 'opt_opt_squared_l2_norm_11.tmp_0', 'opt_opt_squared_l2_norm_12.tmp_0', 'opt_opt_squared_l2_norm_13.tmp_0', 'opt_opt_squared_l2_norm_14.tmp_0', 'opt_opt_squared_l2_norm_15.tmp_0', 'opt_opt_squared_l2_norm_16.tmp_0', 'opt_opt_squared_l2_norm_17.tmp_0', 'opt_opt_squared_l2_norm_18.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_sum_0.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_0.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_opt_sum_0.tmp_0']}, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD']} = elementwise_mul(inputs={X=['embedding_0.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_0.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_1.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_2.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_3.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_4.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_5.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_6.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_0.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_0.w_0', 'create_parameter_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_1.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_14.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_15.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_16.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_17.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_18.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_19.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_20.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_4.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_4.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_5.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_5.w_0@GRAD'], Y=['opt_elementwise_div_0']}, axis = -1, op_device = , op_namescope = /gradient_clip_1/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Beta1PowOut=['embedding_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['embedding_0.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['embedding_0.w_0_fp32_master_1'], Moment1Out=['embedding_0.w_0_fp32_master_1_moment1_0'], Moment2Out=['embedding_0.w_0_fp32_master_1_moment2_0'], ParamOut=['embedding_0.w_0']} = adamw(inputs={Beta1Pow=['embedding_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['embedding_0.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['embedding_0.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['embedding_0.w_0_fp32_master_1'], Moment1=['embedding_0.w_0_fp32_master_1_moment1_0'], Moment2=['embedding_0.w_0_fp32_master_1_moment2_0'], Param=['embedding_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_39/, op_role = 2, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_0.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_0.w_0_fp32_master_1'], Moment1Out=['linear_0.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_0.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_0.w_0']} = adamw(inputs={Beta1Pow=['linear_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_0.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_0.w_0_fp32_master_1'], Moment1=['linear_0.w_0_fp32_master_1_moment1_0'], Moment2=['linear_0.w_0_fp32_master_1_moment2_0'], Param=['linear_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_40/, op_role = 2, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_1.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_1.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_1.w_0_fp32_master_1'], Moment1Out=['linear_1.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_1.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_1.w_0']} = adamw(inputs={Beta1Pow=['linear_1.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_1.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_1.w_0_fp32_master_1'], Moment1=['linear_1.w_0_fp32_master_1_moment1_0'], Moment2=['linear_1.w_0_fp32_master_1_moment2_0'], Param=['linear_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_41/, op_role = 2, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_2.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_2.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_2.w_0_fp32_master_1'], Moment1Out=['linear_2.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_2.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_2.w_0']} = adamw(inputs={Beta1Pow=['linear_2.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_2.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_2.w_0_fp32_master_1'], Moment1=['linear_2.w_0_fp32_master_1_moment1_0'], Moment2=['linear_2.w_0_fp32_master_1_moment2_0'], Param=['linear_2.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_42/, op_role = 2, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_3.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_3.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_3.w_0_fp32_master_1'], Moment1Out=['linear_3.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_3.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_3.w_0']} = adamw(inputs={Beta1Pow=['linear_3.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_3.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_3.w_0_fp32_master_1'], Moment1=['linear_3.w_0_fp32_master_1_moment1_0'], Moment2=['linear_3.w_0_fp32_master_1_moment2_0'], Param=['linear_3.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_43/, op_role = 2, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_4.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_4.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_4.w_0_fp32_master_1'], Moment1Out=['linear_4.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_4.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_4.w_0']} = adamw(inputs={Beta1Pow=['linear_4.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_4.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_4.w_0_fp32_master_1'], Moment1=['linear_4.w_0_fp32_master_1_moment1_0'], Moment2=['linear_4.w_0_fp32_master_1_moment2_0'], Param=['linear_4.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_44/, op_role = 2, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_5.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_5.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_5.w_0_fp32_master_1'], Moment1Out=['linear_5.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_5.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_5.w_0']} = adamw(inputs={Beta1Pow=['linear_5.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_5.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_5.w_0_fp32_master_1'], Moment1=['linear_5.w_0_fp32_master_1_moment1_0'], Moment2=['linear_5.w_0_fp32_master_1_moment2_0'], Param=['linear_5.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_45/, op_role = 2, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_6.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_6.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_6.w_0_fp32_master_1'], Moment1Out=['linear_6.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_6.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_6.w_0']} = adamw(inputs={Beta1Pow=['linear_6.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_6.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_6.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_6.w_0_fp32_master_1'], Moment1=['linear_6.w_0_fp32_master_1_moment1_0'], Moment2=['linear_6.w_0_fp32_master_1_moment2_0'], Param=['linear_6.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_46/, op_role = 2, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_0.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_0.w_0_fp32_master_1'], Moment1Out=['create_parameter_0.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_0.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_0.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_0.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_0.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_0.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_0.w_0_fp32_master_1'], Moment1=['create_parameter_0.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_0.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_47/, op_role = 2, op_role_var = ['create_parameter_0.w_0', 'create_parameter_0.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_1.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_1.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_1.w_0_fp32_master_1'], Moment1Out=['create_parameter_1.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_1.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_1.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_1.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_1.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_1.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_1.w_0_fp32_master_1'], Moment1=['create_parameter_1.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_1.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_48/, op_role = 2, op_role_var = ['create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_14.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_14.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_14.w_0_fp32_master_1'], Moment1Out=['linear_14.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_14.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_14.w_0']} = adamw(inputs={Beta1Pow=['linear_14.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_14.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_14.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_14.w_0_fp32_master_1'], Moment1=['linear_14.w_0_fp32_master_1_moment1_0'], Moment2=['linear_14.w_0_fp32_master_1_moment2_0'], Param=['linear_14.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_49/, op_role = 2, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_15.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_15.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_15.w_0_fp32_master_1'], Moment1Out=['linear_15.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_15.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_15.w_0']} = adamw(inputs={Beta1Pow=['linear_15.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_15.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_15.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_15.w_0_fp32_master_1'], Moment1=['linear_15.w_0_fp32_master_1_moment1_0'], Moment2=['linear_15.w_0_fp32_master_1_moment2_0'], Param=['linear_15.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_50/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_16.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_16.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_16.w_0_fp32_master_1'], Moment1Out=['linear_16.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_16.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_16.w_0']} = adamw(inputs={Beta1Pow=['linear_16.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_16.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_16.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_16.w_0_fp32_master_1'], Moment1=['linear_16.w_0_fp32_master_1_moment1_0'], Moment2=['linear_16.w_0_fp32_master_1_moment2_0'], Param=['linear_16.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_51/, op_role = 2, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_17.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_17.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_17.w_0_fp32_master_1'], Moment1Out=['linear_17.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_17.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_17.w_0']} = adamw(inputs={Beta1Pow=['linear_17.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_17.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_17.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_17.w_0_fp32_master_1'], Moment1=['linear_17.w_0_fp32_master_1_moment1_0'], Moment2=['linear_17.w_0_fp32_master_1_moment2_0'], Param=['linear_17.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_52/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_18.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_18.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_18.w_0_fp32_master_1'], Moment1Out=['linear_18.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_18.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_18.w_0']} = adamw(inputs={Beta1Pow=['linear_18.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_18.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_18.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_18.w_0_fp32_master_1'], Moment1=['linear_18.w_0_fp32_master_1_moment1_0'], Moment2=['linear_18.w_0_fp32_master_1_moment2_0'], Param=['linear_18.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_53/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_19.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_19.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_19.w_0_fp32_master_1'], Moment1Out=['linear_19.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_19.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_19.w_0']} = adamw(inputs={Beta1Pow=['linear_19.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_19.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_19.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_19.w_0_fp32_master_1'], Moment1=['linear_19.w_0_fp32_master_1_moment1_0'], Moment2=['linear_19.w_0_fp32_master_1_moment2_0'], Param=['linear_19.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_54/, op_role = 2, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_20.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['linear_20.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['linear_20.w_0_fp32_master_1'], Moment1Out=['linear_20.w_0_fp32_master_1_moment1_0'], Moment2Out=['linear_20.w_0_fp32_master_1_moment2_0'], ParamOut=['linear_20.w_0']} = adamw(inputs={Beta1Pow=['linear_20.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_20.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_20.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['linear_20.w_0_fp32_master_1'], Moment1=['linear_20.w_0_fp32_master_1_moment1_0'], Moment2=['linear_20.w_0_fp32_master_1_moment2_0'], Param=['linear_20.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_55/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_4.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_4.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_4.w_0_fp32_master_1'], Moment1Out=['create_parameter_4.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_4.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_4.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_4.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_4.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_4.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_4.w_0_fp32_master_1'], Moment1=['create_parameter_4.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_4.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_4.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_56/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_5.w_0_fp32_master_1_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_5.w_0_fp32_master_1_beta2_pow_acc_0'], MasterParamOut=['create_parameter_5.w_0_fp32_master_1'], Moment1Out=['create_parameter_5.w_0_fp32_master_1_moment1_0'], Moment2Out=['create_parameter_5.w_0_fp32_master_1_moment2_0'], ParamOut=['create_parameter_5.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_5.w_0_fp32_master_1_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_5.w_0_fp32_master_1_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_5.w_0@GRAD'], LearningRate=['learning_rate_1'], MasterParam=['create_parameter_5.w_0_fp32_master_1'], Moment1=['create_parameter_5.w_0_fp32_master_1_moment1_0'], Moment2=['create_parameter_5.w_0_fp32_master_1_moment2_0'], Param=['create_parameter_5.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_57/, op_role = 2, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
}

[2024-02-22 15:21:03,612] [    INFO] process_group.py:150 - group_id: 0, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:52587
[2024-02-22 15:21:03,777] [    INFO] process_group.py:150 - group_id: 26, ranks: [0, 1], nranks: 2, trainer_endpoints: 172.17.0.3:52587
[2024-02-22 15:21:04,152] [    INFO] process_group.py:150 - group_id: 28, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:52587
[2024-02-22 15:21:04,443] [    INFO] process_group.py:150 - group_id: 30, ranks: [1, 3], nranks: 2, trainer_endpoints: 172.17.0.3:52587
/usr/local/lib/python3.9/dist-packages/paddle/distributed/auto_parallel/static/process_group.py:255: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-02-22 15:21:04,760] [    INFO] process_group.py:150 - group_id: 32, ranks: [3, 1], nranks: 2, trainer_endpoints: 172.17.0.3:52587
I0222 15:21:05.076354 24060 program_interpreter.cc:220] New Executor is Running.
I0222 15:21:07.767246 24060 interpreter_util.cc:652] Standalone Executor is Used.
I0222 15:21:18.218976 24520 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
[32m[2024-02-22 15:21:18,465] [    INFO][0m - loss: 0.0, learning_rate: 8.273e-05, global_step: 1, interval_runtime: 13.3306, interval_samples_per_second: 1.2002489463259831, interval_steps_per_second: 0.07501555914537394[0m
[32m[2024-02-22 15:21:28,039] [    INFO][0m - loss: 0.0, learning_rate: 7.364e-05, global_step: 2, interval_runtime: 9.5722, interval_samples_per_second: 1.671498652304935, interval_steps_per_second: 0.10446866576905844[0m
[32m[2024-02-22 15:21:37,650] [    INFO][0m - loss: 0.0, learning_rate: 6.455e-05, global_step: 3, interval_runtime: 9.6087, interval_samples_per_second: 1.6651598836120136, interval_steps_per_second: 0.10407249272575085[0m
[32m[2024-02-22 15:21:47,297] [    INFO][0m - loss: 0.0, learning_rate: 5.545e-05, global_step: 4, interval_runtime: 9.6444, interval_samples_per_second: 1.6589992542411311, interval_steps_per_second: 0.1036874533900707[0m
[32m[2024-02-22 15:21:56,999] [    INFO][0m - loss: 0.0, learning_rate: 4.636e-05, global_step: 5, interval_runtime: 9.7005, interval_samples_per_second: 1.649395013738678, interval_steps_per_second: 0.10308718835866737[0m
