{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var input_ids : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(2, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var range_0.tmp_0 : LOD_TENSOR.shape(2048,).dtype(int64).stop_gradient(True)
    var expand_0.tmp_0 : LOD_TENSOR.shape(2, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(2, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, 2, 2048).dtype(bool).stop_gradient(True)
    var tmp_0 : LOD_TENSOR.shape(2, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var expand_1.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_13.tmp_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var tril_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_1 : LOD_TENSOR.shape(0, 2048, 2048).dtype(bool).stop_gradient(True)
    var expand_2.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var bitwise_and_0.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var fill_constant_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_2.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var cast_0.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_1.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var elementwise_add_1 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_2 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_3 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_4 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_2.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var where_0.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var tmp_1 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_45 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_46 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_47 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_48 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_4.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_49 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_14.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_15.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_16.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_7 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_4.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_4.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_8 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_10.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_10.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_12.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_12.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_50 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_51 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_52 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_53 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_54 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_55 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_56 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_57 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_58 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_13.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_59 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_60 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_17.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_62 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_63 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_64 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_5.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_65 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_18.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_19.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_66 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_20.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_68 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_69 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_70 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_6.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_71 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_21.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_22.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_23.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_10 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_6.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_11 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_14.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_14.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_16.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_16.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_72 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_73 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_74 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_75 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_76 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_77 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_78 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_79 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_80 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_18.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_81 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_82 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_24.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_84 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_85 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_86 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_7.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_87 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_25.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_26.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_88 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_27.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_89 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_90 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_91 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_92 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_8.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_93 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param llama_lm_head_auto_0.w_0 : LOD_TENSOR.shape(4096, 32000).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var tmp_94 : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var labels : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_18.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_18.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var tmp_95 : LOD_TENSOR.shape().dtype(float16).stop_gradient(True)
    var tmp_96 : LOD_TENSOR.shape(1, 2048, 1).dtype(bool).stop_gradient(False)
    var masked_select_0.tmp_0 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var tmp_97 : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var mean_9.tmp_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var mean_9.tmp_0.cast_fp32_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var scaled_loss_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var scaled_loss_1@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var mean_9.tmp_0.cast_fp32_0@GRAD_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var mean_9.tmp_0@GRAD : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var tmp_97@GRAD : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var masked_select_0.tmp_0@GRAD : LOD_TENSOR.shape(-1,).dtype(float16).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_94@GRAD : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32000).dtype(float16).stop_gradient(False)
    var tmp_93@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var llama_lm_head_auto_0.w_0@GRAD : LOD_TENSOR.shape(4096, 32000).dtype(float16).stop_gradient(False)
    var tmp_92@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_8.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_89@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_91@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_90@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_89@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_68.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_69.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_70.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_71.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var squeeze_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_6.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_7.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_14.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_14.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_15.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_16.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_16.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_17.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_72.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_1.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_73.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_74.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_75.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_76.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_1.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_77.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_78.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_79.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_80.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_18.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_81.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_1.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_82.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_1.subprog_0 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_84.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_85.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_7.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_86.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_87.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_3.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_88.subprog_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0.subprog_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_89@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_27.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_88@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_27.w_0@GRAD : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var silu_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_26.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_87@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_26.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_25.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_87@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_25.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var tmp_87@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_86@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_7.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_85@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_84@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_24.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_24.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_82@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_1@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_81@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_80@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_79@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_75@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_76@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_78@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_77@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_72@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_74@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_73@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_23.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_23.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_22.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_22.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_21.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_21.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var tmp_71@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_70@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_6.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_69@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_68@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_46.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_47.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_48.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_49.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var squeeze_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_4.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_5.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_10.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_10.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_11.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_12.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_12.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_13.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_50.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_1.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_51.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_52.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_53.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_54.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_1.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_55.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_56.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_57.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_58.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_13.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_59.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_1.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_60.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_1.subprog_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_62.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_63.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_5.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_64.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_65.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_2.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_66.subprog_1 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0.subprog_1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_67@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_20.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_66@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_20.w_0@GRAD : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var silu_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_19.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_65@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_19.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_18.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_65@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_18.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var tmp_65@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_64@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_5.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_63@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_62@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_61@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_17.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_17.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_60@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_1@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_59@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_58@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_57@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_53@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_54@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_56@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_55@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_50@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_52@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_51@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_16.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_16.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_15.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_15.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_14.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_14.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var tmp_49@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_48@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_4.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_47@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_46@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var opt_opt_squared_l2_norm_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_20.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_22.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_23.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_25.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_stack_0.tmp_0 : LOD_TENSOR.shape(39, 1).dtype(float16).stop_gradient(False)
    var opt_opt_sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var opt_tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_tmp_21 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_23 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_24 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_26 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_27 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_32 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_34 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_38 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_39 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var linear_15.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_15.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_15.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_17.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_17.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(True)
    persist var linear_18.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_18.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(True)
    persist var linear_20.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(11008, 4096).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_20.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_4.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_4.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_24.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_24.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(True)
    persist var linear_26.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_26.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_8.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_8.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 32000).dtype(float32).stop_gradient(True)
    persist var llama_lm_head_auto_0.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 32000).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 32000).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var llama_lm_head_auto_0.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var split@RESHARD.tmp_0 : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(False)
    var split@RESHARD.tmp_1 : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(False)
    var split@RESHARD.tmp_2 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_3 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(False)

    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 1, op original_id: 1, process_mesh (annotated): {shape: [2,2,1], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 3, op original_id: 3, process_mesh (annotated): {shape: [2,2,1], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_9.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 2048, value = 2048.0, with_quant_attr = False)
    {Out=['fill_constant_11.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1, value = 1.0, with_quant_attr = False)
    {Out=['range_0.tmp_0']} = range(inputs={End=['fill_constant_9.tmp_0'], Start=['fill_constant_7.tmp_0'], Step=['fill_constant_11.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_0.tmp_0']} = expand_v2(inputs={Shape=[], X=['range_0.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [2, 2048], with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['fill_constant_5.tmp_0']}, axes = [1, 2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_0']} = cast(inputs={X=['unsqueeze2_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['expand_1.tmp_0']} = expand_v2(inputs={Shape=[], X=['tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [2, 1, 2048, 2048], with_quant_attr = False)
    {Out=['fill_constant_13.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2048, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['tril_0']} = tril_triu(inputs={X=['fill_constant_13.tmp_0']}, diagonal = 0, lower = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_1.tmp_0'], XShape=['unsqueeze2_1.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['tril_0']}, axes = [0, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_2.tmp_0']} = expand_v2(inputs={Shape=[], X=['unsqueeze2_1.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [2, 1, 2048, 2048], with_quant_attr = False)
    {Out=['bitwise_and_0.tmp_0']} = bitwise_and(inputs={X=['expand_1.tmp_0'], Y=['expand_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['fill_constant_15.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_17.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = -3.4028234663852886e+38, value = -3.4028234663852886e+38, with_quant_attr = False)
    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['fill_constant_15.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_1.tmp_0']} = fill_any_like(inputs={X=['fill_constant_17.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_2.tmp_0']} = fill_any_like(inputs={X=['bitwise_and_0.tmp_0']}, dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['cast_0.tmp_0']} = cast(inputs={X=['full_like_2.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['cast_1.tmp_0']} = cast(inputs={X=['bitwise_and_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['elementwise_add_0']} = elementwise_add(inputs={X=['full_like_0.tmp_0'], Y=['full_like_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_1']} = elementwise_add(inputs={X=['elementwise_add_0'], Y=['cast_0.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_2']} = elementwise_add(inputs={X=['fill_constant_15.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_3']} = elementwise_add(inputs={X=['fill_constant_17.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_4']} = elementwise_add(inputs={X=['cast_1.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['elementwise_add_4']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['where_0.tmp_0']} = where(inputs={Condition=['cast_2.tmp_0'], X=['elementwise_add_2'], Y=['elementwise_add_3']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1']} = cast(inputs={X=['where_0.tmp_0']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_45']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 32, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_46']} = cast(inputs={X=['tmp_45']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 161, op original_id: 161, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_45's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_46's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_4.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_46']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 162, op original_id: 162, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_46's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_4.tmp_0']} = reduce_mean(inputs={X=['pow_4.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 163, op original_id: 163, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; pow_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_47']} = scale(inputs={ScaleTensor=[], X=['mean_4.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 164, op original_id: 164, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; mean_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_47's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_4.tmp_0']} = rsqrt(inputs={X=['tmp_47']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 165, op original_id: 165, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_47's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_48']} = elementwise_mul(inputs={X=['rsqrt_4.tmp_0'], Y=['tmp_45']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 166, op original_id: 166, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; rsqrt_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_45's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_48's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_49']} = elementwise_mul(inputs={X=['tmp_48'], Y=['create_parameter_4.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 167, op original_id: 167, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_48's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_4.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_49's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_14.tmp_0']} = matmul_v2(inputs={X=['tmp_49'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 168, op original_id: 168, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_49's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_14.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_10.tmp_0'], XShape=['reshape2_10.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 169, op original_id: 169, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_15.tmp_0']} = matmul_v2(inputs={X=['tmp_49'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 170, op original_id: 170, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_49's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_15.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_11.tmp_0'], XShape=['reshape2_11.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 171, op original_id: 171, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_16.tmp_0']} = matmul_v2(inputs={X=['tmp_49'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 172, op original_id: 172, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_49's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_16.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_12.tmp_0'], XShape=['reshape2_12.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 173, op original_id: 173, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_12.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_4.tmp_0'], XShape=['squeeze_4.tmp_1']} = squeeze2(inputs={X=['eager_tmp_7']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 174, op original_id: 174, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; eager_tmp_7's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_5.tmp_0'], XShape=['squeeze_5.tmp_1']} = squeeze2(inputs={X=['eager_tmp_8']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 175, op original_id: 175, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; eager_tmp_8's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['split@RESHARD.tmp_0', 'split@RESHARD.tmp_1']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['expand_0.tmp_0']}, axis = 0, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['unsqueeze2_10.tmp_0'], XShape=['unsqueeze2_10.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 176, op original_id: 176, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_10.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_4.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_10.tmp_0'], X=['squeeze_4.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 177, op original_id: 177, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; unsqueeze2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_11.tmp_0'], XShape=['unsqueeze2_11.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_4.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 178, op original_id: 178, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; gather_nd_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_12.tmp_0'], XShape=['unsqueeze2_12.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 179, op original_id: 179, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_12.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_5.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_12.tmp_0'], X=['squeeze_5.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 180, op original_id: 180, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; unsqueeze2_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_13.tmp_0'], XShape=['unsqueeze2_13.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_5.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 181, op original_id: 181, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; gather_nd_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_50']} = elementwise_mul(inputs={X=['reshape2_10.tmp_0'], Y=['unsqueeze2_11.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 182, op original_id: 182, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_50's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_10.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 183, op original_id: 183, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_10.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 184, op original_id: 184, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_51']} = scale(inputs={ScaleTensor=[], X=['reshape2_10.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 185, op original_id: 185, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_10.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_51's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_4.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_51', 'reshape2_10.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 186, op original_id: 186, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_51's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_10.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_52']} = elementwise_mul(inputs={X=['concat_4.tmp_0'], Y=['unsqueeze2_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 187, op original_id: 187, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; concat_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_52's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_53']} = elementwise_add(inputs={X=['tmp_50'], Y=['tmp_52']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 188, op original_id: 188, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_50's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_52's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_53's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_54']} = elementwise_mul(inputs={X=['reshape2_11.tmp_0'], Y=['unsqueeze2_11.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 189, op original_id: 189, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_54's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_11.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 190, op original_id: 190, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_11.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 191, op original_id: 191, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_55']} = scale(inputs={ScaleTensor=[], X=['reshape2_11.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 192, op original_id: 192, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_11.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_55's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_5.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_55', 'reshape2_11.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 193, op original_id: 193, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_55's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_11.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_56']} = elementwise_mul(inputs={X=['concat_5.tmp_0'], Y=['unsqueeze2_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 194, op original_id: 194, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; concat_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_56's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_57']} = elementwise_add(inputs={X=['tmp_54'], Y=['tmp_56']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 195, op original_id: 195, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_54's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_56's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_57's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['tmp_53']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 196, op original_id: 196, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_53's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_10.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['tmp_57']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 197, op original_id: 197, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_57's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_11.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_12.tmp_0'], XShape=['transpose_12.tmp_1']} = transpose2(inputs={X=['reshape2_12.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 198, op original_id: 198, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_12.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_58']} = scale(inputs={ScaleTensor=[], X=['transpose_10.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 199, op original_id: 199, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; transpose_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_58's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_13.tmp_0'], XShape=['transpose_13.tmp_1']} = transpose2(inputs={X=['transpose_11.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 200, op original_id: 200, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; transpose_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_13.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_4.tmp_0']} = matmul_v2(inputs={X=['tmp_58'], Y=['transpose_13.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 201, op original_id: 201, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_58's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['split@RESHARD.tmp_2', 'split@RESHARD.tmp_3']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['tmp_1']}, axis = 0, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['reshape2_13.tmp_0'], XShape=['reshape2_13.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 202, op original_id: 202, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_13.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_59']} = elementwise_add(inputs={X=['matmul_v2_4.tmp_0'], Y=['reshape2_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 203, op original_id: 203, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; matmul_v2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_59's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_2.tmp_0']} = cast(inputs={X=['tmp_59']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 204, op original_id: 204, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_59's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_2.tmp_1']} = softmax(inputs={X=['softmax_2.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 205, op original_id: 205, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; softmax_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_60']} = cast(inputs={X=['softmax_2.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 206, op original_id: 206, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; softmax_2.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_60's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_5.tmp_0']} = matmul_v2(inputs={X=['tmp_60'], Y=['transpose_12.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 207, op original_id: 207, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_60's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_14.tmp_0'], XShape=['transpose_14.tmp_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 208, op original_id: 208, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; matmul_v2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_14.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_14.tmp_0'], XShape=['reshape2_14.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 209, op original_id: 209, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; transpose_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_14.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_17.tmp_0']} = matmul_v2(inputs={X=['reshape2_14.tmp_0'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 210, op original_id: 210, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_17.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_61']} = elementwise_add(inputs={X=['tmp_45'], Y=['linear_17.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 211, op original_id: 211, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_45's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_61's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_62']} = cast(inputs={X=['tmp_61']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 212, op original_id: 212, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_61's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_62's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_5.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_62']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 213, op original_id: 213, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_62's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_5.tmp_0']} = reduce_mean(inputs={X=['pow_5.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 214, op original_id: 214, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; pow_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_63']} = scale(inputs={ScaleTensor=[], X=['mean_5.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 215, op original_id: 215, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; mean_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_63's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_5.tmp_0']} = rsqrt(inputs={X=['tmp_63']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 216, op original_id: 216, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_63's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_64']} = elementwise_mul(inputs={X=['rsqrt_5.tmp_0'], Y=['tmp_61']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 217, op original_id: 217, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; rsqrt_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_61's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_64's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_65']} = elementwise_mul(inputs={X=['tmp_64'], Y=['create_parameter_5.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 218, op original_id: 218, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_64's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_5.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_65's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_18.tmp_0']} = matmul_v2(inputs={X=['tmp_65'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 219, op original_id: 219, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_65's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_18.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_18.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_2.tmp_0']} = silu(inputs={X=['linear_18.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 220, op original_id: 220, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_18.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_19.tmp_0']} = matmul_v2(inputs={X=['tmp_65'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 221, op original_id: 221, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_65's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_19.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_19.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_66']} = elementwise_mul(inputs={X=['silu_2.tmp_0'], Y=['linear_19.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 222, op original_id: 222, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; silu_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_19.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_66's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_20.tmp_0']} = matmul_v2(inputs={X=['tmp_66'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 223, op original_id: 223, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_66's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_20.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_20.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_67']} = elementwise_add(inputs={X=['tmp_61'], Y=['linear_20.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 224, op original_id: 224, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_61's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_20.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_67's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_68']} = cast(inputs={X=['tmp_67']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 225, op original_id: 225, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_67's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_68's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_6.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_68']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 226, op original_id: 226, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_68's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_6.tmp_0']} = reduce_mean(inputs={X=['pow_6.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 227, op original_id: 227, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; pow_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_69']} = scale(inputs={ScaleTensor=[], X=['mean_6.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 228, op original_id: 228, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; mean_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_69's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_6.tmp_0']} = rsqrt(inputs={X=['tmp_69']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 229, op original_id: 229, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_69's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_70']} = elementwise_mul(inputs={X=['rsqrt_6.tmp_0'], Y=['tmp_67']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 230, op original_id: 230, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; rsqrt_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_67's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_70's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_71']} = elementwise_mul(inputs={X=['tmp_70'], Y=['create_parameter_6.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 231, op original_id: 231, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_70's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_6.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_71's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_21.tmp_0']} = matmul_v2(inputs={X=['tmp_71'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 232, op original_id: 232, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_71's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_21.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_21.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_15.tmp_0'], XShape=['reshape2_15.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_21.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 233, op original_id: 233, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_21.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_22.tmp_0']} = matmul_v2(inputs={X=['tmp_71'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 234, op original_id: 234, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_71's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_22.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_22.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_16.tmp_0'], XShape=['reshape2_16.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_22.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 235, op original_id: 235, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_22.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_23.tmp_0']} = matmul_v2(inputs={X=['tmp_71'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 236, op original_id: 236, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_71's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_23.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_23.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_17.tmp_0'], XShape=['reshape2_17.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 237, op original_id: 237, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_23.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_17.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_6.tmp_0'], XShape=['squeeze_6.tmp_1']} = squeeze2(inputs={X=['eager_tmp_10']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 238, op original_id: 238, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; eager_tmp_10's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_7.tmp_0'], XShape=['squeeze_7.tmp_1']} = squeeze2(inputs={X=['eager_tmp_11']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 239, op original_id: 239, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; eager_tmp_11's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_14.tmp_0'], XShape=['unsqueeze2_14.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 240, op original_id: 240, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_14.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_14.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_6.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_14.tmp_0'], X=['squeeze_6.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 241, op original_id: 241, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; unsqueeze2_14.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_15.tmp_0'], XShape=['unsqueeze2_15.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_6.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 242, op original_id: 242, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; gather_nd_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_16.tmp_0'], XShape=['unsqueeze2_16.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 243, op original_id: 243, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_16.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_7.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_16.tmp_0'], X=['squeeze_7.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 244, op original_id: 244, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; unsqueeze2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_17.tmp_0'], XShape=['unsqueeze2_17.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_7.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 245, op original_id: 245, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; gather_nd_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_72']} = elementwise_mul(inputs={X=['reshape2_15.tmp_0'], Y=['unsqueeze2_15.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 246, op original_id: 246, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_72's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_15.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 247, op original_id: 247, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_15.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 248, op original_id: 248, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_73']} = scale(inputs={ScaleTensor=[], X=['reshape2_15.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 249, op original_id: 249, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_15.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_73's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_6.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_73', 'reshape2_15.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 250, op original_id: 250, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_73's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_15.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_74']} = elementwise_mul(inputs={X=['concat_6.tmp_0'], Y=['unsqueeze2_17.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 251, op original_id: 251, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; concat_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_74's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_75']} = elementwise_add(inputs={X=['tmp_72'], Y=['tmp_74']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 252, op original_id: 252, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_72's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_74's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_75's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_76']} = elementwise_mul(inputs={X=['reshape2_16.tmp_0'], Y=['unsqueeze2_15.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 253, op original_id: 253, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_76's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_16.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 254, op original_id: 254, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_16.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 255, op original_id: 255, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_77']} = scale(inputs={ScaleTensor=[], X=['reshape2_16.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 256, op original_id: 256, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_16.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_77's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_7.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_77', 'reshape2_16.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 257, op original_id: 257, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_77's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_16.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_78']} = elementwise_mul(inputs={X=['concat_7.tmp_0'], Y=['unsqueeze2_17.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 258, op original_id: 258, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; concat_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_78's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_79']} = elementwise_add(inputs={X=['tmp_76'], Y=['tmp_78']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 259, op original_id: 259, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_76's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_78's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_79's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_15.tmp_0'], XShape=['transpose_15.tmp_1']} = transpose2(inputs={X=['tmp_75']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 260, op original_id: 260, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_75's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_15.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_15.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_16.tmp_0'], XShape=['transpose_16.tmp_1']} = transpose2(inputs={X=['tmp_79']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 261, op original_id: 261, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_79's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_16.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_16.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_17.tmp_0'], XShape=['transpose_17.tmp_1']} = transpose2(inputs={X=['reshape2_17.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 262, op original_id: 262, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_17.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_17.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_80']} = scale(inputs={ScaleTensor=[], X=['transpose_15.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 263, op original_id: 263, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; transpose_15.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_80's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_18.tmp_0'], XShape=['transpose_18.tmp_1']} = transpose2(inputs={X=['transpose_16.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 264, op original_id: 264, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; transpose_16.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_18.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_18.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_6.tmp_0']} = matmul_v2(inputs={X=['tmp_80'], Y=['transpose_18.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 265, op original_id: 265, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_80's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_18.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_18.tmp_0'], XShape=['reshape2_18.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 266, op original_id: 266, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_18.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_18.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_81']} = elementwise_add(inputs={X=['matmul_v2_6.tmp_0'], Y=['reshape2_18.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 267, op original_id: 267, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; matmul_v2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_18.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_81's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_3.tmp_0']} = cast(inputs={X=['tmp_81']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 268, op original_id: 268, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_81's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_3.tmp_1']} = softmax(inputs={X=['softmax_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 269, op original_id: 269, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; softmax_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_82']} = cast(inputs={X=['softmax_3.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 270, op original_id: 270, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; softmax_3.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_82's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_7.tmp_0']} = matmul_v2(inputs={X=['tmp_82'], Y=['transpose_17.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 271, op original_id: 271, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_82's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_17.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_19.tmp_0'], XShape=['transpose_19.tmp_1']} = transpose2(inputs={X=['matmul_v2_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 272, op original_id: 272, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; matmul_v2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_19.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_19.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_19.tmp_0'], XShape=['reshape2_19.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 273, op original_id: 273, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; transpose_19.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_19.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_19.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_24.tmp_0']} = matmul_v2(inputs={X=['reshape2_19.tmp_0'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 274, op original_id: 274, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; reshape2_19.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_24.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_24.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_83']} = elementwise_add(inputs={X=['tmp_67'], Y=['linear_24.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 275, op original_id: 275, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_67's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_24.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_83's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_84']} = cast(inputs={X=['tmp_83']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 276, op original_id: 276, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_83's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_84's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_7.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_84']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 277, op original_id: 277, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_84's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_7.tmp_0']} = reduce_mean(inputs={X=['pow_7.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 278, op original_id: 278, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; pow_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_85']} = scale(inputs={ScaleTensor=[], X=['mean_7.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 279, op original_id: 279, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; mean_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_85's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_7.tmp_0']} = rsqrt(inputs={X=['tmp_85']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 280, op original_id: 280, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_85's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_86']} = elementwise_mul(inputs={X=['rsqrt_7.tmp_0'], Y=['tmp_83']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 281, op original_id: 281, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; rsqrt_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_83's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_86's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_87']} = elementwise_mul(inputs={X=['tmp_86'], Y=['create_parameter_7.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 282, op original_id: 282, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_86's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_7.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_87's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_25.tmp_0']} = matmul_v2(inputs={X=['tmp_87'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 283, op original_id: 283, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_87's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_25.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_25.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_3.tmp_0']} = silu(inputs={X=['linear_25.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 284, op original_id: 284, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; linear_25.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_26.tmp_0']} = matmul_v2(inputs={X=['tmp_87'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 285, op original_id: 285, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_87's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_26.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_26.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_88']} = elementwise_mul(inputs={X=['silu_3.tmp_0'], Y=['linear_26.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 286, op original_id: 286, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; silu_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_26.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_88's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_27.tmp_0']} = matmul_v2(inputs={X=['tmp_88'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 287, op original_id: 287, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_88's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_27.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_27.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_89']} = elementwise_add(inputs={X=['tmp_83'], Y=['linear_27.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 288, op original_id: 288, process_mesh (annotated): {shape: [2,1], process_ids: [1,3], dim_names: [dp,mp]}; tmp_83's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_27.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_89's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_90']} = cast(inputs={X=['tmp_89']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_8.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_90']}, factor = 2.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['mean_8.tmp_0']} = reduce_mean(inputs={X=['pow_8.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_91']} = scale(inputs={ScaleTensor=[], X=['mean_8.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_8.tmp_0']} = rsqrt(inputs={X=['tmp_91']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_92']} = elementwise_mul(inputs={X=['rsqrt_8.tmp_0'], Y=['tmp_89']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_93']} = elementwise_mul(inputs={X=['tmp_92'], Y=['create_parameter_8.w_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0']} = matmul_v2(inputs={X=['tmp_93'], Y=['llama_lm_head_auto_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_94']} = cast(inputs={X=['matmul_v2_8.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['unsqueeze2_18.tmp_0'], XShape=['unsqueeze2_18.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['labels']}, axes = [2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['unsqueeze2_18.tmp_0'], Logits=['tmp_94']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {Out=['tmp_95']} = fill_constant(inputs={}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [], str_value = , value = 0.0, with_quant_attr = False)
    {Out=['tmp_96']} = greater_than(inputs={X=['softmax_with_cross_entropy_0.tmp_1'], Y=['tmp_95']}, axis = -1, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Y=['masked_select_0.tmp_0']} = masked_select(inputs={Mask=['tmp_96'], X=['softmax_with_cross_entropy_0.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_97']} = cast(inputs={X=['masked_select_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['mean_9.tmp_0']} = reduce_mean(inputs={X=['tmp_97']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['mean_9.tmp_0.cast_fp32_0']} = cast(inputs={X=['mean_9.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, with_quant_attr = False)
    {Out=['scaled_loss_0']} = elementwise_mul(inputs={X=['mean_9.tmp_0.cast_fp32_0'], Y=['loss_scaling_0']}, axis = -1, op_device = , op_namescope = /, op_role = 256, op_role_var = [], with_quant_attr = False)
    {Out=['scaled_loss_1@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = , op_role = 257, op_role_var = [], place_type = -1, shape = [], str_value = , value = 1.0, with_quant_attr = False)
    {X@GRAD=['mean_9.tmp_0.cast_fp32_0@GRAD_0'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['scaled_loss_1@GRAD'], X=['mean_9.tmp_0.cast_fp32_0'], Y=['loss_scaling_0']}, axis = -1, op_role = 1)
    {Out=['mean_9.tmp_0@GRAD']} = cast(inputs={X=['mean_9.tmp_0.cast_fp32_0@GRAD_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['tmp_97@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_9.tmp_0@GRAD'], X=['tmp_97']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['masked_select_0.tmp_0@GRAD']} = cast(inputs={X=['tmp_97@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD']} = masked_select_grad(inputs={Mask=['tmp_96'], X=['softmax_with_cross_entropy_0.tmp_1'], Y@GRAD=['masked_select_0.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Logits@GRAD=['tmp_94@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['unsqueeze2_18.tmp_0'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False, use_softmax = True, with_quant_attr = False)
    {Out=['matmul_v2_8.tmp_0@GRAD']} = cast(inputs={X=['tmp_94@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['tmp_93@GRAD'], Y@GRAD=['llama_lm_head_auto_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_8.tmp_0@GRAD'], X=['tmp_93'], Y=['llama_lm_head_auto_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD']} = c_reduce_sum(inputs={X=['llama_lm_head_auto_0.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['llama_lm_head_auto_0.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['tmp_92@GRAD'], Y@GRAD=['create_parameter_8.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_93@GRAD'], X=['tmp_92'], Y=['create_parameter_8.w_0']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_8.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_8.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_8.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_8.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['rsqrt_8.tmp_0@GRAD'], Y@GRAD=['tmp_89@GRAD@RENAME@block0@0']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_92@GRAD'], X=['rsqrt_8.tmp_0'], Y=['tmp_89']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_91@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_8.tmp_0'], Out@GRAD=['rsqrt_8.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_91@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_8.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_8.tmp_0@GRAD'], X=['pow_8.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_90@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_8.tmp_0@GRAD'], X=['tmp_90']}, factor = 2.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_89@GRAD@RENAME@block0@1']} = cast(inputs={X=['tmp_90@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_68.subprog_0']} = cast(inputs={X=['tmp_67']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_6.tmp_0.subprog_0']} = pow(inputs={FactorTensor=[], X=['tmp_68.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_6.tmp_0.subprog_0']} = reduce_mean(inputs={X=['pow_6.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_69.subprog_0']} = scale(inputs={ScaleTensor=[], X=['mean_6.tmp_0.subprog_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_6.tmp_0.subprog_0']} = rsqrt(inputs={X=['tmp_69.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_70.subprog_0']} = elementwise_mul(inputs={X=['rsqrt_6.tmp_0.subprog_0'], Y=['tmp_67']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_71.subprog_0']} = elementwise_mul(inputs={X=['tmp_70.subprog_0'], Y=['create_parameter_6.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_21.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_71.subprog_0'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_15.tmp_0.subprog_0'], XShape=['reshape2_15.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_21.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_22.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_71.subprog_0'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_16.tmp_0.subprog_0'], XShape=['reshape2_16.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_22.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_23.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_71.subprog_0'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_17.tmp_0.subprog_0'], XShape=['reshape2_17.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_23.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_6.tmp_0.subprog_0'], XShape=['squeeze_6.tmp_1.subprog_0']} = squeeze2(inputs={X=['eager_tmp_10']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_7.tmp_0.subprog_0'], XShape=['squeeze_7.tmp_1.subprog_0']} = squeeze2(inputs={X=['eager_tmp_11']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_14.tmp_0.subprog_0'], XShape=['unsqueeze2_14.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_6.tmp_0.subprog_0']} = gather_nd(inputs={Index=['unsqueeze2_14.tmp_0.subprog_0'], X=['squeeze_6.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_15.tmp_0.subprog_0'], XShape=['unsqueeze2_15.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_6.tmp_0.subprog_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_16.tmp_0.subprog_0'], XShape=['unsqueeze2_16.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_7.tmp_0.subprog_0']} = gather_nd(inputs={Index=['unsqueeze2_16.tmp_0.subprog_0'], X=['squeeze_7.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_17.tmp_0.subprog_0'], XShape=['unsqueeze2_17.tmp_1.subprog_0']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_7.tmp_0.subprog_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_72.subprog_0']} = elementwise_mul(inputs={X=['reshape2_15.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0_slice_0.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0_slice_1.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_73.subprog_0']} = scale(inputs={ScaleTensor=[], X=['reshape2_15.tmp_0_slice_1.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_6.tmp_0.subprog_0']} = concat(inputs={AxisTensor=[], X=['tmp_73.subprog_0', 'reshape2_15.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_74.subprog_0']} = elementwise_mul(inputs={X=['concat_6.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_75.subprog_0']} = elementwise_add(inputs={X=['tmp_72.subprog_0'], Y=['tmp_74.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_76.subprog_0']} = elementwise_mul(inputs={X=['reshape2_16.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0_slice_0.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0_slice_1.subprog_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_77.subprog_0']} = scale(inputs={ScaleTensor=[], X=['reshape2_16.tmp_0_slice_1.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_7.tmp_0.subprog_0']} = concat(inputs={AxisTensor=[], X=['tmp_77.subprog_0', 'reshape2_16.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_78.subprog_0']} = elementwise_mul(inputs={X=['concat_7.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_79.subprog_0']} = elementwise_add(inputs={X=['tmp_76.subprog_0'], Y=['tmp_78.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_15.tmp_0.subprog_0'], XShape=['transpose_15.tmp_1.subprog_0']} = transpose2(inputs={X=['tmp_75.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_16.tmp_0.subprog_0'], XShape=['transpose_16.tmp_1.subprog_0']} = transpose2(inputs={X=['tmp_79.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_17.tmp_0.subprog_0'], XShape=['transpose_17.tmp_1.subprog_0']} = transpose2(inputs={X=['reshape2_17.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_80.subprog_0']} = scale(inputs={ScaleTensor=[], X=['transpose_15.tmp_0.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_18.tmp_0.subprog_0'], XShape=['transpose_18.tmp_1.subprog_0']} = transpose2(inputs={X=['transpose_16.tmp_0.subprog_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_6.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_80.subprog_0'], Y=['transpose_18.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_18.tmp_0.subprog_0'], XShape=['reshape2_18.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_81.subprog_0']} = elementwise_add(inputs={X=['matmul_v2_6.tmp_0.subprog_0'], Y=['reshape2_18.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_3.tmp_0.subprog_0']} = cast(inputs={X=['tmp_81.subprog_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_3.tmp_1.subprog_0']} = softmax(inputs={X=['softmax_3.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_82.subprog_0']} = cast(inputs={X=['softmax_3.tmp_1.subprog_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_7.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_82.subprog_0'], Y=['transpose_17.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_19.tmp_0.subprog_0'], XShape=['transpose_19.tmp_1.subprog_0']} = transpose2(inputs={X=['matmul_v2_7.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_19.tmp_0.subprog_0'], XShape=['reshape2_19.tmp_1.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_24.tmp_0.subprog_0']} = matmul_v2(inputs={X=['reshape2_19.tmp_0.subprog_0'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_83.subprog_0']} = elementwise_add(inputs={X=['tmp_67'], Y=['linear_24.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_84.subprog_0']} = cast(inputs={X=['tmp_83.subprog_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_7.tmp_0.subprog_0']} = pow(inputs={FactorTensor=[], X=['tmp_84.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_7.tmp_0.subprog_0']} = reduce_mean(inputs={X=['pow_7.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_85.subprog_0']} = scale(inputs={ScaleTensor=[], X=['mean_7.tmp_0.subprog_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_7.tmp_0.subprog_0']} = rsqrt(inputs={X=['tmp_85.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_86.subprog_0']} = elementwise_mul(inputs={X=['rsqrt_7.tmp_0.subprog_0'], Y=['tmp_83.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_87.subprog_0']} = elementwise_mul(inputs={X=['tmp_86.subprog_0'], Y=['create_parameter_7.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_25.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_87.subprog_0'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_3.tmp_0.subprog_0']} = silu(inputs={X=['linear_25.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_26.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_87.subprog_0'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_88.subprog_0']} = elementwise_mul(inputs={X=['silu_3.tmp_0.subprog_0'], Y=['linear_26.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_27.tmp_0.subprog_0']} = matmul_v2(inputs={X=['tmp_88.subprog_0'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_89@GRAD']} = sum(inputs={X=['tmp_89@GRAD@RENAME@block0@0', 'tmp_89@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_83@GRAD@RENAME@block0@0'], Y@GRAD=['linear_27.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_89@GRAD'], X=['tmp_83.subprog_0'], Y=['linear_27.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_88@GRAD'], Y@GRAD=['linear_27.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_27.tmp_0@GRAD'], X=['tmp_88.subprog_0'], Y=['linear_27.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_27.w_0', 'linear_27.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_27.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_27.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_27.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['silu_3.tmp_0@GRAD'], Y@GRAD=['linear_26.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_88@GRAD'], X=['silu_3.tmp_0.subprog_0'], Y=['linear_26.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_87@GRAD@RENAME@block0@0'], Y@GRAD=['linear_26.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_26.tmp_0@GRAD'], X=['tmp_87.subprog_0'], Y=['linear_26.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_26.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_26.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['linear_25.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_3.tmp_0.subprog_0'], Out@GRAD=['silu_3.tmp_0@GRAD'], X=['linear_25.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_87@GRAD@RENAME@block0@1'], Y@GRAD=['linear_25.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_25.tmp_0@GRAD'], X=['tmp_87.subprog_0'], Y=['linear_25.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_25.w_0', 'linear_25.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_25.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_25.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_25.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['tmp_87@GRAD']} = sum(inputs={X=['tmp_87@GRAD@RENAME@block0@0', 'tmp_87@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_86@GRAD'], Y@GRAD=['create_parameter_7.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_87@GRAD'], X=['tmp_86.subprog_0'], Y=['create_parameter_7.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['create_parameter_7.w_0', 'create_parameter_7.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_7.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_7.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_7.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_7.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['rsqrt_7.tmp_0@GRAD'], Y@GRAD=['tmp_83@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_86@GRAD'], X=['rsqrt_7.tmp_0.subprog_0'], Y=['tmp_83.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_85@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_7.tmp_0.subprog_0'], Out@GRAD=['rsqrt_7.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_7.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_85@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_7.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_7.tmp_0@GRAD'], X=['pow_7.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_84@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_7.tmp_0@GRAD'], X=['tmp_84.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_83@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_84@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_83@GRAD']} = sum(inputs={X=['tmp_83@GRAD@RENAME@block0@0', 'tmp_83@GRAD@RENAME@block0@1', 'tmp_83@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_67@GRAD@RENAME@block0@0'], Y@GRAD=['linear_24.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_83@GRAD'], X=['tmp_67'], Y=['linear_24.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_19.tmp_0@GRAD'], Y@GRAD=['linear_24.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_24.tmp_0@GRAD'], X=['reshape2_19.tmp_0.subprog_0'], Y=['linear_24.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_24.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_24.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['transpose_19.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_19.tmp_0@GRAD'], XShape=['reshape2_19.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [2, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['transpose_19.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_82@GRAD'], Y@GRAD=['transpose_17.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_7.tmp_0@GRAD'], X=['tmp_82.subprog_0'], Y=['transpose_17.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_3.tmp_1@GRAD']} = cast(inputs={X=['tmp_82@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_3.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_3.tmp_1.subprog_0'], Out@GRAD=['softmax_3.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_81@GRAD']} = cast(inputs={X=['softmax_3.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_6.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_81@GRAD'], X=['matmul_v2_6.tmp_0.subprog_0'], Y=['reshape2_18.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_80@GRAD'], Y@GRAD=['transpose_18.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_6.tmp_0@GRAD'], X=['tmp_80.subprog_0'], Y=['transpose_18.tmp_0.subprog_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_16.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_18.tmp_0@GRAD'], XShape=['transpose_18.tmp_1.subprog_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_15.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_80@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_17.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_17.tmp_0@GRAD'], XShape=['transpose_17.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_79@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_16.tmp_0@GRAD'], XShape=['transpose_16.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_75@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['transpose_15.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_76@GRAD'], Y@GRAD=['tmp_78@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_79@GRAD'], X=['tmp_76.subprog_0'], Y=['tmp_78.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_7.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_78@GRAD'], X=['concat_7.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_77@GRAD', 'reshape2_16.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_7.tmp_0@GRAD'], X=['tmp_77.subprog_0', 'reshape2_16.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_16.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_77@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_16.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], Out@GRAD=['reshape2_16.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_16.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_16.tmp_0.subprog_0'], Out@GRAD=['reshape2_16.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_16.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_76@GRAD'], X=['reshape2_16.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_72@GRAD'], Y@GRAD=['tmp_74@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_75@GRAD'], X=['tmp_72.subprog_0'], Y=['tmp_74.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_6.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_74@GRAD'], X=['concat_6.tmp_0.subprog_0'], Y=['unsqueeze2_17.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_73@GRAD', 'reshape2_15.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_6.tmp_0@GRAD'], X=['tmp_73.subprog_0', 'reshape2_15.tmp_0_slice_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_15.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_73@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_15.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], Out@GRAD=['reshape2_15.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_15.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_15.tmp_0.subprog_0'], Out@GRAD=['reshape2_15.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_15.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_72@GRAD'], X=['reshape2_15.tmp_0.subprog_0'], Y=['unsqueeze2_15.tmp_0.subprog_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_23.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_17.tmp_0@GRAD'], XShape=['reshape2_17.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_71@GRAD@RENAME@block0@0'], Y@GRAD=['linear_23.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_23.tmp_0@GRAD'], X=['tmp_71.subprog_0'], Y=['linear_23.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_23.w_0', 'linear_23.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_23.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_23.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_23.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['reshape2_16.tmp_0@GRAD']} = sum(inputs={X=['reshape2_16.tmp_0@GRAD@RENAME@block0@0', 'reshape2_16.tmp_0@GRAD@RENAME@block0@1', 'reshape2_16.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_22.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_16.tmp_0@GRAD'], XShape=['reshape2_16.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_71@GRAD@RENAME@block0@1'], Y@GRAD=['linear_22.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_22.tmp_0@GRAD'], X=['tmp_71.subprog_0'], Y=['linear_22.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_22.w_0', 'linear_22.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_22.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_22.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_22.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['reshape2_15.tmp_0@GRAD']} = sum(inputs={X=['reshape2_15.tmp_0@GRAD@RENAME@block0@0', 'reshape2_15.tmp_0@GRAD@RENAME@block0@1', 'reshape2_15.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_21.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_15.tmp_0@GRAD'], XShape=['reshape2_15.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_71@GRAD@RENAME@block0@2'], Y@GRAD=['linear_21.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_21.tmp_0@GRAD'], X=['tmp_71.subprog_0'], Y=['linear_21.w_0']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['linear_21.w_0', 'linear_21.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_21.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_21.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_21.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['tmp_71@GRAD']} = sum(inputs={X=['tmp_71@GRAD@RENAME@block0@0', 'tmp_71@GRAD@RENAME@block0@1', 'tmp_71@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_70@GRAD'], Y@GRAD=['create_parameter_6.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_71@GRAD'], X=['tmp_70.subprog_0'], Y=['create_parameter_6.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = ['create_parameter_6.w_0', 'create_parameter_6.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_6.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_6.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_6.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_6.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['rsqrt_6.tmp_0@GRAD'], Y@GRAD=['tmp_67@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_70@GRAD'], X=['rsqrt_6.tmp_0.subprog_0'], Y=['tmp_67']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_69@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_6.tmp_0.subprog_0'], Out@GRAD=['rsqrt_6.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_6.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_69@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_6.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_6.tmp_0@GRAD'], X=['pow_6.tmp_0.subprog_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_68@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_6.tmp_0@GRAD'], X=['tmp_68.subprog_0']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_3/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_67@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_68@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_46.subprog_1']} = cast(inputs={X=['tmp_45']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_4.tmp_0.subprog_1']} = pow(inputs={FactorTensor=[], X=['tmp_46.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_4.tmp_0.subprog_1']} = reduce_mean(inputs={X=['pow_4.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_47.subprog_1']} = scale(inputs={ScaleTensor=[], X=['mean_4.tmp_0.subprog_1']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_4.tmp_0.subprog_1']} = rsqrt(inputs={X=['tmp_47.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_48.subprog_1']} = elementwise_mul(inputs={X=['rsqrt_4.tmp_0.subprog_1'], Y=['tmp_45']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_49.subprog_1']} = elementwise_mul(inputs={X=['tmp_48.subprog_1'], Y=['create_parameter_4.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_14.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_49.subprog_1'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_10.tmp_0.subprog_1'], XShape=['reshape2_10.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_14.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_15.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_49.subprog_1'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_11.tmp_0.subprog_1'], XShape=['reshape2_11.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_15.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_16.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_49.subprog_1'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_12.tmp_0.subprog_1'], XShape=['reshape2_12.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_16.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_4.tmp_0.subprog_1'], XShape=['squeeze_4.tmp_1.subprog_1']} = squeeze2(inputs={X=['eager_tmp_7']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_5.tmp_0.subprog_1'], XShape=['squeeze_5.tmp_1.subprog_1']} = squeeze2(inputs={X=['eager_tmp_8']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_10.tmp_0.subprog_1'], XShape=['unsqueeze2_10.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_4.tmp_0.subprog_1']} = gather_nd(inputs={Index=['unsqueeze2_10.tmp_0.subprog_1'], X=['squeeze_4.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_11.tmp_0.subprog_1'], XShape=['unsqueeze2_11.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_4.tmp_0.subprog_1']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_12.tmp_0.subprog_1'], XShape=['unsqueeze2_12.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_0']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_5.tmp_0.subprog_1']} = gather_nd(inputs={Index=['unsqueeze2_12.tmp_0.subprog_1'], X=['squeeze_5.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_13.tmp_0.subprog_1'], XShape=['unsqueeze2_13.tmp_1.subprog_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_5.tmp_0.subprog_1']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_50.subprog_1']} = elementwise_mul(inputs={X=['reshape2_10.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0_slice_0.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0_slice_1.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_51.subprog_1']} = scale(inputs={ScaleTensor=[], X=['reshape2_10.tmp_0_slice_1.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_4.tmp_0.subprog_1']} = concat(inputs={AxisTensor=[], X=['tmp_51.subprog_1', 'reshape2_10.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_52.subprog_1']} = elementwise_mul(inputs={X=['concat_4.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_53.subprog_1']} = elementwise_add(inputs={X=['tmp_50.subprog_1'], Y=['tmp_52.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_54.subprog_1']} = elementwise_mul(inputs={X=['reshape2_11.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0_slice_0.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0_slice_1.subprog_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_55.subprog_1']} = scale(inputs={ScaleTensor=[], X=['reshape2_11.tmp_0_slice_1.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_5.tmp_0.subprog_1']} = concat(inputs={AxisTensor=[], X=['tmp_55.subprog_1', 'reshape2_11.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_56.subprog_1']} = elementwise_mul(inputs={X=['concat_5.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_57.subprog_1']} = elementwise_add(inputs={X=['tmp_54.subprog_1'], Y=['tmp_56.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_10.tmp_0.subprog_1'], XShape=['transpose_10.tmp_1.subprog_1']} = transpose2(inputs={X=['tmp_53.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_11.tmp_0.subprog_1'], XShape=['transpose_11.tmp_1.subprog_1']} = transpose2(inputs={X=['tmp_57.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_12.tmp_0.subprog_1'], XShape=['transpose_12.tmp_1.subprog_1']} = transpose2(inputs={X=['reshape2_12.tmp_0.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_58.subprog_1']} = scale(inputs={ScaleTensor=[], X=['transpose_10.tmp_0.subprog_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_13.tmp_0.subprog_1'], XShape=['transpose_13.tmp_1.subprog_1']} = transpose2(inputs={X=['transpose_11.tmp_0.subprog_1']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_4.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_58.subprog_1'], Y=['transpose_13.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_13.tmp_0.subprog_1'], XShape=['reshape2_13.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_59.subprog_1']} = elementwise_add(inputs={X=['matmul_v2_4.tmp_0.subprog_1'], Y=['reshape2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_2.tmp_0.subprog_1']} = cast(inputs={X=['tmp_59.subprog_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_2.tmp_1.subprog_1']} = softmax(inputs={X=['softmax_2.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_60.subprog_1']} = cast(inputs={X=['softmax_2.tmp_1.subprog_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_5.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_60.subprog_1'], Y=['transpose_12.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_14.tmp_0.subprog_1'], XShape=['transpose_14.tmp_1.subprog_1']} = transpose2(inputs={X=['matmul_v2_5.tmp_0.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_14.tmp_0.subprog_1'], XShape=['reshape2_14.tmp_1.subprog_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_14.tmp_0.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_17.tmp_0.subprog_1']} = matmul_v2(inputs={X=['reshape2_14.tmp_0.subprog_1'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_61.subprog_1']} = elementwise_add(inputs={X=['tmp_45'], Y=['linear_17.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_62.subprog_1']} = cast(inputs={X=['tmp_61.subprog_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_5.tmp_0.subprog_1']} = pow(inputs={FactorTensor=[], X=['tmp_62.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_5.tmp_0.subprog_1']} = reduce_mean(inputs={X=['pow_5.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_63.subprog_1']} = scale(inputs={ScaleTensor=[], X=['mean_5.tmp_0.subprog_1']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_5.tmp_0.subprog_1']} = rsqrt(inputs={X=['tmp_63.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_64.subprog_1']} = elementwise_mul(inputs={X=['rsqrt_5.tmp_0.subprog_1'], Y=['tmp_61.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_65.subprog_1']} = elementwise_mul(inputs={X=['tmp_64.subprog_1'], Y=['create_parameter_5.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_18.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_65.subprog_1'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_2.tmp_0.subprog_1']} = silu(inputs={X=['linear_18.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_19.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_65.subprog_1'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_66.subprog_1']} = elementwise_mul(inputs={X=['silu_2.tmp_0.subprog_1'], Y=['linear_19.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_20.tmp_0.subprog_1']} = matmul_v2(inputs={X=['tmp_66.subprog_1'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_67@GRAD']} = sum(inputs={X=['tmp_67@GRAD@RENAME@block0@0', 'tmp_67@GRAD@RENAME@block0@1', 'tmp_67@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_61@GRAD@RENAME@block0@0'], Y@GRAD=['linear_20.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_67@GRAD'], X=['tmp_61.subprog_1'], Y=['linear_20.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_66@GRAD'], Y@GRAD=['linear_20.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_20.tmp_0@GRAD'], X=['tmp_66.subprog_1'], Y=['linear_20.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_20.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_20.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['silu_2.tmp_0@GRAD'], Y@GRAD=['linear_19.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_66@GRAD'], X=['silu_2.tmp_0.subprog_1'], Y=['linear_19.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_65@GRAD@RENAME@block0@0'], Y@GRAD=['linear_19.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_19.tmp_0@GRAD'], X=['tmp_65.subprog_1'], Y=['linear_19.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_19.w_0', 'linear_19.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_19.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_19.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_19.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['linear_18.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_2.tmp_0.subprog_1'], Out@GRAD=['silu_2.tmp_0@GRAD'], X=['linear_18.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_65@GRAD@RENAME@block0@1'], Y@GRAD=['linear_18.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_18.tmp_0@GRAD'], X=['tmp_65.subprog_1'], Y=['linear_18.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_18.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_18.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['tmp_65@GRAD']} = sum(inputs={X=['tmp_65@GRAD@RENAME@block0@0', 'tmp_65@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_64@GRAD'], Y@GRAD=['create_parameter_5.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_65@GRAD'], X=['tmp_64.subprog_1'], Y=['create_parameter_5.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['create_parameter_5.w_0', 'create_parameter_5.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_5.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_5.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_5.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_5.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['rsqrt_5.tmp_0@GRAD'], Y@GRAD=['tmp_61@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_64@GRAD'], X=['rsqrt_5.tmp_0.subprog_1'], Y=['tmp_61.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_63@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_5.tmp_0.subprog_1'], Out@GRAD=['rsqrt_5.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_63@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_5.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_5.tmp_0@GRAD'], X=['pow_5.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_62@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_5.tmp_0@GRAD'], X=['tmp_62.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_61@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_62@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_61@GRAD']} = sum(inputs={X=['tmp_61@GRAD@RENAME@block0@0', 'tmp_61@GRAD@RENAME@block0@1', 'tmp_61@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_45@GRAD@RENAME@block0@0'], Y@GRAD=['linear_17.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_61@GRAD'], X=['tmp_45'], Y=['linear_17.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_14.tmp_0@GRAD'], Y@GRAD=['linear_17.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_17.tmp_0@GRAD'], X=['reshape2_14.tmp_0.subprog_1'], Y=['linear_17.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_17.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_17.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['transpose_14.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_14.tmp_0@GRAD'], XShape=['reshape2_14.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [2, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_14.tmp_0@GRAD'], XShape=['transpose_14.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_60@GRAD'], Y@GRAD=['transpose_12.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_5.tmp_0@GRAD'], X=['tmp_60.subprog_1'], Y=['transpose_12.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_2.tmp_1@GRAD']} = cast(inputs={X=['tmp_60@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_2.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_2.tmp_1.subprog_1'], Out@GRAD=['softmax_2.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_59@GRAD']} = cast(inputs={X=['softmax_2.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_4.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_59@GRAD'], X=['matmul_v2_4.tmp_0.subprog_1'], Y=['reshape2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_58@GRAD'], Y@GRAD=['transpose_13.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_4.tmp_0@GRAD'], X=['tmp_58.subprog_1'], Y=['transpose_13.tmp_0.subprog_1']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_13.tmp_0@GRAD'], XShape=['transpose_13.tmp_1.subprog_1']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_10.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_58@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_12.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_12.tmp_0@GRAD'], XShape=['transpose_12.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_57@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_53@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1.subprog_1']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_54@GRAD'], Y@GRAD=['tmp_56@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_57@GRAD'], X=['tmp_54.subprog_1'], Y=['tmp_56.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_5.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_56@GRAD'], X=['concat_5.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_55@GRAD', 'reshape2_11.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_5.tmp_0@GRAD'], X=['tmp_55.subprog_1', 'reshape2_11.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_11.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_55@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_11.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], Out@GRAD=['reshape2_11.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_11.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_11.tmp_0.subprog_1'], Out@GRAD=['reshape2_11.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_11.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_54@GRAD'], X=['reshape2_11.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_50@GRAD'], Y@GRAD=['tmp_52@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_53@GRAD'], X=['tmp_50.subprog_1'], Y=['tmp_52.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_4.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_52@GRAD'], X=['concat_4.tmp_0.subprog_1'], Y=['unsqueeze2_13.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_51@GRAD', 'reshape2_10.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_4.tmp_0@GRAD'], X=['tmp_51.subprog_1', 'reshape2_10.tmp_0_slice_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_10.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_51@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_10.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], Out@GRAD=['reshape2_10.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_10.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_10.tmp_0.subprog_1'], Out@GRAD=['reshape2_10.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_10.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_50@GRAD'], X=['reshape2_10.tmp_0.subprog_1'], Y=['unsqueeze2_11.tmp_0.subprog_1']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_16.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_12.tmp_0@GRAD'], XShape=['reshape2_12.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_49@GRAD@RENAME@block0@0'], Y@GRAD=['linear_16.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_16.tmp_0@GRAD'], X=['tmp_49.subprog_1'], Y=['linear_16.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_16.w_0', 'linear_16.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_16.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_16.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_16.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['reshape2_11.tmp_0@GRAD']} = sum(inputs={X=['reshape2_11.tmp_0@GRAD@RENAME@block0@0', 'reshape2_11.tmp_0@GRAD@RENAME@block0@1', 'reshape2_11.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_15.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_49@GRAD@RENAME@block0@1'], Y@GRAD=['linear_15.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_15.tmp_0@GRAD'], X=['tmp_49.subprog_1'], Y=['linear_15.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_15.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_15.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['reshape2_10.tmp_0@GRAD']} = sum(inputs={X=['reshape2_10.tmp_0@GRAD@RENAME@block0@0', 'reshape2_10.tmp_0@GRAD@RENAME@block0@1', 'reshape2_10.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_14.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_10.tmp_0@GRAD'], XShape=['reshape2_10.tmp_1.subprog_1']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_49@GRAD@RENAME@block0@2'], Y@GRAD=['linear_14.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_14.tmp_0@GRAD'], X=['tmp_49.subprog_1'], Y=['linear_14.w_0']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['linear_14.w_0', 'linear_14.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_14.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_14.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_14.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['tmp_49@GRAD']} = sum(inputs={X=['tmp_49@GRAD@RENAME@block0@0', 'tmp_49@GRAD@RENAME@block0@1', 'tmp_49@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_48@GRAD'], Y@GRAD=['create_parameter_4.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_49@GRAD'], X=['tmp_48.subprog_1'], Y=['create_parameter_4.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_4.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_4.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 28, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_4.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_4.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['rsqrt_4.tmp_0@GRAD'], Y@GRAD=['tmp_45@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_48@GRAD'], X=['rsqrt_4.tmp_0.subprog_1'], Y=['tmp_45']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_47@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_4.tmp_0.subprog_1'], Out@GRAD=['rsqrt_4.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_47@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_4.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_4.tmp_0@GRAD'], X=['pow_4.tmp_0.subprog_1']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_46@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_4.tmp_0@GRAD'], X=['tmp_46.subprog_1']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_2/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_45@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_46@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_45@GRAD']} = sum(inputs={X=['tmp_45@GRAD@RENAME@block0@0', 'tmp_45@GRAD@RENAME@block0@1', 'tmp_45@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    send_v2(inputs={X=['tmp_45@GRAD']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], peer = 1, ring_id = 30, use_calc_stream = True, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['linear_15.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_26.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['linear_15.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_26.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, op_role_var = [], ring_id = 29, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['linear_15.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_26.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['linear_15.w_0@GRAD', 'linear_17.w_0@GRAD', 'linear_18.w_0@GRAD', 'linear_20.w_0@GRAD', 'create_parameter_4.w_0@GRAD', 'linear_24.w_0@GRAD', 'linear_26.w_0@GRAD', 'create_parameter_8.w_0@GRAD', 'llama_lm_head_auto_0.w_0@GRAD']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_20.tmp_0']} = squared_l2_norm(inputs={X=['linear_15.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_22.tmp_0']} = squared_l2_norm(inputs={X=['linear_17.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_23.tmp_0']} = squared_l2_norm(inputs={X=['linear_18.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_25.tmp_0']} = squared_l2_norm(inputs={X=['linear_20.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_26.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_4.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_31.tmp_0']} = squared_l2_norm(inputs={X=['linear_24.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_33.tmp_0']} = squared_l2_norm(inputs={X=['linear_26.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_37.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_8.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_38.tmp_0']} = squared_l2_norm(inputs={X=['llama_lm_head_auto_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Y=['opt_opt_stack_0.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_20.tmp_0', 'opt_opt_squared_l2_norm_22.tmp_0', 'opt_opt_squared_l2_norm_23.tmp_0', 'opt_opt_squared_l2_norm_25.tmp_0', 'opt_opt_squared_l2_norm_26.tmp_0', 'opt_opt_squared_l2_norm_31.tmp_0', 'opt_opt_squared_l2_norm_33.tmp_0', 'opt_opt_squared_l2_norm_37.tmp_0', 'opt_opt_squared_l2_norm_38.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_sum_0.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_0.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_tmp_0']} = cast(inputs={X=['opt_opt_sum_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], out_dtype = 5, with_quant_attr = False)
    {Out=['opt_tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['opt_tmp_0']}, op_device = , op_namescope = /auto_parallel/global_norm_synchronization, op_role = 2, op_role_var = [], ring_id = 0, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_21']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['linear_15.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_15.w_0@GRAD'], Y=['opt_tmp_21']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_23']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['linear_17.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_17.w_0@GRAD'], Y=['opt_tmp_23']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_24']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['linear_18.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_18.w_0@GRAD'], Y=['opt_tmp_24']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_26']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['linear_20.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_20.w_0@GRAD'], Y=['opt_tmp_26']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_27']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['create_parameter_4.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_4.w_0@GRAD'], Y=['opt_tmp_27']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_32']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['linear_24.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_24.w_0@GRAD'], Y=['opt_tmp_32']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_34']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['linear_26.w_0@GRAD']} = elementwise_mul(inputs={X=['linear_26.w_0@GRAD'], Y=['opt_tmp_34']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_38']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['create_parameter_8.w_0@GRAD']} = elementwise_mul(inputs={X=['create_parameter_8.w_0@GRAD'], Y=['opt_tmp_38']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], with_quant_attr = False)
    {Out=['opt_tmp_39']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], out_dtype = 4, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0@GRAD']} = elementwise_mul(inputs={X=['llama_lm_head_auto_0.w_0@GRAD'], Y=['opt_tmp_39']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], with_quant_attr = False)
    {Beta1PowOut=['create_parameter_4.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_4.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['create_parameter_4.w_0_fp32_master_0'], Moment1Out=['create_parameter_4.w_0_fp32_master_0_moment1_0'], Moment2Out=['create_parameter_4.w_0_fp32_master_0_moment2_0'], ParamOut=['create_parameter_4.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_4.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_4.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_4.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['create_parameter_4.w_0_fp32_master_0'], Moment1=['create_parameter_4.w_0_fp32_master_0_moment1_0'], Moment2=['create_parameter_4.w_0_fp32_master_0_moment2_0'], Param=['create_parameter_4.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_26/, op_role = 2, op_role_var = ['create_parameter_4.w_0', 'create_parameter_4.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_15.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_15.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_15.w_0_fp32_master_0'], Moment1Out=['linear_15.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_15.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_15.w_0']} = adamw(inputs={Beta1Pow=['linear_15.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_15.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_15.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['linear_15.w_0_fp32_master_0'], Moment1=['linear_15.w_0_fp32_master_0_moment1_0'], Moment2=['linear_15.w_0_fp32_master_0_moment2_0'], Param=['linear_15.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_20/, op_role = 2, op_role_var = ['linear_15.w_0', 'linear_15.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_17.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_17.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_17.w_0_fp32_master_0'], Moment1Out=['linear_17.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_17.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_17.w_0']} = adamw(inputs={Beta1Pow=['linear_17.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_17.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_17.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['linear_17.w_0_fp32_master_0'], Moment1=['linear_17.w_0_fp32_master_0_moment1_0'], Moment2=['linear_17.w_0_fp32_master_0_moment2_0'], Param=['linear_17.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_22/, op_role = 2, op_role_var = ['linear_17.w_0', 'linear_17.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_18.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_18.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_18.w_0_fp32_master_0'], Moment1Out=['linear_18.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_18.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_18.w_0']} = adamw(inputs={Beta1Pow=['linear_18.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_18.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_18.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['linear_18.w_0_fp32_master_0'], Moment1=['linear_18.w_0_fp32_master_0_moment1_0'], Moment2=['linear_18.w_0_fp32_master_0_moment2_0'], Param=['linear_18.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_23/, op_role = 2, op_role_var = ['linear_18.w_0', 'linear_18.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    1122{Beta1PowOut=['linear_20.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_20.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_20.w_0_fp32_master_0'], Moment1Out=['linear_20.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_20.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_20.w_0']} = adamw(inputs={Beta1Pow=['linear_20.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_20.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_20.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['linear_20.w_0_fp32_master_0'], Moment1=['linear_20.w_0_fp32_master_0_moment1_0'], Moment2=['linear_20.w_0_fp32_master_0_moment2_0'], Param=['linear_20.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_25/, op_role = 2, op_role_var = ['linear_20.w_0', 'linear_20.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_24.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_24.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_24.w_0_fp32_master_0'], Moment1Out=['linear_24.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_24.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_24.w_0']} = adamw(inputs={Beta1Pow=['linear_24.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_24.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_24.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['linear_24.w_0_fp32_master_0'], Moment1=['linear_24.w_0_fp32_master_0_moment1_0'], Moment2=['linear_24.w_0_fp32_master_0_moment2_0'], Param=['linear_24.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_31/, op_role = 2, op_role_var = ['linear_24.w_0', 'linear_24.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_26.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_26.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_26.w_0_fp32_master_0'], Moment1Out=['linear_26.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_26.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_26.w_0']} = adamw(inputs={Beta1Pow=['linear_26.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_26.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_26.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['linear_26.w_0_fp32_master_0'], Moment1=['linear_26.w_0_fp32_master_0_moment1_0'], Moment2=['linear_26.w_0_fp32_master_0_moment2_0'], Param=['linear_26.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_33/, op_role = 2, op_role_var = ['linear_26.w_0', 'linear_26.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_8.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_8.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['create_parameter_8.w_0_fp32_master_0'], Moment1Out=['create_parameter_8.w_0_fp32_master_0_moment1_0'], Moment2Out=['create_parameter_8.w_0_fp32_master_0_moment2_0'], ParamOut=['create_parameter_8.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_8.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_8.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_8.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['create_parameter_8.w_0_fp32_master_0'], Moment1=['create_parameter_8.w_0_fp32_master_0_moment1_0'], Moment2=['create_parameter_8.w_0_fp32_master_0_moment2_0'], Param=['create_parameter_8.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_37/, op_role = 2, op_role_var = ['create_parameter_8.w_0', 'create_parameter_8.w_0@GRAD'], use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['llama_lm_head_auto_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['llama_lm_head_auto_0.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['llama_lm_head_auto_0.w_0_fp32_master_0'], Moment1Out=['llama_lm_head_auto_0.w_0_fp32_master_0_moment1_0'], Moment2Out=['llama_lm_head_auto_0.w_0_fp32_master_0_moment2_0'], ParamOut=['llama_lm_head_auto_0.w_0']} = adamw(inputs={Beta1Pow=['llama_lm_head_auto_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['llama_lm_head_auto_0.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['llama_lm_head_auto_0.w_0@GRAD'], LearningRate=['learning_rate_0'], MasterParam=['llama_lm_head_auto_0.w_0_fp32_master_0'], Moment1=['llama_lm_head_auto_0.w_0_fp32_master_0_moment1_0'], Moment2=['llama_lm_head_auto_0.w_0_fp32_master_0_moment2_0'], Param=['llama_lm_head_auto_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_38/, op_role = 2, op_role_var = ['llama_lm_head_auto_0.w_0', 'llama_lm_head_auto_0.w_0@GRAD'], use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Out=['create_parameter_4.w_0']} = c_broadcast(inputs={X=['create_parameter_4.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_14.w_0']} = c_broadcast(inputs={X=['linear_14.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_15.w_0']} = c_broadcast(inputs={X=['linear_15.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_16.w_0']} = c_broadcast(inputs={X=['linear_16.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_17.w_0']} = c_broadcast(inputs={X=['linear_17.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_5.w_0']} = c_broadcast(inputs={X=['create_parameter_5.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_18.w_0']} = c_broadcast(inputs={X=['linear_18.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_19.w_0']} = c_broadcast(inputs={X=['linear_19.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_20.w_0']} = c_broadcast(inputs={X=['linear_20.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_6.w_0']} = c_broadcast(inputs={X=['create_parameter_6.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_21.w_0']} = c_broadcast(inputs={X=['linear_21.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_22.w_0']} = c_broadcast(inputs={X=['linear_22.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_23.w_0']} = c_broadcast(inputs={X=['linear_23.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_24.w_0']} = c_broadcast(inputs={X=['linear_24.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_7.w_0']} = c_broadcast(inputs={X=['create_parameter_7.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_25.w_0']} = c_broadcast(inputs={X=['linear_25.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_26.w_0']} = c_broadcast(inputs={X=['linear_26.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_27.w_0']} = c_broadcast(inputs={X=['linear_27.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_8.w_0']} = c_broadcast(inputs={X=['create_parameter_8.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['llama_lm_head_auto_0.w_0']} = c_broadcast(inputs={X=['llama_lm_head_auto_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, op_role_var = [], ring_id = 28, root = 0, use_calc_stream = True, with_quant_attr = False)
}