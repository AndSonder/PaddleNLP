grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
[33m[2024-03-01 04:42:19,671] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[32m[2024-03-01 04:42:19,672] [    INFO][0m - PP configs:{"enable":"True","schedule_mode":"1F1B","vpp_degree":"1","vpp_seg_method":"","micro_batch_size":"1","accumulate_steps":"16","generation_batch_size":"1","enable_send_recv_overlap":"False","job_schedule_profiler_start":"-1","job_schedule_profiler_stop":"-1",}, use master_grad: False[0m
[33m[2024-03-01 04:42:19,672] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[2024-03-01 04:42:19,673] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_call_stack_level', current_value=2, default_value=1)
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0301 04:42:19.674790 12188 tcp_utils.cc:130] Successfully connected to 172.17.0.3:53271
I0301 04:42:19.692351 12188 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
I0301 04:42:19.693529 12188 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 04:42:19,693] [    INFO] topology.py:359 - Total 2 pipe comm group(s) create successfully!
W0301 04:42:19.694864 12188 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.8
W0301 04:42:19.697115 12188 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
I0301 04:42:24.938513 12188 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 04:42:24,939] [    INFO] topology.py:359 - Total 2 data comm group(s) create successfully!
[2024-03-01 04:42:24,939] [    INFO] topology.py:359 - Total 4 model comm group(s) create successfully!
[2024-03-01 04:42:24,940] [    INFO] topology.py:359 - Total 4 sharding comm group(s) create successfully!
I0301 04:42:24.940469 12188 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
I0301 04:42:24.940575 12188 process_group_nccl.cc:130] ProcessGroupNCCL pg_timeout_ 1800000
[2024-03-01 04:42:24,940] [    INFO] topology.py:289 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 1, pp_degree: 2, dp_degree: 2, sep_degree: 1, mp_group: [2],  sharding_group: [2], pp_group: [2, 3], dp_group: [0, 2], sep:group: None, check/clip group: [2, 3]
[32m[2024-03-01 04:42:24,941] [    INFO][0m - {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 1024.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': [], 'custom_black_list': [], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': False}","sharding":"{'enable': True, 'stage': 2, 'degree': 2, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': True, 'k_steps': 16, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': '1F1B', 'vpp_degree': 1, 'vpp_seg_method': '', 'micro_batch_size': 1, 'accumulate_steps': 16, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': False}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': False}","sp_optimization":"{'enable': False}",}[0m
/home/workspace/PaddleNLP/llm/llama/auto_parallel/run_pretrain_auto_static.py:429: DeprecationWarning: Seeding based on hashing is deprecated
since Python 3.9 and will be removed in a subsequent version. The only 
supported seed types are: None, int, float, str, bytes, and bytearray.
  random.seed(random_seed)
[32m[2024-03-01 04:42:24,946] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_call_stack_level', current_value=2, default_value=1)
FLAGS(name='FLAGS_curand_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/usr/local/lib/python3.9/dist-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
[35m[2024-03-01 04:42:24,949] [   DEBUG][0m - ============================================================[0m
[35m[2024-03-01 04:42:24,949] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-03-01 04:42:24,949] [   DEBUG][0m - paddle commit id              : ab0f50d88a7cf8aa431a63794292ec56da388e59[0m
[35m[2024-03-01 04:42:24,949] [   DEBUG][0m - paddlenlp commit id           : f6eb5ca6555cf3ea81b445fa3ad34d81418a6df4.dirty[0m
[35m[2024-03-01 04:42:24,949] [   DEBUG][0m - config_name                   : None[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - continue_training             : False[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - fuse_attention_ffn            : False[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - fuse_attention_qkv            : False[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - hidden_size                   : None[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - intermediate_size             : None[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - model_name_or_path            : facebook/llama-7b[0m
[35m[2024-03-01 04:42:24,950] [   DEBUG][0m - model_type                    : llama[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - num_attention_heads           : None[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - num_hidden_layers             : None[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-03-01 04:42:24,951] [   DEBUG][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - use_flash_attention           : False[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - vocab_size                    : None[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - [0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - ============================================================[0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-03-01 04:42:24,952] [   DEBUG][0m - paddle commit id              : ab0f50d88a7cf8aa431a63794292ec56da388e59[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - paddlenlp commit id           : f6eb5ca6555cf3ea81b445fa3ad34d81418a6df4.dirty[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - data_cache                    : None[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - data_impl                     : mmap[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - input_dir                     : ../data[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - share_folder                  : False[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - skip_warmup                   : True[0m
[35m[2024-03-01 04:42:24,953] [   DEBUG][0m - split                         : 949,50,1[0m
[35m[2024-03-01 04:42:24,954] [   DEBUG][0m - [0m
[33m[2024-03-01 04:42:24,954] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-03-01 04:42:24,954] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-03-01 04:42:24,955] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-03-01 04:42:24,978] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-03-01 04:42:24,979] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-03-01 04:42:25,155] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-03-01 04:42:25,157] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-03-01 04:42:25,159] [    INFO][0m - Reset vocab size to 32000 for batter amp peformance.[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 4,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "seq_length": 2048,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": true,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

Using old dataet (.npy & .npz)
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-03-01 04:42:25,945] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-03-01 04:42:25,970] [    INFO][0m - . How?? Well thats the magic!! Its a bit complicated to explain here. Get the book and read Chapter 9. But it works! Check this out:Amazing isn't it? And the even more amazing thing is that it is not specific to factorial. See thisWoohoo!! On a roll now..You can take any recursive function, and rewrite it in the above style and the Y combinator will make a recursive version of it. How cool is that?</s> Perception.

Computational models of facial attractiveness judgments.

Strikingly, both models produced estimates of facial attractiveness that were indistinguishable from human ratings.

The image factors that the model discovered correspond to two of the main contemporary hypotheses of averageness judgments: facial attractiveness and sexual dimorphism.

This provides novel evidence for the importance of averageness and sexual dimorphism, but not symmetry, in human judgments of facial attractiveness.

Very related: Computer taught to recognize female attractiveness 2008;37(1):126-42.Related ArticlesBronstad PM, Langlois JH, Russell R.We designed two computational models to replicate human facial attractiveness ratings. The primary model used partial least squares (PLS) to identify image factors associated with facial attractiveness from facial images and attractiveness ratings of those images. For comparison we also made a model similar to previous models of facial attractiveness, in that it used manually derived measurements between features as inputs, though we took the additional step of dimensionality reduction via principal component analysis (PCA) and weighting of PCA dimensions via a perceptron.Because PLS extracts a small number of image factors from the facial images that covary with attractiveness ratings of the images, it is possible to determine the information used by the model.In contrast, facial symmetry was not important to the model, and an explicit feature-based measurement of symmetry was not correlated with human judgments of facial attractiveness.</s> The news comes after the Telegraph launched a campaign calling for a fair deal for motorists. This follows a 50 per cent increase in the amount of car-related tax since Labour came to power.</s> Mengenal Gaya Bermain Poker Online Pada Pemain Profesional

Mengenal Gaya Bermain Poker Online Pada Pemain Profesional – Ada beberapa hal yang harus anda ketahui ketika anda bermain poker online ini. Salah satunya adalah mengenal gaya bermain poker online pada setiap pemain yang melakukan taruhan ini. Gaya bermain ini sering dikatakan sebagai playing styles, dimana terdapat banyak sekali gaya yang harus anda ketahui. Berikut […]</s>U.S. troops working the streets of the capital fear one Iraqi weapon more than others -- a copper-plated explosive that can penetrate armor and has proved devastating to Humvees and even capable of severely damaging tanks.

The power of what the military calls an EFP -- for explosively formed penetrator, or projectile -- to spray molten metal balls that punch through the armor on vehicles has some American troops rethinking their tactics. They are asking whether the U.S. should give up its reliance on making constant improvements to vehicle defenses.

Instead, these troops think, it is time to leave the armor behind -- and get out and walk.

“In our area, the biggest threat for us is EFPs. When you are in the vehicles, you are a big target,” said Army Staff Sgt. Cavin Moskwa, 33, of Hawaii, who patrols Baghdad’s Zafraniya neighborhood with the Bravo Battery of the 2nd Battalion, 17th Field Artillery Regiment. “But when you are dismounted... you are a lot safer.”

Advertisement

In the last three days, 15 U.S. troops have been killed in Iraq, nine of them in two powerful roadside bomb blasts. The military does not publicly identify the kind of weapon used in improvised explosive attacks, but the deadly nature of the blasts Wednesday and Thursday suggested that EFPs may have been used.

The deaths brought to 3,545 the total number of U.S. troops killed in the Iraq theater since the March 2003 American-led invasion, the U.S. military said. Hundreds of these troops have been killed by EFPs and other kinds of improvised explosive devices, or IEDs. The Pentagon’s most recent Iraq status report said EFP attacks were at an all-time high.

Foot patrols, of course, are not a fail-safe method. On city streets, snipers remain a threat. And bombs can still kill dismounted troops. But when blasts occur in the middle of a foot patrol, the number of casualties are generally lower because the troops are more spread out.

Before a foot patrol last week through a neighborhood next to Baghdad’s Sadr City district, a private with Alpha Company of the Army’s 1st Battalion, 8th Cavalry Regiment, began complaining about having to walk. But EFPs have claimed the lives of several soldiers in the unit, and Sgt. Leland Kidd, 28, of Gonzales, Texas, said the private should be thankful they were on foot.

Advertisement

“When I walk on my feet, I don’t have to worry about being blown up,” Kidd told the private. “In the vehicle, I have to.”

Top commanders have been encouraging more such units in Baghdad to take just that tack.

A counterinsurgency guidance memo released last week by Army Lt. Gen Raymond T. Odierno, the commander of day-to-day military operations, urges Iraqi and American troops to “get out and walk.”

The memo argues that although Humvees offer protection, they also make units predictable and “insulate us from the Iraqi people we intend to secure.”

Advertisement

The original draft of the memo, written by counterinsurgency expert David Kilcullen, goes further. It notes that EFP attacks on Humvees damage them heavily. “So we gain little in safety, but sacrifice much in effectiveness,” the draft reads.

One reason for the increased number of troops victimized by roadside bombs is that there are more forces in Iraq now, Marine Gen. Peter Pace, chairman of the Joint Chiefs of Staff, said at a Pentagon news conference Thursday. This month, the final additional American combat units arrived in Baghdad, as part of a counterinsurgency strategy announced by President Bush in January that has increased the U.S. military presence in Iraq by 28,500 troops.

“As we’re taking the fight to the enemy with the additional troops, we can expect that there’s going to be tough fighting ahead,” Pace said. “So it is an expectation that this surge is going to result in more contact and therefore more casualties.”

But another reason for the rising death toll is the ability of Iraq’s militants to adapt to new U.S. military tactics.

Advertisement

During the 2003 invasion, most American Humvees were outfitted with flimsy canvas doors. When the first improvised explosive devices made from artillery shells appeared, the military scrambled to put stronger armor on the vehicles. Since then, the military has repeatedly upgraded Humvee armor as militants have made bigger and bigger bombs.

But the small and easily hidden EFPs, which often are powered by C-4 plastic explosives, are not just a more powerful IED. Military personnel experienced with the projectiles say that what makes the weapons so deadly is that they use the Americans’ own armor against them. As the hot copper slug melts through the armor of a Humvee, it transforms the protective plating into shrapnel that sprays into the passenger cabin, they say.

“We joked about going back to canvas doors. That way, unless it hits you directly, you are OK,” said Army Sgt. William Bowman, 31, of Fort Myers, Fla.

But to Moskwa, the staff sergeant from Hawaii, the question of armor is no joke. Moskwa, who served as an Army recruiter in Pasadena before deploying to Iraq, thinks armor on vehicles and body armor on troops are too restrictive, hampering a service member’s ability to move quickly and agilely.

Advertisement

“I would rather go out without any armor or gear,” he said. “If an EFP hits the vehicle, you are dead anyway no[0m
[32m[2024-03-01 04:42:25,971] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-03-01 04:42:25,993] [    INFO][0m - intolerant of large, polluting industrial plants on their doorsteps. Second, American power companies are fearful that they will soon have to pay for one particular pollutant, carbon dioxide, as is starting to happen in other parts of the rich world. Having invested heavily in gas-fired stations, only to find themselves locked into an increasingly expensive fuel, they do not want to make another mistake.

That has opened up a capacity gap and an opportunity for wind and sunlight. The future price of these resources—zero—is known. That certainty has economic value as a hedge, even if the capital cost of wind and solar power stations is, at the moment, higher than that of coal-fired ones.

The reasons for the boom, then, are tangled, and the way they are perceived may change. Global warming, a long-range phenomenon, may not be uppermost in people's minds during an economic downturn. High fuel prices may fall as new sources of supply are exploited to fill rising demand from Asia. Security of supply may improve if hostile governments are replaced by friendly ones and sources become more diversified. But none of the reasons is likely to go away entirely.

Global warming certainly will not. “Peak oil”, if oil means the traditional sort that comes cheaply out of holes in the ground, probably will arrive soon. There is oil aplenty of other sorts (tar sands, liquefied coal and so on), so the stuff is unlikely to run out for a long time yet. But it will get more expensive to produce, putting a floor on the price that is way above today's. And political risk will always be there—particularly for oil, which is so often associated with bad government for the simple reason that its very presence causes bad government in states that do not have strong institutions to curb their politicians.

A prize beyond the dreams of avarice

The market for energy is huge. At present, the world's population consumes about 15 terawatts of power. (A terawatt is 1,000 gigawatts, and a gigawatt is the capacity of the largest sort of coal-fired power station.) That translates into a business worth $6 trillion a year—about a tenth of the world's economic output—according to John Doerr, a venture capitalist who is heavily involved in the industry. And by 2050, power consumption is likely to have risen to 30 terawatts.

Scale is one of the important differences between the coming energy boom, if it materialises, and its recent predecessors—particularly those that relied on information technology, a market measured in mere hundreds of billions. Another difference is that new information technologies tend to be disruptive, forcing the replacement of existing equipment, whereas, say, building wind farms does not force the closure of coal-fired power stations.

For both of these reasons, any transition from an economy based on fossil fuels to one based on renewable, alternative, green energy—call it what you will—is likely to be slow, as similar changes have been in the past (see chart 1). On the other hand, the scale of the market provides opportunities for alternatives to prove themselves at the margin and then move into the mainstream, as is happening with wind power at the moment. And some energy technologies do have the potential to be disruptive. Plug-in cars, for example, could be fuelled with electricity at a price equivalent to 25 cents a litre of petrol. That could shake up the oil, carmaking and electricity industries all in one go.

The innovation lull of the past few decades also provides opportunities for technological leapfrogging. Indeed, it may be that the field of energy gives the not-quite-booms in biotechnology and nanotechnology the industrial applications they need to grow really big, and that the three aspiring booms will thus merge into one.

The possibility of thus recapturing the good times of their youth has brought many well-known members of the “technorati” out of their homes in places like Woodside, California. Energy has become supercool. Elon Musk, who co-founded PayPal, has developed a battery-powered sports car. Larry Page and Sergey Brin, the founders of Google, have started an outfit called Google.org that is searching for a way to make renewable energy truly cheaper than coal (or RE

Vinod Khosla, one of the founders of Sun Microsystems, is turning his considerable skills as a venture capitalist towards renewable energy, as are Robert Metcalfe, who invented the ethernet system used to connect computers together in local networks, and Mr Doerr, who works at Kleiner Perkins Caufield & Byers, one of Silicon Valley's best-known venture-capital firms. Sir Richard Branson, too, is getting in on the act with his Virgin Green Fund.

This renewed interest in energy is bringing forth a raft of ideas, some bright, some batty, that is indeed reminiscent of the dotcom boom. As happened in that boom, most of these ideas will come to naught. But there could just be a PayPal or a Google or a Sun among them.

More traditional companies are also taking an interest. General Electric (GE), a large American engineering firm, already has a thriving wind-turbine business and is gearing up its solar-energy business. The energy researchers at its laboratories in Schenectady, New York, enjoy much of the intellectual freedom associated with start-up firms, combined with a secure supply of money.

Meanwhile, BP and Shell, two of the world's biggest oil companies, are sponsoring both academic researchers and new, small firms with bright ideas, as is DuPont, one of the biggest chemical companies. Not everyone has joined in. Exxon Mobil, the world's largest oil company not in government hands, is conspicuously absent. But in many boardrooms renewables are no longer seen as just a way of keeping environmentalists off companies' backs.

Some people complain that many existing forms of renewable energy rely on subsidies or other forms of special treatment for their viability. On the surface, that is true. Look beneath, though, and the whole energy sector is riddled with subsidies, both explicit and hidden, and costs that are not properly accounted for. Drawing on the work of people like Boyden Gray, a former White House counsel, Mr Woolsey estimates that American oil companies receive preferential treatment from their government worth more than $250 billion a year. And the Intergovernmental Panel on Climate Change (IPCC), a United Nations-appointed group of scientific experts, reckons that fossil fuels should carry a tax of $20-50 for every tonne of carbon dioxide they generate in order to pay for the environmental effects of burning them (hence the fears of the power-generators).

So the subsidies and mandates offered to renewable sources of power such as wind turbines often just level the playing field. It is true that some subsidies amount to unwarranted market-rigging: examples include those handed by cloudy Germany to its solar-power industry and by America to its maize-based ethanol farmers when Brazilian sugar-based ethanol is far cheaper. Others, though, such as a requirement that a certain proportion of electricity be derived from non-fossil-fuel sources, make no attempt to pick particular technological winners. They merely act to stimulate innovation by guaranteeing a market to things that actually work.

If the world were rational, all of these measures would be swept away and replaced by a proper tax on carbon—as is starting to happen in Europe, where the price arrived at by the cap-and-trade system being introduced is close to the IPCC's recommendation. If that occurred, wind-based electricity would already be competitive with fossil fuels and others would be coming close. Failing that, special treatment for alternatives is probably the least bad option—though such measures need to be crafted in ways that favour neither incumbents nor particular ways of doing things, and need to be withdrawn when they are no longer necessary.

The poor world turns greener too

That, at least, is the view from the rich world. But poorer, rapidly developing countries are also taking more of an interest in renewable energy sources, despite assertions to the contrary by some Western politicians and businessmen. It is true that China is building coal-fired power stations at a blazing rate. But it also has a large wind-generation capacity, which is expected to grow by two-thirds this year, and is the world's second-largest manufacturer of solar panels—not to mention having the largest number of solar-heated rooftop hot-water systems in its buildings.

Brazil, meanwhile, has the world's second-largest (just behind America) and most econom[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - ============================================================[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m -          Configuration Arguments        [0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - paddle commit id              : ab0f50d88a7cf8aa431a63794292ec56da388e59[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - paddlenlp commit id           : f6eb5ca6555cf3ea81b445fa3ad34d81418a6df4.dirty[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-03-01 04:42:25,994] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - bf16                          : False[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-03-01 04:42:25,995] [    INFO][0m - dataloader_num_workers        : 1[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - decay_steps                   : 20[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - device                        : gpu[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - do_eval                       : True[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - do_export                     : False[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - do_predict                    : False[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - do_train                      : True[0m
[32m[2024-03-01 04:42:25,996] [    INFO][0m - enable_auto_parallel          : True[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - eval_batch_size               : 16[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - eval_steps                    : 1000[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - fp16                          : True[0m
[32m[2024-03-01 04:42:25,997] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - fused_linear_param_grad_add   : False[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - gradient_accumulation_steps   : 16[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - greater_is_better             : None[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - hybrid_parallel_topo_order    : None[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - job_schedule_profiler_end     : -1[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - job_schedule_profiler_start   : -1[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - label_names                   : None[0m
[32m[2024-03-01 04:42:25,998] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - learning_rate                 : 0.0001[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - log_level                     : -1[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - logging_dir                   : output/llama_auto_static_dp2sharding2mp2pp2_vpp2/runs/Mar01_04-42-19_c4d082bc6a0b[0m
[32m[2024-03-01 04:42:25,999] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - logical_process_index         : 4[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - max_steps                     : 20[0m
[32m[2024-03-01 04:42:26,000] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - optimizer_name_suffix         : None[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - output_dir                    : output/llama_auto_static_dp2sharding2mp2pp2_vpp2[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-03-01 04:42:26,001] [    INFO][0m - past_index                    : -1[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - per_device_eval_batch_size    : 16[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - per_device_train_batch_size   : 1[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - pipeline_parallel_degree      : 2[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - pipeline_schedule_mode        : 1F1B[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - power                         : 1.0[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - process_index                 : 2[0m
[32m[2024-03-01 04:42:26,002] [    INFO][0m - recompute                     : True[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - refined_ops_patterns          : None[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - run_name                      : output/llama_auto_static_dp2sharding2mp2pp2_vpp2[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - save_steps                    : 5000[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-03-01 04:42:26,003] [    INFO][0m - save_total_limit              : None[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - scale_loss                    : 1024.0[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - seed                          : 42[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_GRAD_OP: 'stage2'>][0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-03-01 04:42:26,004] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - should_log                    : True[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - should_save                   : True[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - sr                            : 0[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - strategy                      : {"auto_mode":"semi","gradient_scale":"True","use_cache":"True","return_numpy":"True","all_ranks":"False","split_data":"True","seed":"None","reinit":"False","recompute":"{'enable': True, 'checkpoints': [], 'no_recompute_segments': [], 'sr': 0, 'refined_ops_patterns': [], 'enable_tuning': False}","amp":"{'enable': True, 'dtype': 'float16', 'level': 'o2', 'init_loss_scaling': 1024.0, 'incr_every_n_steps': 1000, 'decr_every_n_nan_or_inf': 2, 'incr_ratio': 2.0, 'decr_ratio': 0.8, 'use_dynamic_loss_scaling': True, 'custom_white_list': [], 'custom_black_list': [], 'custom_black_varnames': [], 'use_fp16_guard': False, 'use_bf16_guard': False, 'use_master_grad': False}","sharding":"{'enable': True, 'stage': 2, 'degree': 2, 'enable_overlap': False, 'param_comm_stream_num': 1, 'grad_comm_stream_num': 1, 'param_bucket_size_numel': 1, 'grad_bucket_size_numel': 1, 'enable_hierarchical_comm': False, 'partition_algor': 'greedy_even', 'enable_tuning': False, 'tuning_range': []}","gradient_merge":"{'enable': True, 'k_steps': 16, 'avg': True}","pipeline":"{'enable': True, 'schedule_mode': '1F1B', 'vpp_degree': 1, 'vpp_seg_method': '', 'micro_batch_size': 1, 'accumulate_steps': 16, 'generation_batch_size': 1, 'enable_send_recv_overlap': False, 'job_schedule_profiler_start': -1, 'job_schedule_profiler_stop': -1}","qat":"{'enable': False, 'channel_wise_abs_max': True, 'weight_bits': 8, 'activation_bits': 8, 'not_quant_pattern': ['skip_quant'], 'algo': None, 'onnx_format': True}","tuning":"{'enable': False, 'profile_start_step': 1, 'profile_end_step': 1, 'run_after_tuning': True, 'debug': False}","dataset":"{'enable': False, 'num_shards': 1}","fused_passes":"{'enable': False, 'fused_passes_list': []}","fused_linear_promotion":"{'enable': False}","dp_optimization":"{'enable': False, 'fuse_all_reduce_ops': True, 'fuse_grad_size_in_MB': 32, 'overlap_comm_cacl': True}","mp_optimization":"{'allreduce_matmul_grad_overlapping': False}","sp_optimization":"{'enable': False}",}[0m
[32m[2024-03-01 04:42:26,005] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - tensor_parallel_degree        : 1[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - to_static                     : False[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - train_batch_size              : 1[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - use_hybrid_parallel           : False[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - virtual_pipeline_seg_method   : LlamaDecoderLayerAuto[0m
[32m[2024-03-01 04:42:26,006] [    INFO][0m - wandb_api_key                 : None[0m
[32m[2024-03-01 04:42:26,007] [    INFO][0m - warmup_ratio                  : 0.01[0m
[32m[2024-03-01 04:42:26,007] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-03-01 04:42:26,007] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-03-01 04:42:26,007] [    INFO][0m - weight_name_suffix            : None[0m
[32m[2024-03-01 04:42:26,007] [    INFO][0m - world_size                    : 4[0m
[32m[2024-03-01 04:42:26,007] [    INFO][0m - [0m
[2024-03-01 04:42:26,009] [    INFO] cluster.py:914 - Node Count: 1, Local Device Size: 4, GPU Model: NVIDIA GeForce GTX 1080 Ti, GPU Memory: 11GB, World size: 4, EndPoint: 172.17.0.3:53274.
[2024-03-01 04:42:26,009] [    INFO] engine.py:202 - Distribute training by paddle.distributed.launch
[2024-03-01 04:42:26,009] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
[2024-03-01 04:42:26,010] [ WARNING] fleet.py:272 - The dygraph parallel environment has been initialized.
[2024-03-01 04:42:26,010] [ WARNING] fleet.py:295 - The dygraph hybrid parallel environment has been initialized.
[2024-03-01 04:42:26,010] [    INFO] engine.py:655 - Building model with 'to_static' method.
INFO 2024-03-01 04:42:26,011 helper.py:245] start to build program for mode = train.
/usr/local/lib/python3.9/dist-packages/paddle/base/framework.py:3124: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
WARNING: there are some orphan tensors or ops which are not used in the execution.
[2024-03-01 04:42:32,866] [    INFO] parallelizer_v2.py:283 - Applying AMP-float16-o2 ...
[2024-03-01 04:42:33,299] [    INFO] auto_parallel_recompute.py:392 - The excluded ops in recompute segments are:
[[], [], []]
INFO 2024-03-01 04:42:35,392 auto_parallel_sharding.py:1762] Sharding the Order of param being used: ['embedding_0.w_0', 'create_parameter_0.w_0', 'linear_0.w_0', 'linear_1.w_0', 'linear_2.w_0', 'linear_3.w_0', 'create_parameter_1.w_0', 'linear_4.w_0', 'linear_5.w_0', 'linear_6.w_0', 'create_parameter_2.w_0', 'linear_7.w_0', 'linear_8.w_0', 'linear_9.w_0', 'linear_10.w_0', 'create_parameter_3.w_0', 'linear_11.w_0', 'linear_12.w_0', 'linear_13.w_0'].
INFO 2024-03-01 04:42:35,392 auto_parallel_sharding.py:1704] Sharding Parameter Partition:
INFO 2024-03-01 04:42:35,392 auto_parallel_sharding.py:1706] Rank:0, Parameter Size:540.0078125 MB.
INFO 2024-03-01 04:42:35,392 auto_parallel_sharding.py:1709] Params in this rank: ['embedding_0.w_0', 'linear_6.w_0', 'linear_9.w_0', 'create_parameter_3.w_0', 'linear_11.w_0', 'linear_13.w_0'].
INFO 2024-03-01 04:42:35,393 auto_parallel_sharding.py:1706] Rank:1, Parameter Size:482.0234375 MB.
INFO 2024-03-01 04:42:35,393 auto_parallel_sharding.py:1709] Params in this rank: ['create_parameter_0.w_0', 'linear_0.w_0', 'linear_1.w_0', 'linear_2.w_0', 'linear_3.w_0', 'create_parameter_1.w_0', 'linear_4.w_0', 'linear_5.w_0', 'create_parameter_2.w_0', 'linear_7.w_0', 'linear_8.w_0', 'linear_10.w_0', 'linear_12.w_0'].
[_remove_and_get_optimizer_op] op type:  check_finite_and_unscale
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  c_allreduce_max
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  concat
[_remove_and_get_optimizer_op] op type:  reduce_any
[_remove_and_get_optimizer_op] op type:  memcpy_d2h
[_remove_and_get_optimizer_op] op type:  update_loss_scaling
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  squared_l2_norm
[_remove_and_get_optimizer_op] op type:  stack
[_remove_and_get_optimizer_op] op type:  reduce_sum
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  c_allreduce_sum
[_remove_and_get_optimizer_op] op type:  sqrt
[_remove_and_get_optimizer_op] op type:  fill_constant
[_remove_and_get_optimizer_op] op type:  elementwise_max
[_remove_and_get_optimizer_op] op type:  elementwise_div
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  cast
[_remove_and_get_optimizer_op] op type:  elementwise_mul
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  adamw
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
[_remove_and_get_optimizer_op] op type:  c_broadcast
params_grads [(persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False), var linear_12.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)), (persist trainable param linear_10.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_10.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_8.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var create_parameter_2.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False), var linear_5.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)), (persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False), var linear_4.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)), (persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var create_parameter_1.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)), (persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_3.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_2.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_1.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), var linear_0.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)), (persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), var create_parameter_0.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False))]
new_params_to_grads [[persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var create_parameter_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var create_parameter_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False), persist var linear_4.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)], [persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False), persist var linear_5.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)], [persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False), persist var create_parameter_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)], [persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_7.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_8.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_10.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False), persist var linear_10.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)], [persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False), persist var linear_12.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)]]
op type:  check_finite_and_unscale
op type:  cast
op type:  c_allreduce_max
op type:  cast
op type:  concat
op type:  reduce_any
op type:  memcpy_d2h
op type:  update_loss_scaling
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  squared_l2_norm
op type:  stack
op type:  reduce_sum
op type:  cast
op type:  c_allreduce_sum
op type:  sqrt
op type:  fill_constant
op type:  elementwise_max
op type:  elementwise_div
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  cast
op type:  elementwise_mul
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  adamw
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
op type:  c_broadcast
{ // block_idx:0  parent_idx:-1  forward_idx:-1  backward_idx:-1
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var input_ids : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    persist trainable param embedding_0.w_0 : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(2, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var fill_constant_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    var range_0.tmp_0 : LOD_TENSOR.shape(2048,).dtype(int64).stop_gradient(True)
    var expand_0.tmp_0 : LOD_TENSOR.shape(2, 2048).dtype(int64).stop_gradient(True)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(2, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, 2, 2048).dtype(bool).stop_gradient(True)
    var tmp_0 : LOD_TENSOR.shape(2, 1, 1, 2048).dtype(bool).stop_gradient(True)
    var expand_1.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_13.tmp_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var tril_0 : LOD_TENSOR.shape(2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var unsqueeze2_1.tmp_1 : LOD_TENSOR.shape(0, 2048, 2048).dtype(bool).stop_gradient(True)
    var expand_2.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var bitwise_and_0.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var fill_constant_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var fill_constant_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var full_like_2.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var cast_0.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_1.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_0 : LOD_TENSOR.shape(1,).dtype(float64).stop_gradient(True)
    var elementwise_add_1 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_2 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_3 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var elementwise_add_4 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var cast_2.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(bool).stop_gradient(True)
    var where_0.tmp_0 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float64).stop_gradient(True)
    var tmp_1 : LOD_TENSOR.shape(2, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_0.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_0.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_1.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_2.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_1 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_4.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_6 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_3.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_15 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_3.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_1.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_4.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_5.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_6.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_2.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_7.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_8.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_9.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var eager_tmp_4 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    persist var eager_tmp_5 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_0 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_6.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_8.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_8.tmp_1 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_28 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_8.tmp_1 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_37 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    persist trainable param linear_10.w_0 : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_42 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param create_parameter_3.w_0 : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist trainable param linear_11.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_12.w_0 : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    persist trainable param linear_13.w_0 : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var labels : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(True)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_24.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_25.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_26.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var squeeze_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_2.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_3.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_6.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_28.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_29.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_30.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_31.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_32.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_33.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_34.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_35.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_36.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_8.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_37.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_38.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_1.subprog_2 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_41.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_42.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_43.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_44.subprog_2 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0.subprog_2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_13.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_44@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_13.w_0@GRAD : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var silu_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_12.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_12.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_11.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_11.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var tmp_43@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_42@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_3.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_10.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_10.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_1@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_9.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_9.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_8.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_8.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_7.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_7.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_2.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_2.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_3.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_4.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 4096).dtype(float16).stop_gradient(False)
    var squeeze_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1024, 128).dtype(float16).stop_gradient(True)
    var squeeze_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(int64).stop_gradient(True)
    var unsqueeze2_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048).dtype(int64).stop_gradient(True)
    var gather_nd_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1, 128).dtype(float16).stop_gradient(True)
    var unsqueeze2_5.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 128).dtype(float16).stop_gradient(True)
    var tmp_6.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_7.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_8.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_9.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_10.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var tmp_11.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_12.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_13.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_14.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var reshape2_3.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 1, 2048, 2048).dtype(float16).stop_gradient(True)
    var tmp_15.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_16.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_1.subprog_3 : LOD_TENSOR.shape(0, 1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_19.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_20.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_21.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_22.subprog_3 : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0.subprog_3 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_6.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_6.w_0@GRAD : LOD_TENSOR.shape(11008, 4096).dtype(float16).stop_gradient(False)
    var silu_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var linear_5.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_5.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var linear_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 11008).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_4.w_0@GRAD : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_1.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_3.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_1@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var matmul_v2_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 128, 2048).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 32, 2048, 128).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var concat_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0_slice_1@GRAD : LOD_TENSOR.shape(1, 2048, 32, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_2.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_2.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_1.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_1.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 32, 128).dtype(float16).stop_gradient(False)
    var linear_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var linear_0.w_0@GRAD : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var create_parameter_0.w_0@GRAD : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    var rsqrt_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 1).dtype(float16).stop_gradient(False)
    var pow_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.tmp_0@GRAD : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var embedding_0.w_0@GRAD : LOD_TENSOR.shape(32000, 4096).dtype(float16).stop_gradient(False)
    var find_infinite_scale.@fp16_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.@fp16_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var concat.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    var memcopy__0 : LOD_TENSOR.shape().dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var opt_opt_squared_l2_norm_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_19.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_20.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_21.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_22.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_23.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_24.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_25.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_27.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_28.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_29.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_30.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_32.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_34.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_35.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_36.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_squared_l2_norm_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_opt_stack_0.tmp_0 : LOD_TENSOR.shape(39, 1).dtype(float16).stop_gradient(False)
    var opt_opt_sum_0.tmp_0 : LOD_TENSOR.shape().dtype(float16).stop_gradient(False)
    var opt_tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_sqrt_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var opt_opt_fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var opt_elementwise_max_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_elementwise_div_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var opt_tmp_2 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_3 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_4 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_5 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_6 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_7 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_9 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_10 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_11 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_12 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_14 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_16 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    var opt_tmp_18 : LOD_TENSOR.shape(1,).dtype(float16).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)
    persist var linear_0.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_0.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_0.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_1.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_1.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_2.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_2.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_3.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_3.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(True)
    persist var linear_4.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_4.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(True)
    persist var linear_5.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_5.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_0.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_0.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_1.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_1.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_7.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_7.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_8.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_8.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(True)
    persist var linear_10.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 4096).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_10.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(True)
    persist var linear_12.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096, 11008).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var linear_12.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(True)
    persist var create_parameter_2.w_0_fp32_master_0_moment1_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_0_moment2_0 : LOD_TENSOR.shape(4096,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var create_parameter_2.w_0_fp32_master_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var split@RESHARD.tmp_0 : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(False)
    var split@RESHARD.tmp_1 : LOD_TENSOR.shape(1, 2048).dtype(int64).stop_gradient(False)
    var split@RESHARD.tmp_2 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var split@RESHARD.tmp_3 : LOD_TENSOR.shape(1, 1, 2048, 2048).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@recv_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD@RESHARD_0 : LOD_TENSOR.shape(1, 2048, 4096).dtype(float16).stop_gradient(False)
    persist var create_parameter_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_0.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_3.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var create_parameter_1.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_4.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var linear_5.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var create_parameter_2.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096,).dtype(float16).stop_gradient(False)
    persist var linear_7.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_8.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_10.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 4096).dtype(float16).stop_gradient(False)
    persist var linear_12.w_0@GRAD@MERGE : LOD_TENSOR.shape(4096, 11008).dtype(float16).stop_gradient(False)
    persist var gradient_merge_k : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_zero : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_step : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var gradient_merge_cond : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(True)
    var _generated_var_0 : STEP_SCOPES)

    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 1, op original_id: 1, process_mesh (annotated): {shape: [2,2,1], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['embedding_0.tmp_0']} = lookup_table_v2(inputs={Ids=['input_ids'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, with_quant_attr = False, dist_attr = {op type: lookup_table_v2, op id: 2, op original_id: 2, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; input_ids's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; embedding_0.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; embedding_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 4, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False, dist_attr = {op type: fill_constant, op id: 3, op original_id: 3, process_mesh (annotated): {shape: [2,2,1], process_ids: [0,1,2,3], dim_names: [dp,pp,mp]}; fill_constant_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_9.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 2048, value = 2048.0, with_quant_attr = False)
    {Out=['fill_constant_11.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 3, force_cpu = True, op_device = cpu, op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 1, value = 1.0, with_quant_attr = False)
    {Out=['range_0.tmp_0']} = range(inputs={End=['fill_constant_9.tmp_0'], Start=['fill_constant_7.tmp_0'], Step=['fill_constant_11.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_0.tmp_0']} = expand_v2(inputs={Shape=[], X=['range_0.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [2, 2048], with_quant_attr = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['fill_constant_5.tmp_0']}, axes = [1, 2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_0']} = cast(inputs={X=['unsqueeze2_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['expand_1.tmp_0']} = expand_v2(inputs={Shape=[], X=['tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [2, 1, 2048, 2048], with_quant_attr = False)
    {Out=['fill_constant_13.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 0, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [2048, 2048], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['tril_0']} = tril_triu(inputs={X=['fill_constant_13.tmp_0']}, diagonal = 0, lower = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_1.tmp_0'], XShape=['unsqueeze2_1.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['tril_0']}, axes = [0, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['expand_2.tmp_0']} = expand_v2(inputs={Shape=[], X=['unsqueeze2_1.tmp_0'], expand_shapes_tensor=[]}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [2, 1, 2048, 2048], with_quant_attr = False)
    {Out=['bitwise_and_0.tmp_0']} = bitwise_and(inputs={X=['expand_1.tmp_0'], Y=['expand_2.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['fill_constant_15.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0, with_quant_attr = False)
    {Out=['fill_constant_17.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 6, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = -3.4028234663852886e+38, value = -3.4028234663852886e+38, with_quant_attr = False)
    {Out=['full_like_0.tmp_0']} = fill_any_like(inputs={X=['fill_constant_15.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_1.tmp_0']} = fill_any_like(inputs={X=['fill_constant_17.tmp_0']}, dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['full_like_2.tmp_0']} = fill_any_like(inputs={X=['bitwise_and_0.tmp_0']}, dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], value = 0.0, with_quant_attr = False)
    {Out=['cast_0.tmp_0']} = cast(inputs={X=['full_like_2.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['cast_1.tmp_0']} = cast(inputs={X=['bitwise_and_0.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 6, with_quant_attr = False)
    {Out=['elementwise_add_0']} = elementwise_add(inputs={X=['full_like_0.tmp_0'], Y=['full_like_1.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_1']} = elementwise_add(inputs={X=['elementwise_add_0'], Y=['cast_0.tmp_0']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_2']} = elementwise_add(inputs={X=['fill_constant_15.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_3']} = elementwise_add(inputs={X=['fill_constant_17.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['elementwise_add_4']} = elementwise_add(inputs={X=['cast_1.tmp_0'], Y=['elementwise_add_1']}, axis = -1, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['elementwise_add_4']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 0, with_quant_attr = False)
    {Out=['where_0.tmp_0']} = where(inputs={Condition=['cast_2.tmp_0'], X=['elementwise_add_2'], Y=['elementwise_add_3']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_1']} = cast(inputs={X=['where_0.tmp_0']}, in_dtype = 6, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_2']} = cast(inputs={X=['embedding_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 33, op original_id: 33, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_2's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_0.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 34, op original_id: 34, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_2's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['pow_0.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 35, op original_id: 35, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_3']} = scale(inputs={ScaleTensor=[], X=['mean_0.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 36, op original_id: 36, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_3's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_0.tmp_0']} = rsqrt(inputs={X=['tmp_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 37, op original_id: 37, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_3's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_4']} = elementwise_mul(inputs={X=['rsqrt_0.tmp_0'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 38, op original_id: 38, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_4's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_5']} = elementwise_mul(inputs={X=['tmp_4'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 39, op original_id: 39, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_4's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_0.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_5's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_0.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 40, op original_id: 40, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_0.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 41, op original_id: 41, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_1.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 42, op original_id: 42, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_1.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 43, op original_id: 43, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_2.tmp_0']} = matmul_v2(inputs={X=['tmp_5'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 44, op original_id: 44, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_2.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 45, op original_id: 45, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_0.tmp_0'], XShape=['squeeze_0.tmp_1']} = squeeze2(inputs={X=['eager_tmp_1']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 46, op original_id: 46, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_1.tmp_0'], XShape=['squeeze_1.tmp_1']} = squeeze2(inputs={X=['eager_tmp_2']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 47, op original_id: 47, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_2's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['split@RESHARD.tmp_0', 'split@RESHARD.tmp_1']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['expand_0.tmp_0']}, axis = 0, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['unsqueeze2_2.tmp_0'], XShape=['unsqueeze2_2.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 48, op original_id: 48, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_0.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_2.tmp_0'], X=['squeeze_0.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 49, op original_id: 49, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_3.tmp_0'], XShape=['unsqueeze2_3.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_0.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 50, op original_id: 50, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_4.tmp_0'], XShape=['unsqueeze2_4.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 51, op original_id: 51, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_1.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_4.tmp_0'], X=['squeeze_1.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 52, op original_id: 52, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_5.tmp_0'], XShape=['unsqueeze2_5.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_1.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 53, op original_id: 53, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_6']} = elementwise_mul(inputs={X=['reshape2_0.tmp_0'], Y=['unsqueeze2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 54, op original_id: 54, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_6's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 55, op original_id: 55, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_0.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 56, op original_id: 56, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_7']} = scale(inputs={ScaleTensor=[], X=['reshape2_0.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 57, op original_id: 57, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_0.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_7's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_0.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_7', 'reshape2_0.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 58, op original_id: 58, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_7's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_0.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_8']} = elementwise_mul(inputs={X=['concat_0.tmp_0'], Y=['unsqueeze2_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 59, op original_id: 59, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_8's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_9']} = elementwise_add(inputs={X=['tmp_6'], Y=['tmp_8']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 60, op original_id: 60, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_6's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_8's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_9's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_10']} = elementwise_mul(inputs={X=['reshape2_1.tmp_0'], Y=['unsqueeze2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 61, op original_id: 61, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_10's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 62, op original_id: 62, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_1.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 63, op original_id: 63, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_11']} = scale(inputs={ScaleTensor=[], X=['reshape2_1.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 64, op original_id: 64, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_1.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_11's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_1.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_11', 'reshape2_1.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 65, op original_id: 65, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_11's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_1.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_12']} = elementwise_mul(inputs={X=['concat_1.tmp_0'], Y=['unsqueeze2_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 66, op original_id: 66, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_12's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_13']} = elementwise_add(inputs={X=['tmp_10'], Y=['tmp_12']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 67, op original_id: 67, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_10's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_12's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_13's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['tmp_9']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 68, op original_id: 68, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_9's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['tmp_13']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 69, op original_id: 69, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_13's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 70, op original_id: 70, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_14']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 71, op original_id: 71, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_14's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['transpose_1.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 72, op original_id: 72, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['tmp_14'], Y=['transpose_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 73, op original_id: 73, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_14's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['split@RESHARD.tmp_2', 'split@RESHARD.tmp_3']} = split(inputs={AxisTensor=[], SectionsTensorList=[], X=['tmp_1']}, axis = 0, num = 2, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], sections = [], with_quant_attr = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 74, op original_id: 74, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_15']} = elementwise_add(inputs={X=['matmul_v2_0.tmp_0'], Y=['reshape2_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 75, op original_id: 75, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_15's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_0.tmp_0']} = cast(inputs={X=['tmp_15']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 76, op original_id: 76, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_15's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_0.tmp_1']} = softmax(inputs={X=['softmax_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 77, op original_id: 77, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_0.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_16']} = cast(inputs={X=['softmax_0.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 78, op original_id: 78, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_0.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_16's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['tmp_16'], Y=['transpose_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 79, op original_id: 79, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_16's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['matmul_v2_1.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 80, op original_id: 80, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_4.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 81, op original_id: 81, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_4.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_3.tmp_0']} = matmul_v2(inputs={X=['reshape2_4.tmp_0'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 82, op original_id: 82, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_3.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_17']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 83, op original_id: 83, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; embedding_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_17's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_18']} = cast(inputs={X=['tmp_17']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 84, op original_id: 84, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_18's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_1.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_18']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 85, op original_id: 85, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_18's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_1.tmp_0']} = reduce_mean(inputs={X=['pow_1.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 86, op original_id: 86, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_19']} = scale(inputs={ScaleTensor=[], X=['mean_1.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 87, op original_id: 87, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_19's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_1.tmp_0']} = rsqrt(inputs={X=['tmp_19']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 88, op original_id: 88, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_19's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_20']} = elementwise_mul(inputs={X=['rsqrt_1.tmp_0'], Y=['tmp_17']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 89, op original_id: 89, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_20's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_21']} = elementwise_mul(inputs={X=['tmp_20'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 90, op original_id: 90, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_20's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_1.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_21's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_4.tmp_0']} = matmul_v2(inputs={X=['tmp_21'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 91, op original_id: 91, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_21's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_4.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_4.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_0.tmp_0']} = silu(inputs={X=['linear_4.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 92, op original_id: 92, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_4.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_0.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_5.tmp_0']} = matmul_v2(inputs={X=['tmp_21'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 93, op original_id: 93, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_21's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_5.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_22']} = elementwise_mul(inputs={X=['silu_0.tmp_0'], Y=['linear_5.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 94, op original_id: 94, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; silu_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_22's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_6.tmp_0']} = matmul_v2(inputs={X=['tmp_22'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 95, op original_id: 95, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_22's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_6.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_23']} = elementwise_add(inputs={X=['tmp_17'], Y=['linear_6.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 96, op original_id: 96, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_17's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_23's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_24']} = cast(inputs={X=['tmp_23']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 97, op original_id: 97, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_24's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_2.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_24']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 98, op original_id: 98, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_24's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_2.tmp_0']} = reduce_mean(inputs={X=['pow_2.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 99, op original_id: 99, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_25']} = scale(inputs={ScaleTensor=[], X=['mean_2.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 100, op original_id: 100, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_25's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_2.tmp_0']} = rsqrt(inputs={X=['tmp_25']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 101, op original_id: 101, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_25's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_26']} = elementwise_mul(inputs={X=['rsqrt_2.tmp_0'], Y=['tmp_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 102, op original_id: 102, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_26's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_27']} = elementwise_mul(inputs={X=['tmp_26'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 103, op original_id: 103, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_26's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_2.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_27's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_7.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 104, op original_id: 104, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_7.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 105, op original_id: 105, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_8.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 106, op original_id: 106, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_8.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 107, op original_id: 107, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_9.tmp_0']} = matmul_v2(inputs={X=['tmp_27'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 108, op original_id: 108, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_27's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_9.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 109, op original_id: 109, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_2.tmp_0'], XShape=['squeeze_2.tmp_1']} = squeeze2(inputs={X=['eager_tmp_4']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 110, op original_id: 110, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_4's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_2.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['squeeze_3.tmp_0'], XShape=['squeeze_3.tmp_1']} = squeeze2(inputs={X=['eager_tmp_5']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: squeeze2, op id: 111, op original_id: 111, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; eager_tmp_5's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; squeeze_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1], partial on dims: []; squeeze_3.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_6.tmp_0'], XShape=['unsqueeze2_6.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 112, op original_id: 112, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_2.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_6.tmp_0'], X=['squeeze_2.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 113, op original_id: 113, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_7.tmp_0'], XShape=['unsqueeze2_7.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_2.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 114, op original_id: 114, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_8.tmp_0'], XShape=['unsqueeze2_8.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 115, op original_id: 115, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; expand_0.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; unsqueeze2_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['gather_nd_3.tmp_0']} = gather_nd(inputs={Index=['unsqueeze2_8.tmp_0'], X=['squeeze_3.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: gather_nd, op id: 116, op original_id: 116, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; unsqueeze2_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; squeeze_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1], partial on dims: []; gather_nd_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['unsqueeze2_9.tmp_0'], XShape=['unsqueeze2_9.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_3.tmp_0']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: unsqueeze2, op id: 117, op original_id: 117, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; gather_nd_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_28']} = elementwise_mul(inputs={X=['reshape2_5.tmp_0'], Y=['unsqueeze2_7.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 118, op original_id: 118, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_28's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 119, op original_id: 119, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_5.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 120, op original_id: 120, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_29']} = scale(inputs={ScaleTensor=[], X=['reshape2_5.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 121, op original_id: 121, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_5.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_29's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_2.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_29', 'reshape2_5.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 122, op original_id: 122, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_29's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_5.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_30']} = elementwise_mul(inputs={X=['concat_2.tmp_0'], Y=['unsqueeze2_9.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 123, op original_id: 123, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_30's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_31']} = elementwise_add(inputs={X=['tmp_28'], Y=['tmp_30']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 124, op original_id: 124, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_28's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_30's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_31's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_32']} = elementwise_mul(inputs={X=['reshape2_6.tmp_0'], Y=['unsqueeze2_7.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 125, op original_id: 125, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_32's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0_slice_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [0], with_quant_attr = False, dist_attr = {op type: slice, op id: 126, op original_id: 126, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_6.tmp_0_slice_1']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], starts = [64], with_quant_attr = False, dist_attr = {op type: slice, op id: 127, op original_id: 127, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_33']} = scale(inputs={ScaleTensor=[], X=['reshape2_6.tmp_0_slice_1']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = -1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 128, op original_id: 128, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_6.tmp_0_slice_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_33's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['concat_3.tmp_0']} = concat(inputs={AxisTensor=[], X=['tmp_33', 'reshape2_6.tmp_0_slice_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: concat, op id: 129, op original_id: 129, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_33's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_6.tmp_0_slice_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; concat_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_34']} = elementwise_mul(inputs={X=['concat_3.tmp_0'], Y=['unsqueeze2_9.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 130, op original_id: 130, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; concat_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; unsqueeze2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_34's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_35']} = elementwise_add(inputs={X=['tmp_32'], Y=['tmp_34']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 131, op original_id: 131, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_32's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_34's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_35's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['tmp_31']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 132, op original_id: 132, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_31's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_5.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_5.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['tmp_35']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 133, op original_id: 133, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_35's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_6.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_6.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['reshape2_7.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 134, op original_id: 134, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_36']} = scale(inputs={ScaleTensor=[], X=['transpose_5.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False, dist_attr = {op type: scale, op id: 135, op original_id: 135, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_5.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_36's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['transpose_6.tmp_0']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 136, op original_id: 136, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_6.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['tmp_36'], Y=['transpose_8.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 137, op original_id: 137, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_36's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_2.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 138, op original_id: 138, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_37']} = elementwise_add(inputs={X=['matmul_v2_2.tmp_0'], Y=['reshape2_8.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 139, op original_id: 139, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_2.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_8.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_37's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_1.tmp_0']} = cast(inputs={X=['tmp_37']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 140, op original_id: 140, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_37's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['softmax_1.tmp_1']} = softmax(inputs={X=['softmax_1.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: softmax, op id: 141, op original_id: 141, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; softmax_1.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_38']} = cast(inputs={X=['softmax_1.tmp_1']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 142, op original_id: 142, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; softmax_1.tmp_1's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; tmp_38's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['tmp_38'], Y=['transpose_7.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 143, op original_id: 143, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_38's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_7.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; matmul_v2_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['matmul_v2_3.tmp_0']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: transpose2, op id: 144, op original_id: 144, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; matmul_v2_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; transpose_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False, dist_attr = {op type: reshape2, op id: 145, op original_id: 145, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; transpose_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1, -1], partial on dims: []; reshape2_9.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; reshape2_9.tmp_1's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_10.tmp_0']} = matmul_v2(inputs={X=['reshape2_9.tmp_0'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 146, op original_id: 146, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; reshape2_9.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_10.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_10.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_39']} = elementwise_add(inputs={X=['tmp_23'], Y=['linear_10.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 147, op original_id: 147, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_23's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_10.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_39's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_40']} = cast(inputs={X=['tmp_39']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = 4, with_quant_attr = False, dist_attr = {op type: cast, op id: 148, op original_id: 148, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_40's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['pow_3.tmp_0']} = pow(inputs={FactorTensor=[], X=['tmp_40']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: pow, op id: 149, op original_id: 149, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_40's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; pow_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['mean_3.tmp_0']} = reduce_mean(inputs={X=['pow_3.tmp_0']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False, dist_attr = {op type: reduce_mean, op id: 150, op original_id: 150, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; pow_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; mean_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_41']} = scale(inputs={ScaleTensor=[], X=['mean_3.tmp_0']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], scale = 1.0, with_quant_attr = False, dist_attr = {op type: scale, op id: 151, op original_id: 151, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; mean_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_41's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['rsqrt_3.tmp_0']} = rsqrt(inputs={X=['tmp_41']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: rsqrt, op id: 152, op original_id: 152, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_41's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; rsqrt_3.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_42']} = elementwise_mul(inputs={X=['rsqrt_3.tmp_0'], Y=['tmp_39']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 153, op original_id: 153, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; rsqrt_3.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_42's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_43']} = elementwise_mul(inputs={X=['tmp_42'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 154, op original_id: 154, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_42's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; create_parameter_3.w_0's dims_mapping (input, non-annotated, parameter): [-1], partial on dims: []; tmp_43's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_11.tmp_0']} = matmul_v2(inputs={X=['tmp_43'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 155, op original_id: 155, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_43's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_11.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_11.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['silu_1.tmp_0']} = silu(inputs={X=['linear_11.tmp_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: silu, op id: 156, op original_id: 156, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; linear_11.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; silu_1.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_12.tmp_0']} = matmul_v2(inputs={X=['tmp_43'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 157, op original_id: 157, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_43's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_12.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_12.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_44']} = elementwise_mul(inputs={X=['silu_1.tmp_0'], Y=['linear_12.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_mul, op id: 158, op original_id: 158, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; silu_1.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_12.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_44's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['linear_13.tmp_0']} = matmul_v2(inputs={X=['tmp_44'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False, dist_attr = {op type: matmul_v2, op id: 159, op original_id: 159, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_44's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_13.w_0's dims_mapping (input, non-annotated, parameter): [-1, -1], partial on dims: []; linear_13.tmp_0's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    {Out=['tmp_45']} = elementwise_add(inputs={X=['tmp_39'], Y=['linear_13.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 0, op_role_var = [], with_quant_attr = False, dist_attr = {op type: elementwise_add, op id: 160, op original_id: 160, process_mesh (annotated): {shape: [2,1], process_ids: [0,2], dim_names: [dp,mp]}; tmp_39's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; linear_13.tmp_0's dims_mapping (input, non-annotated, non-parameter): [-1, -1, -1], partial on dims: []; tmp_45's dims_mapping (output, non-annotated, non-parameter): [-1, -1, -1], partial on dims: [], dist_impl idx: 0 , dist_impl type: default, chunk_id: 0 })
    send_v2(inputs={X=['tmp_45']}, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 0, op_role_var = [], peer = 1, ring_id = 33, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_24.subprog_2']} = cast(inputs={X=['tmp_23']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_2.tmp_0.subprog_2']} = pow(inputs={FactorTensor=[], X=['tmp_24.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_2.tmp_0.subprog_2']} = reduce_mean(inputs={X=['pow_2.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_25.subprog_2']} = scale(inputs={ScaleTensor=[], X=['mean_2.tmp_0.subprog_2']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_2.tmp_0.subprog_2']} = rsqrt(inputs={X=['tmp_25.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_26.subprog_2']} = elementwise_mul(inputs={X=['rsqrt_2.tmp_0.subprog_2'], Y=['tmp_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_27.subprog_2']} = elementwise_mul(inputs={X=['tmp_26.subprog_2'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_7.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_5.tmp_0.subprog_2'], XShape=['reshape2_5.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_7.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_8.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_6.tmp_0.subprog_2'], XShape=['reshape2_6.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_8.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_9.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_27.subprog_2'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_7.tmp_0.subprog_2'], XShape=['reshape2_7.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_9.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_2.tmp_0.subprog_2'], XShape=['squeeze_2.tmp_1.subprog_2']} = squeeze2(inputs={X=['eager_tmp_4']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_3.tmp_0.subprog_2'], XShape=['squeeze_3.tmp_1.subprog_2']} = squeeze2(inputs={X=['eager_tmp_5']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_6.tmp_0.subprog_2'], XShape=['unsqueeze2_6.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_2.tmp_0.subprog_2']} = gather_nd(inputs={Index=['unsqueeze2_6.tmp_0.subprog_2'], X=['squeeze_2.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_7.tmp_0.subprog_2'], XShape=['unsqueeze2_7.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_2.tmp_0.subprog_2']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_8.tmp_0.subprog_2'], XShape=['unsqueeze2_8.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_3.tmp_0.subprog_2']} = gather_nd(inputs={Index=['unsqueeze2_8.tmp_0.subprog_2'], X=['squeeze_3.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_9.tmp_0.subprog_2'], XShape=['unsqueeze2_9.tmp_1.subprog_2']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_3.tmp_0.subprog_2']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_28.subprog_2']} = elementwise_mul(inputs={X=['reshape2_5.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_0.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_1.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_29.subprog_2']} = scale(inputs={ScaleTensor=[], X=['reshape2_5.tmp_0_slice_1.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_2.tmp_0.subprog_2']} = concat(inputs={AxisTensor=[], X=['tmp_29.subprog_2', 'reshape2_5.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_30.subprog_2']} = elementwise_mul(inputs={X=['concat_2.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_31.subprog_2']} = elementwise_add(inputs={X=['tmp_28.subprog_2'], Y=['tmp_30.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_32.subprog_2']} = elementwise_mul(inputs={X=['reshape2_6.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_0.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_1.subprog_2']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_33.subprog_2']} = scale(inputs={ScaleTensor=[], X=['reshape2_6.tmp_0_slice_1.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_3.tmp_0.subprog_2']} = concat(inputs={AxisTensor=[], X=['tmp_33.subprog_2', 'reshape2_6.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_34.subprog_2']} = elementwise_mul(inputs={X=['concat_3.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_35.subprog_2']} = elementwise_add(inputs={X=['tmp_32.subprog_2'], Y=['tmp_34.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0.subprog_2'], XShape=['transpose_5.tmp_1.subprog_2']} = transpose2(inputs={X=['tmp_31.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_6.tmp_0.subprog_2'], XShape=['transpose_6.tmp_1.subprog_2']} = transpose2(inputs={X=['tmp_35.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_7.tmp_0.subprog_2'], XShape=['transpose_7.tmp_1.subprog_2']} = transpose2(inputs={X=['reshape2_7.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_36.subprog_2']} = scale(inputs={ScaleTensor=[], X=['transpose_5.tmp_0.subprog_2']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_8.tmp_0.subprog_2'], XShape=['transpose_8.tmp_1.subprog_2']} = transpose2(inputs={X=['transpose_6.tmp_0.subprog_2']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_2.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_36.subprog_2'], Y=['transpose_8.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_8.tmp_0.subprog_2'], XShape=['reshape2_8.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_37.subprog_2']} = elementwise_add(inputs={X=['matmul_v2_2.tmp_0.subprog_2'], Y=['reshape2_8.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_1.tmp_0.subprog_2']} = cast(inputs={X=['tmp_37.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_1.tmp_1.subprog_2']} = softmax(inputs={X=['softmax_1.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_38.subprog_2']} = cast(inputs={X=['softmax_1.tmp_1.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_3.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_38.subprog_2'], Y=['transpose_7.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_9.tmp_0.subprog_2'], XShape=['transpose_9.tmp_1.subprog_2']} = transpose2(inputs={X=['matmul_v2_3.tmp_0.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_9.tmp_0.subprog_2'], XShape=['reshape2_9.tmp_1.subprog_2']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_9.tmp_0.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_10.tmp_0.subprog_2']} = matmul_v2(inputs={X=['reshape2_9.tmp_0.subprog_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_39.subprog_2']} = elementwise_add(inputs={X=['tmp_23'], Y=['linear_10.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_40.subprog_2']} = cast(inputs={X=['tmp_39.subprog_2']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_3.tmp_0.subprog_2']} = pow(inputs={FactorTensor=[], X=['tmp_40.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_3.tmp_0.subprog_2']} = reduce_mean(inputs={X=['pow_3.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_41.subprog_2']} = scale(inputs={ScaleTensor=[], X=['mean_3.tmp_0.subprog_2']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_3.tmp_0.subprog_2']} = rsqrt(inputs={X=['tmp_41.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_42.subprog_2']} = elementwise_mul(inputs={X=['rsqrt_3.tmp_0.subprog_2'], Y=['tmp_39.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_43.subprog_2']} = elementwise_mul(inputs={X=['tmp_42.subprog_2'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_11.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_43.subprog_2'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_1.tmp_0.subprog_2']} = silu(inputs={X=['linear_11.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_12.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_43.subprog_2'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_44.subprog_2']} = elementwise_mul(inputs={X=['silu_1.tmp_0.subprog_2'], Y=['linear_12.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_13.tmp_0.subprog_2']} = matmul_v2(inputs={X=['tmp_44.subprog_2'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_45@GRAD@recv_0']} = recv_v2(inputs={}, dtype = 4, dynamic_shape = True, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], out_shape = [1, 2048, 4096], peer = 0, ring_id = 31, use_calc_stream = True, with_quant_attr = False)
    {Out=['tmp_45@GRAD@RESHARD_0']} = assign(inputs={X=['tmp_45@GRAD@recv_0']}, op_device = , op_namescope = /auto_parallel/reshard, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_39@GRAD@RENAME@block0@0'], Y@GRAD=['linear_13.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_45@GRAD@RESHARD_0'], X=['tmp_39.subprog_2'], Y=['linear_13.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_44@GRAD'], Y@GRAD=['linear_13.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_13.tmp_0@GRAD'], X=['tmp_44.subprog_2'], Y=['linear_13.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_13.w_0', 'linear_13.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_13.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_13.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_13.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['silu_1.tmp_0@GRAD'], Y@GRAD=['linear_12.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_44@GRAD'], X=['silu_1.tmp_0.subprog_2'], Y=['linear_12.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_43@GRAD@RENAME@block0@0'], Y@GRAD=['linear_12.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_12.tmp_0@GRAD'], X=['tmp_43.subprog_2'], Y=['linear_12.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_12.w_0', 'linear_12.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_12.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_12.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_12.w_0@GRAD@MERGE'], Y=['linear_12.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_11.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_1.tmp_0.subprog_2'], Out@GRAD=['silu_1.tmp_0@GRAD'], X=['linear_11.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_43@GRAD@RENAME@block0@1'], Y@GRAD=['linear_11.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_11.tmp_0@GRAD'], X=['tmp_43.subprog_2'], Y=['linear_11.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_11.w_0', 'linear_11.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_11.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_11.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_11.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['tmp_43@GRAD']} = sum(inputs={X=['tmp_43@GRAD@RENAME@block0@0', 'tmp_43@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_42@GRAD'], Y@GRAD=['create_parameter_3.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_43@GRAD'], X=['tmp_42.subprog_2'], Y=['create_parameter_3.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['create_parameter_3.w_0', 'create_parameter_3.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_3.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_3.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_3.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['rsqrt_3.tmp_0@GRAD'], Y@GRAD=['tmp_39@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_42@GRAD'], X=['rsqrt_3.tmp_0.subprog_2'], Y=['tmp_39.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_41@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_3.tmp_0.subprog_2'], Out@GRAD=['rsqrt_3.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_3.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_41@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_3.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_3.tmp_0@GRAD'], X=['pow_3.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_40@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_3.tmp_0@GRAD'], X=['tmp_40.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_39@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_40@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_39@GRAD']} = sum(inputs={X=['tmp_39@GRAD@RENAME@block0@0', 'tmp_39@GRAD@RENAME@block0@1', 'tmp_39@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_23@GRAD@RENAME@block0@0'], Y@GRAD=['linear_10.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['tmp_23'], Y=['linear_10.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD'], Y@GRAD=['linear_10.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_10.tmp_0@GRAD'], X=['reshape2_9.tmp_0.subprog_2'], Y=['linear_10.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_10.w_0', 'linear_10.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_10.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_10.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_10.w_0@GRAD@MERGE'], Y=['linear_10.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['transpose_9.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [2, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_38@GRAD'], Y@GRAD=['transpose_7.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['tmp_38.subprog_2'], Y=['transpose_7.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_1.tmp_1@GRAD']} = cast(inputs={X=['tmp_38@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_1.subprog_2'], Out@GRAD=['softmax_1.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_37@GRAD']} = cast(inputs={X=['softmax_1.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['matmul_v2_2.tmp_0.subprog_2'], Y=['reshape2_8.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_36@GRAD'], Y@GRAD=['transpose_8.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['tmp_36.subprog_2'], Y=['transpose_8.tmp_0.subprog_2']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1.subprog_2']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_36@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_7.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_35@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_31@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1.subprog_2']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_32@GRAD'], Y@GRAD=['tmp_34@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_35@GRAD'], X=['tmp_32.subprog_2'], Y=['tmp_34.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_34@GRAD'], X=['concat_3.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_33@GRAD', 'reshape2_6.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_3.tmp_0@GRAD'], X=['tmp_33.subprog_2', 'reshape2_6.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_6.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_33@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], Out@GRAD=['reshape2_6.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_6.tmp_0.subprog_2'], Out@GRAD=['reshape2_6.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_6.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['reshape2_6.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_28@GRAD'], Y@GRAD=['tmp_30@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_31@GRAD'], X=['tmp_28.subprog_2'], Y=['tmp_30.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_2.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['concat_2.tmp_0.subprog_2'], Y=['unsqueeze2_9.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_29@GRAD', 'reshape2_5.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_2.tmp_0@GRAD'], X=['tmp_29.subprog_2', 'reshape2_5.tmp_0_slice_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_29@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], Out@GRAD=['reshape2_5.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_5.tmp_0.subprog_2'], Out@GRAD=['reshape2_5.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['reshape2_5.tmp_0.subprog_2'], Y=['unsqueeze2_7.tmp_0.subprog_2']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_9.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@0'], Y@GRAD=['linear_9.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_9.tmp_0@GRAD'], X=['tmp_27.subprog_2'], Y=['linear_9.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_9.w_0', 'linear_9.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_9.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_9.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_9.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['reshape2_6.tmp_0@GRAD']} = sum(inputs={X=['reshape2_6.tmp_0@GRAD@RENAME@block0@0', 'reshape2_6.tmp_0@GRAD@RENAME@block0@1', 'reshape2_6.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_8.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@1'], Y@GRAD=['linear_8.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_8.tmp_0@GRAD'], X=['tmp_27.subprog_2'], Y=['linear_8.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_8.w_0', 'linear_8.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_8.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_8.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_8.w_0@GRAD@MERGE'], Y=['linear_8.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_5.tmp_0@GRAD']} = sum(inputs={X=['reshape2_5.tmp_0@GRAD@RENAME@block0@0', 'reshape2_5.tmp_0@GRAD@RENAME@block0@1', 'reshape2_5.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1.subprog_2']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_27@GRAD@RENAME@block0@2'], Y@GRAD=['linear_7.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_7.tmp_0@GRAD'], X=['tmp_27.subprog_2'], Y=['linear_7.w_0']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['linear_7.w_0', 'linear_7.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_7.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_7.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_7.w_0@GRAD@MERGE'], Y=['linear_7.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_27@GRAD']} = sum(inputs={X=['tmp_27@GRAD@RENAME@block0@0', 'tmp_27@GRAD@RENAME@block0@1', 'tmp_27@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_26@GRAD'], Y@GRAD=['create_parameter_2.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['tmp_26.subprog_2'], Y=['create_parameter_2.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = ['create_parameter_2.w_0', 'create_parameter_2.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_2.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_2.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['create_parameter_2.w_0@GRAD@MERGE'], Y=['create_parameter_2.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['rsqrt_2.tmp_0@GRAD'], Y@GRAD=['tmp_23@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_26@GRAD'], X=['rsqrt_2.tmp_0.subprog_2'], Y=['tmp_23']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_25@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_2.tmp_0.subprog_2'], Out@GRAD=['rsqrt_2.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_2.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_25@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_2.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_2.tmp_0@GRAD'], X=['pow_2.tmp_0.subprog_2']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_24@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_2.tmp_0@GRAD'], X=['tmp_24.subprog_2']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_1/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_23@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_24@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_2.subprog_3']} = cast(inputs={X=['embedding_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_0.tmp_0.subprog_3']} = pow(inputs={FactorTensor=[], X=['tmp_2.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0.subprog_3']} = reduce_mean(inputs={X=['pow_0.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_3.subprog_3']} = scale(inputs={ScaleTensor=[], X=['mean_0.tmp_0.subprog_3']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_0.tmp_0.subprog_3']} = rsqrt(inputs={X=['tmp_3.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_4.subprog_3']} = elementwise_mul(inputs={X=['rsqrt_0.tmp_0.subprog_3'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_5.subprog_3']} = elementwise_mul(inputs={X=['tmp_4.subprog_3'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_0.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_0.tmp_0.subprog_3'], XShape=['reshape2_0.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_0.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_1.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_1.tmp_0.subprog_3'], XShape=['reshape2_1.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_1.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_2.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_5.subprog_3'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_2.tmp_0.subprog_3'], XShape=['reshape2_2.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['linear_2.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {Out=['squeeze_0.tmp_0.subprog_3'], XShape=['squeeze_0.tmp_1.subprog_3']} = squeeze2(inputs={X=['eager_tmp_1']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['squeeze_1.tmp_0.subprog_3'], XShape=['squeeze_1.tmp_1.subprog_3']} = squeeze2(inputs={X=['eager_tmp_2']}, axes = [0, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_2.tmp_0.subprog_3'], XShape=['unsqueeze2_2.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_0.tmp_0.subprog_3']} = gather_nd(inputs={Index=['unsqueeze2_2.tmp_0.subprog_3'], X=['squeeze_0.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_3.tmp_0.subprog_3'], XShape=['unsqueeze2_3.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_0.tmp_0.subprog_3']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_4.tmp_0.subprog_3'], XShape=['unsqueeze2_4.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['split@RESHARD.tmp_1']}, axes = [-1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gather_nd_1.tmp_0.subprog_3']} = gather_nd(inputs={Index=['unsqueeze2_4.tmp_0.subprog_3'], X=['squeeze_1.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['unsqueeze2_5.tmp_0.subprog_3'], XShape=['unsqueeze2_5.tmp_1.subprog_3']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['gather_nd_1.tmp_0.subprog_3']}, axes = [2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_6.subprog_3']} = elementwise_mul(inputs={X=['reshape2_0.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_0.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_1.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_7.subprog_3']} = scale(inputs={ScaleTensor=[], X=['reshape2_0.tmp_0_slice_1.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_0.tmp_0.subprog_3']} = concat(inputs={AxisTensor=[], X=['tmp_7.subprog_3', 'reshape2_0.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_8.subprog_3']} = elementwise_mul(inputs={X=['concat_0.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_9.subprog_3']} = elementwise_add(inputs={X=['tmp_6.subprog_3'], Y=['tmp_8.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_10.subprog_3']} = elementwise_mul(inputs={X=['reshape2_1.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_0.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_1.subprog_3']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Out=['tmp_11.subprog_3']} = scale(inputs={ScaleTensor=[], X=['reshape2_1.tmp_0_slice_1.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Out=['concat_1.tmp_0.subprog_3']} = concat(inputs={AxisTensor=[], X=['tmp_11.subprog_3', 'reshape2_1.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_12.subprog_3']} = elementwise_mul(inputs={X=['concat_1.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_13.subprog_3']} = elementwise_add(inputs={X=['tmp_10.subprog_3'], Y=['tmp_12.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0.subprog_3'], XShape=['transpose_0.tmp_1.subprog_3']} = transpose2(inputs={X=['tmp_9.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_1.tmp_0.subprog_3'], XShape=['transpose_1.tmp_1.subprog_3']} = transpose2(inputs={X=['tmp_13.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_2.tmp_0.subprog_3'], XShape=['transpose_2.tmp_1.subprog_3']} = transpose2(inputs={X=['reshape2_2.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_14.subprog_3']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0.subprog_3']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {Out=['transpose_3.tmp_0.subprog_3'], XShape=['transpose_3.tmp_1.subprog_3']} = transpose2(inputs={X=['transpose_1.tmp_0.subprog_3']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['matmul_v2_0.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_14.subprog_3'], Y=['transpose_3.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['reshape2_3.tmp_0.subprog_3'], XShape=['reshape2_3.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['split@RESHARD.tmp_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [1, 1, 2048, 2048], use_quantizer = False, with_quant_attr = False)
    {Out=['tmp_15.subprog_3']} = elementwise_add(inputs={X=['matmul_v2_0.tmp_0.subprog_3'], Y=['reshape2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['softmax_0.tmp_0.subprog_3']} = cast(inputs={X=['tmp_15.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['softmax_0.tmp_1.subprog_3']} = softmax(inputs={X=['softmax_0.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_16.subprog_3']} = cast(inputs={X=['softmax_0.tmp_1.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['matmul_v2_1.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_16.subprog_3'], Y=['transpose_2.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['transpose_4.tmp_0.subprog_3'], XShape=['transpose_4.tmp_1.subprog_3']} = transpose2(inputs={X=['matmul_v2_1.tmp_0.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_4.tmp_0.subprog_3'], XShape=['reshape2_4.tmp_1.subprog_3']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_4.tmp_0.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [1, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {Out=['linear_3.tmp_0.subprog_3']} = matmul_v2(inputs={X=['reshape2_4.tmp_0.subprog_3'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_17.subprog_3']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_18.subprog_3']} = cast(inputs={X=['tmp_17.subprog_3']}, in_dtype = 4, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['pow_1.tmp_0.subprog_3']} = pow(inputs={FactorTensor=[], X=['tmp_18.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_1.tmp_0.subprog_3']} = reduce_mean(inputs={X=['pow_1.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['tmp_19.subprog_3']} = scale(inputs={ScaleTensor=[], X=['mean_1.tmp_0.subprog_3']}, bias = 9.999999974752427e-07, bias_after_scale = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {Out=['rsqrt_1.tmp_0.subprog_3']} = rsqrt(inputs={X=['tmp_19.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_20.subprog_3']} = elementwise_mul(inputs={X=['rsqrt_1.tmp_0.subprog_3'], Y=['tmp_17.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21.subprog_3']} = elementwise_mul(inputs={X=['tmp_20.subprog_3'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_4.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_21.subprog_3'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['silu_0.tmp_0.subprog_3']} = silu(inputs={X=['linear_4.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_5.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_21.subprog_3'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_22.subprog_3']} = elementwise_mul(inputs={X=['silu_0.tmp_0.subprog_3'], Y=['linear_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_6.tmp_0.subprog_3']} = matmul_v2(inputs={X=['tmp_22.subprog_3'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['tmp_23@GRAD']} = sum(inputs={X=['tmp_23@GRAD@RENAME@block0@0', 'tmp_23@GRAD@RENAME@block0@1', 'tmp_23@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_17@GRAD@RENAME@block0@0'], Y@GRAD=['linear_6.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['tmp_17.subprog_3'], Y=['linear_6.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_22@GRAD'], Y@GRAD=['linear_6.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_6.tmp_0@GRAD'], X=['tmp_22.subprog_3'], Y=['linear_6.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_6.w_0', 'linear_6.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_6.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_6.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_6.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {X@GRAD=['silu_0.tmp_0@GRAD'], Y@GRAD=['linear_5.tmp_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_22@GRAD'], X=['silu_0.tmp_0.subprog_3'], Y=['linear_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_21@GRAD@RENAME@block0@0'], Y@GRAD=['linear_5.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_5.tmp_0@GRAD'], X=['tmp_21.subprog_3'], Y=['linear_5.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_5.w_0', 'linear_5.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_5.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_5.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_5.w_0@GRAD@MERGE'], Y=['linear_5.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_4.tmp_0@GRAD']} = silu_grad(inputs={Out=['silu_0.tmp_0.subprog_3'], Out@GRAD=['silu_0.tmp_0@GRAD'], X=['linear_4.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_21@GRAD@RENAME@block0@1'], Y@GRAD=['linear_4.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_4.tmp_0@GRAD'], X=['tmp_21.subprog_3'], Y=['linear_4.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_4.w_0', 'linear_4.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_4.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_4.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_4.w_0@GRAD@MERGE'], Y=['linear_4.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_21@GRAD']} = sum(inputs={X=['tmp_21@GRAD@RENAME@block0@0', 'tmp_21@GRAD@RENAME@block0@1']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_20@GRAD'], Y@GRAD=['create_parameter_1.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['tmp_20.subprog_3'], Y=['create_parameter_1.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['create_parameter_1.w_0', 'create_parameter_1.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_1.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_1.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['create_parameter_1.w_0@GRAD@MERGE'], Y=['create_parameter_1.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['rsqrt_1.tmp_0@GRAD'], Y@GRAD=['tmp_17@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['rsqrt_1.tmp_0.subprog_3'], Y=['tmp_17.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_19@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_1.tmp_0.subprog_3'], Out@GRAD=['rsqrt_1.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_1.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_19@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_1.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_1.tmp_0@GRAD'], X=['pow_1.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_18@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_1.tmp_0@GRAD'], X=['tmp_18.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_17@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_18@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['tmp_17@GRAD']} = sum(inputs={X=['tmp_17@GRAD@RENAME@block0@0', 'tmp_17@GRAD@RENAME@block0@1', 'tmp_17@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['embedding_0.tmp_0@GRAD@RENAME@block0@0'], Y@GRAD=['linear_3.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_17@GRAD'], X=['embedding_0.tmp_0'], Y=['linear_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD'], Y@GRAD=['linear_3.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_3.tmp_0@GRAD'], X=['reshape2_4.tmp_0.subprog_3'], Y=['linear_3.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_3.w_0', 'linear_3.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_3.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_3.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_3.w_0@GRAD@MERGE'], Y=['linear_3.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['transpose_4.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [2, 2048, 4096], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_16@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['tmp_16.subprog_3'], Y=['transpose_2.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['softmax_0.tmp_1@GRAD']} = cast(inputs={X=['tmp_16@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_1.subprog_3'], Out@GRAD=['softmax_0.tmp_1@GRAD']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_15@GRAD']} = cast(inputs={X=['softmax_0.tmp_0@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {X@GRAD=['matmul_v2_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_15@GRAD'], X=['matmul_v2_0.tmp_0.subprog_3'], Y=['reshape2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_14@GRAD'], Y@GRAD=['transpose_3.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_0.tmp_0@GRAD'], X=['tmp_14.subprog_3'], Y=['transpose_3.tmp_0.subprog_3']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, with_quant_attr = False)
    {X@GRAD=['transpose_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1.subprog_3']}, axis = [0, 1, 3, 2], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['transpose_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_14@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 0.0883883461356163, with_quant_attr = False)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_13@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_9@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1.subprog_3']}, axis = [0, 2, 1, 3], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_10@GRAD'], Y@GRAD=['tmp_12@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['tmp_10.subprog_3'], Y=['tmp_12.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['concat_1.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_11@GRAD', 'reshape2_1.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_1.tmp_0@GRAD'], X=['tmp_11.subprog_3', 'reshape2_1.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_11@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], Out@GRAD=['reshape2_1.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_1.tmp_0.subprog_3'], Out@GRAD=['reshape2_1.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['reshape2_1.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_6@GRAD'], Y@GRAD=['tmp_8@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['tmp_6.subprog_3'], Y=['tmp_8.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['concat_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['concat_0.tmp_0.subprog_3'], Y=['unsqueeze2_5.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_7@GRAD', 'reshape2_0.tmp_0_slice_0@GRAD']} = concat_grad(inputs={AxisTensor=[], Out@GRAD=['concat_0.tmp_0@GRAD'], X=['tmp_7.subprog_3', 'reshape2_0.tmp_0_slice_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0_slice_1@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_7@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = -1.0, with_quant_attr = False)
    {Input@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@0']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], Out@GRAD=['reshape2_0.tmp_0_slice_1@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [2147483647], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [64], with_quant_attr = False)
    {Input@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@1']} = slice_grad(inputs={EndsTensor=[], EndsTensorList=[], Input=['reshape2_0.tmp_0.subprog_3'], Out@GRAD=['reshape2_0.tmp_0_slice_0@GRAD'], StartsTensor=[], StartsTensorList=[]}, axes = [3], decrease_axis = [], ends = [64], infer_flags = [1], op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], starts = [0], with_quant_attr = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD@RENAME@block0@2'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['reshape2_0.tmp_0.subprog_3'], Y=['unsqueeze2_3.tmp_0.subprog_3']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_2.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@0'], Y@GRAD=['linear_2.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_2.tmp_0@GRAD'], X=['tmp_5.subprog_3'], Y=['linear_2.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_2.w_0', 'linear_2.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_2.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_2.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_2.w_0@GRAD@MERGE'], Y=['linear_2.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_1.tmp_0@GRAD']} = sum(inputs={X=['reshape2_1.tmp_0@GRAD@RENAME@block0@0', 'reshape2_1.tmp_0@GRAD@RENAME@block0@1', 'reshape2_1.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_1.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@1'], Y@GRAD=['linear_1.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_1.tmp_0@GRAD'], X=['tmp_5.subprog_3'], Y=['linear_1.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_1.w_0', 'linear_1.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_1.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_1.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_1.w_0@GRAD@MERGE'], Y=['linear_1.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['reshape2_0.tmp_0@GRAD']} = sum(inputs={X=['reshape2_0.tmp_0@GRAD@RENAME@block0@0', 'reshape2_0.tmp_0@GRAD@RENAME@block0@1', 'reshape2_0.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['linear_0.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1.subprog_3']}, mkldnn_data_type = float32, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], shape = [0, 0, 32, 128], use_quantizer = False, with_quant_attr = False)
    {X@GRAD=['tmp_5@GRAD@RENAME@block0@2'], Y@GRAD=['linear_0.w_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['linear_0.tmp_0@GRAD'], X=['tmp_5.subprog_3'], Y=['linear_0.w_0']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['linear_0.w_0', 'linear_0.w_0@GRAD'], trans_x = False, trans_y = False, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD']} = c_reduce_sum(inputs={X=['linear_0.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['linear_0.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['linear_0.w_0@GRAD@MERGE'], Y=['linear_0.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['tmp_5@GRAD']} = sum(inputs={X=['tmp_5@GRAD@RENAME@block0@0', 'tmp_5@GRAD@RENAME@block0@1', 'tmp_5@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_4@GRAD'], Y@GRAD=['create_parameter_0.w_0@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['tmp_4.subprog_3'], Y=['create_parameter_0.w_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = ['create_parameter_0.w_0', 'create_parameter_0.w_0@GRAD'], with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD']} = c_reduce_sum(inputs={X=['create_parameter_0.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['create_parameter_0.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD@MERGE']} = elementwise_add(inputs={X=['create_parameter_0.w_0@GRAD@MERGE'], Y=['create_parameter_0.w_0@GRAD']}, axis = -1, op_device = , op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['rsqrt_0.tmp_0@GRAD'], Y@GRAD=['embedding_0.tmp_0@GRAD@RENAME@block0@1']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['rsqrt_0.tmp_0.subprog_3'], Y=['embedding_0.tmp_0']}, axis = -1, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {X@GRAD=['tmp_3@GRAD']} = rsqrt_grad(inputs={Out=['rsqrt_0.tmp_0.subprog_3'], Out@GRAD=['rsqrt_0.tmp_0@GRAD']}, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_3@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = , op_role = 1, op_role_var = [], scale = 1.0, with_quant_attr = False)
    {X@GRAD=['pow_0.tmp_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['pow_0.tmp_0.subprog_3']}, dim = [-1], in_dtype = -1, keep_dim = True, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {X@GRAD=['tmp_2@GRAD']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_0.tmp_0@GRAD'], X=['tmp_2.subprog_3']}, factor = 2.0, op_device = , op_namescope = //auto_parallel/rc_0/, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD@RENAME@block0@2']} = cast(inputs={X=['tmp_2@GRAD']}, in_dtype = 4, op_device = , op_namescope = , op_role = 1, op_role_var = [], out_dtype = 4, with_quant_attr = False)
    {Out=['embedding_0.tmp_0@GRAD']} = sum(inputs={X=['embedding_0.tmp_0@GRAD@RENAME@block0@0', 'embedding_0.tmp_0@GRAD@RENAME@block0@1', 'embedding_0.tmp_0@GRAD@RENAME@block0@2']}, op_device = , op_namescope = , op_role = 1, op_role_var = [], with_quant_attr = False)
    {W@GRAD=['embedding_0.w_0@GRAD']} = lookup_table_v2_grad(inputs={Ids=['input_ids'], Out@GRAD=['embedding_0.tmp_0@GRAD'], W=['embedding_0.w_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = ['embedding_0.w_0', 'embedding_0.w_0@GRAD'], padding_idx = -1, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD']} = c_reduce_sum(inputs={X=['embedding_0.w_0@GRAD']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], ring_id = 27, root_id = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['embedding_0.w_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['embedding_0.w_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 1, op_role_var = [], scale = 0.5, with_quant_attr = False)
    {Out=['gradient_merge_step']} = increment(inputs={X=['gradient_merge_step']}, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], step = 1.0, with_quant_attr = False)
    {Out=['gradient_merge_step']} = elementwise_mod(inputs={X=['gradient_merge_step'], Y=['gradient_merge_k']}, axis = -1, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['gradient_merge_cond']} = equal(inputs={X=['gradient_merge_step'], Y=['gradient_merge_zero']}, axis = -1, force_cpu = False, op_device = cpu, op_namescope = /, op_role = 1, op_role_var = [], with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE', 'opt_tmp_14', 'create_parameter_1.w_0_fp32_master_0_moment1_0', 'opt_tmp_18', 'linear_5.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_opt_squared_l2_norm_3.tmp_0', 'linear_2.w_0@GRAD@MERGE', 'linear_7.w_0_fp32_master_0_moment2_0', 'create_parameter_0.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_1.w_0_fp32_master_0_moment2_0', 'linear_12.w_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_10.w_0_fp32_master_0_beta1_pow_acc_0', 'find_infinite_scale.@fp16_0@cast_int32', 'opt_tmp_2', 'linear_8.w_0', 'create_parameter_2.w_0_fp32_master_0_moment1_0', 'linear_3.w_0', 'linear_12.w_0_fp32_master_0_moment2_0', 'opt_tmp_12', 'create_parameter_1.w_0', 'linear_7.w_0_fp32_master_0', 'linear_12.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_10.w_0_fp32_master_0', 'create_parameter_0.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_1.w_0', 'linear_4.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_2.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_1.w_0_fp32_master_0', 'linear_5.w_0_fp32_master_0_moment2_0', 'linear_10.w_0@GRAD@MERGE', 'linear_8.w_0_fp32_master_0_beta2_pow_acc_0', 'memcopy__0', 'linear_3.w_0_fp32_master_0_moment2_0', 'find_infinite_scale.tmp_0', 'linear_3.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_elementwise_div_0', 'linear_8.w_0@GRAD@MERGE', 'linear_7.w_0', 'linear_5.w_0_fp32_master_0_moment1_0', 'linear_7.w_0@GRAD@MERGE', 'linear_12.w_0_fp32_master_0_beta2_pow_acc_0', 'opt_tmp_7', 'linear_6.w_0', 'linear_5.w_0_fp32_master_0', 'create_parameter_2.w_0@GRAD@MERGE', 'opt_opt_squared_l2_norm_2.tmp_0', 'opt_tmp_16', 'linear_8.w_0_fp32_master_0_moment2_0', 'linear_10.w_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0_moment1_0', 'opt_tmp_10', 'linear_4.w_0_fp32_master_0_moment2_0', 'linear_1.w_0_fp32_master_0', 'linear_7.w_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0', 'linear_8.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_stack_0.tmp_0', 'create_parameter_0.w_0_fp32_master_0_moment1_0', 'linear_5.w_0', 'linear_0.w_0_fp32_master_0', 'opt_opt_sum_0.tmp_0', 'create_parameter_1.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_1.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.w_0_fp32_master_0_moment1_0', 'linear_10.w_0', 'create_parameter_2.w_0_fp32_master_0', 'linear_0.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_tmp_6', 'opt_opt_squared_l2_norm_10.tmp_0', 'linear_2.w_0', 'opt_opt_squared_l2_norm_1.tmp_0', 'create_parameter_3.w_0', 'opt_opt_squared_l2_norm_4.tmp_0', 'linear_2.w_0_fp32_master_0_moment2_0', 'create_parameter_0.w_0', 'opt_opt_squared_l2_norm_8.tmp_0', 'linear_3.w_0_fp32_master_0', 'linear_4.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_1.w_0_fp32_master_0_moment2_0', 'linear_8.w_0_fp32_master_0_moment1_0', 'concat.tmp_0', 'opt_opt_squared_l2_norm_5.tmp_0', 'opt_tmp_0', 'create_parameter_0.w_0_fp32_master_0', 'embedding_0.w_0', 'create_parameter_1.w_0@GRAD@MERGE', 'linear_3.w_0_fp32_master_0_moment1_0', 'linear_4.w_0_fp32_master_0', 'opt_tmp_9', 'linear_3.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD@MERGE', 'linear_12.w_0', 'create_parameter_2.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.w_0_fp32_master_0', 'create_parameter_0.w_0_fp32_master_0_moment2_0', 'opt_tmp_5', 'linear_7.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_4.w_0@GRAD@MERGE', 'linear_9.w_0', 'linear_4.w_0', 'opt_elementwise_max_0', 'linear_0.w_0_fp32_master_0_moment1_0', 'linear_0.w_0@GRAD@MERGE', 'opt_opt_sqrt_0.tmp_0', 'opt_opt_squared_l2_norm_13.tmp_0', 'linear_10.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_7.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_5.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_13.w_0', 'linear_10.w_0_fp32_master_0_moment2_0', 'num_bad_steps_0', 'linear_3.w_0_fp32_master_0_beta2_pow_acc_0', 'create_parameter_2.w_0_fp32_master_0_moment2_0', 'opt_opt_squared_l2_norm_15.tmp_0', 'loss_scaling_0', 'create_parameter_2.w_0', 'opt_opt_squared_l2_norm_17.tmp_0', 'linear_2.w_0_fp32_master_0', 'linear_1.w_0_fp32_master_0_beta2_pow_acc_0', 'find_infinite_scale.@fp16_0', 'linear_0.w_0_fp32_master_0_moment2_0', 'linear_11.w_0', 'opt_tmp_11', 'linear_2.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0', 'opt_tmp_3', 'linear_1.w_0_fp32_master_0_moment1_0', 'linear_1.w_0_fp32_master_0_beta1_pow_acc_0', 'num_good_steps_0', 'opt_tmp_4', 'opt_opt_fill_constant_1.tmp_0', 'linear_2.w_0_fp32_master_0_moment1_0', 'create_parameter_2.w_0_fp32_master_0_beta1_pow_acc_0', 'opt_opt_squared_l2_norm_9.tmp_0', 'opt_opt_squared_l2_norm_11.tmp_0', 'opt_opt_squared_l2_norm_6.tmp_0'], Scope=['_generated_var_0']} = conditional_block(inputs={Cond=['gradient_merge_cond'], Input=['linear_5.w_0@GRAD@MERGE', 'create_parameter_1.w_0_fp32_master_0_moment1_0', 'linear_0.w_0_fp32_master_0_beta1_pow_acc_0', 'learning_rate_0', 'linear_5.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_2.w_0@GRAD@MERGE', 'linear_2.w_0', 'linear_7.w_0_fp32_master_0_moment2_0', 'create_parameter_3.w_0', 'create_parameter_0.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_1.w_0_fp32_master_0_moment2_0', 'linear_12.w_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_10.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_8.w_0', 'create_parameter_2.w_0_fp32_master_0_moment1_0', 'linear_3.w_0', 'linear_12.w_0_fp32_master_0_moment2_0', 'linear_2.w_0_fp32_master_0_moment2_0', 'create_parameter_0.w_0', 'linear_3.w_0_fp32_master_0', 'linear_4.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_1.w_0_fp32_master_0_moment2_0', 'linear_8.w_0_fp32_master_0_moment1_0', 'create_parameter_1.w_0', 'linear_7.w_0_fp32_master_0', 'linear_12.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_10.w_0_fp32_master_0', 'create_parameter_0.w_0_fp32_master_0', 'create_parameter_0.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0_fp32_master_0_beta2_pow_acc_0', 'embedding_0.w_0', 'create_parameter_1.w_0@GRAD@MERGE', 'linear_3.w_0_fp32_master_0_moment1_0', 'linear_4.w_0_fp32_master_0', 'linear_3.w_0@GRAD@MERGE', 'linear_1.w_0', 'linear_4.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_2.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_0.w_0@GRAD@MERGE', 'create_parameter_1.w_0_fp32_master_0', 'linear_5.w_0_fp32_master_0_moment2_0', 'linear_10.w_0@GRAD@MERGE', 'linear_8.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_12.w_0', 'create_parameter_2.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_3.w_0_fp32_master_0_moment2_0', 'linear_8.w_0_fp32_master_0', 'create_parameter_0.w_0_fp32_master_0_moment2_0', 'linear_7.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_3.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_9.w_0', 'linear_4.w_0', 'linear_0.w_0_fp32_master_0_moment1_0', 'linear_0.w_0@GRAD@MERGE', 'linear_10.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_8.w_0@GRAD@MERGE', 'linear_7.w_0', 'linear_7.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_5.w_0_fp32_master_0_moment1_0', 'linear_5.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_7.w_0@GRAD@MERGE', 'linear_13.w_0', 'linear_10.w_0_fp32_master_0_moment2_0', 'linear_12.w_0_fp32_master_0_beta2_pow_acc_0', 'num_bad_steps_0', 'linear_6.w_0', 'linear_3.w_0_fp32_master_0_beta2_pow_acc_0', 'create_parameter_2.w_0_fp32_master_0_moment2_0', 'linear_5.w_0_fp32_master_0', 'loss_scaling_0', 'create_parameter_2.w_0@GRAD@MERGE', 'linear_8.w_0_fp32_master_0_moment2_0', 'linear_10.w_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0_moment1_0', 'create_parameter_2.w_0', 'linear_2.w_0_fp32_master_0', 'linear_1.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.w_0_fp32_master_0_moment2_0', 'linear_1.w_0_fp32_master_0', 'linear_7.w_0_fp32_master_0_moment1_0', 'linear_12.w_0_fp32_master_0', 'linear_8.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_0.w_0_fp32_master_0_moment2_0', 'linear_11.w_0', 'create_parameter_0.w_0_fp32_master_0_moment1_0', 'linear_5.w_0', 'linear_2.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_0.w_0_fp32_master_0', 'linear_0.w_0', 'linear_1.w_0_fp32_master_0_moment1_0', 'create_parameter_1.w_0_fp32_master_0_beta1_pow_acc_0', 'create_parameter_1.w_0_fp32_master_0_beta2_pow_acc_0', 'linear_4.w_0_fp32_master_0_moment1_0', 'linear_10.w_0', 'linear_1.w_0_fp32_master_0_beta1_pow_acc_0', 'num_good_steps_0', 'linear_2.w_0_fp32_master_0_moment1_0', 'create_parameter_2.w_0_fp32_master_0', 'create_parameter_2.w_0_fp32_master_0_beta1_pow_acc_0', 'linear_4.w_0@GRAD@MERGE']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 2, op_role_var = [], sub_block = block[1], with_quant_attr = False)
}
{ // block_idx:1  parent_idx:0  forward_idx:-1  backward_idx:-1

    {Out=['create_parameter_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['create_parameter_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_0.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_3.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['create_parameter_1.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_4.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_5.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['create_parameter_2.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_7.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_8.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_10.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = scale(inputs={ScaleTensor=[], X=['linear_12.w_0@GRAD@MERGE']}, bias = 0.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 2, op_role_var = [], scale = 0.0625, with_quant_attr = False)
    {FoundInfinite=['find_infinite_scale.@fp16_0'], Out=['linear_0.w_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD@MERGE', 'create_parameter_1.w_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'create_parameter_2.w_0@GRAD@MERGE']} = check_finite_and_unscale(inputs={Scale=['loss_scaling_0'], X=['linear_0.w_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD@MERGE', 'create_parameter_1.w_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'create_parameter_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.@fp16_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, out_dtype = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, op_device = , op_namescope = /auto_parallel/amp_flag_synchronization, op_role = 2, ring_id = 29, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['find_infinite_scale.@fp16_0']} = cast(inputs={X=['find_infinite_scale.@fp16_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, out_dtype = 0, with_quant_attr = False)
    {Out=['concat.tmp_0']} = concat(inputs={AxisTensor=[], X=['find_infinite_scale.@fp16_0']}, axis = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {Out=['find_infinite_scale.tmp_0']} = reduce_any(inputs={AxisTensor=[], AxisTensorList=[], X=['concat.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 2, out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['memcopy__0']} = memcpy_d2h(inputs={X=['find_infinite_scale.tmp_0']}, dst_place_type = 0, op_device = , op_namescope = /, op_role = 2, with_quant_attr = False)
    {LossScaling=['loss_scaling_0'], Out=['linear_0.w_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD@MERGE', 'create_parameter_1.w_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'create_parameter_2.w_0@GRAD@MERGE'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], StopUpdate=[], X=['linear_0.w_0@GRAD@MERGE', 'linear_1.w_0@GRAD@MERGE', 'linear_2.w_0@GRAD@MERGE', 'linear_3.w_0@GRAD@MERGE', 'linear_4.w_0@GRAD@MERGE', 'linear_5.w_0@GRAD@MERGE', 'create_parameter_0.w_0@GRAD@MERGE', 'create_parameter_1.w_0@GRAD@MERGE', 'linear_7.w_0@GRAD@MERGE', 'linear_8.w_0@GRAD@MERGE', 'linear_10.w_0@GRAD@MERGE', 'linear_12.w_0@GRAD@MERGE', 'create_parameter_2.w_0@GRAD@MERGE']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.800000011920929, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, stop_update = False, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_1.tmp_0']} = squared_l2_norm(inputs={X=['linear_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_2.tmp_0']} = squared_l2_norm(inputs={X=['linear_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_3.tmp_0']} = squared_l2_norm(inputs={X=['linear_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_4.tmp_0']} = squared_l2_norm(inputs={X=['linear_3.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_5.tmp_0']} = squared_l2_norm(inputs={X=['linear_4.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_6.tmp_0']} = squared_l2_norm(inputs={X=['linear_5.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_8.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_0.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_9.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_1.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_10.tmp_0']} = squared_l2_norm(inputs={X=['linear_7.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_11.tmp_0']} = squared_l2_norm(inputs={X=['linear_8.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_13.tmp_0']} = squared_l2_norm(inputs={X=['linear_10.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_15.tmp_0']} = squared_l2_norm(inputs={X=['linear_12.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_squared_l2_norm_17.tmp_0']} = squared_l2_norm(inputs={X=['create_parameter_2.w_0@GRAD@MERGE']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Y=['opt_opt_stack_0.tmp_0']} = stack(inputs={X=['opt_opt_squared_l2_norm_1.tmp_0', 'opt_opt_squared_l2_norm_2.tmp_0', 'opt_opt_squared_l2_norm_3.tmp_0', 'opt_opt_squared_l2_norm_4.tmp_0', 'opt_opt_squared_l2_norm_5.tmp_0', 'opt_opt_squared_l2_norm_6.tmp_0', 'opt_opt_squared_l2_norm_8.tmp_0', 'opt_opt_squared_l2_norm_9.tmp_0', 'opt_opt_squared_l2_norm_10.tmp_0', 'opt_opt_squared_l2_norm_11.tmp_0', 'opt_opt_squared_l2_norm_13.tmp_0', 'opt_opt_squared_l2_norm_15.tmp_0', 'opt_opt_squared_l2_norm_17.tmp_0']}, axis = 0, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_sum_0.tmp_0']} = reduce_sum(inputs={X=['opt_opt_stack_0.tmp_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = -1, reduce_all = False, with_quant_attr = False)
    {Out=['opt_tmp_0']} = cast(inputs={X=['opt_opt_sum_0.tmp_0']}, in_dtype = 4, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 5, with_quant_attr = False)
    {Out=['opt_tmp_0']} = c_allreduce_sum(inputs={Cond=[], X=['opt_tmp_0']}, op_device = , op_namescope = /auto_parallel/global_norm_synchronization, op_role = 2, ring_id = 0, use_calc_stream = True, use_model_parallel = False, with_quant_attr = False)
    {Out=['opt_opt_sqrt_0.tmp_0']} = sqrt(inputs={X=['opt_tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_opt_fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, place_type = -1, shape = [1], str_value = 1.0, value = 1.0, with_quant_attr = False)
    {Out=['opt_elementwise_max_0']} = elementwise_max(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_opt_sqrt_0.tmp_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_elementwise_div_0']} = elementwise_div(inputs={X=['opt_opt_fill_constant_1.tmp_0'], Y=['opt_elementwise_max_0']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_2']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_0.w_0@GRAD@MERGE'], Y=['opt_tmp_2']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_3']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_1.w_0@GRAD@MERGE'], Y=['opt_tmp_3']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_4']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_2.w_0@GRAD@MERGE'], Y=['opt_tmp_4']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_5']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_3.w_0@GRAD@MERGE'], Y=['opt_tmp_5']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_6']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_4.w_0@GRAD@MERGE'], Y=['opt_tmp_6']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_7']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_5.w_0@GRAD@MERGE'], Y=['opt_tmp_7']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_9']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['create_parameter_0.w_0@GRAD@MERGE'], Y=['opt_tmp_9']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_10']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['create_parameter_1.w_0@GRAD@MERGE'], Y=['opt_tmp_10']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_11']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_7.w_0@GRAD@MERGE'], Y=['opt_tmp_11']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_12']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_8.w_0@GRAD@MERGE'], Y=['opt_tmp_12']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_14']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_10.w_0@GRAD@MERGE'], Y=['opt_tmp_14']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_16']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['linear_12.w_0@GRAD@MERGE'], Y=['opt_tmp_16']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Out=['opt_tmp_18']} = cast(inputs={X=['opt_elementwise_div_0']}, in_dtype = 5, op_device = , op_namescope = /gradient_clip/, op_role = 2, out_dtype = 4, with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD@MERGE']} = elementwise_mul(inputs={X=['create_parameter_2.w_0@GRAD@MERGE'], Y=['opt_tmp_18']}, axis = -1, op_device = , op_namescope = /gradient_clip/, op_role = 2, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_0.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['create_parameter_0.w_0_fp32_master_0'], Moment1Out=['create_parameter_0.w_0_fp32_master_0_moment1_0'], Moment2Out=['create_parameter_0.w_0_fp32_master_0_moment2_0'], ParamOut=['create_parameter_0.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_0.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_0.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['create_parameter_0.w_0_fp32_master_0'], Moment1=['create_parameter_0.w_0_fp32_master_0_moment1_0'], Moment2=['create_parameter_0.w_0_fp32_master_0_moment2_0'], Param=['create_parameter_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_8/, op_role = 2, use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_0.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_0.w_0_fp32_master_0'], Moment1Out=['linear_0.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_0.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_0.w_0']} = adamw(inputs={Beta1Pow=['linear_0.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_0.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_0.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_0.w_0_fp32_master_0'], Moment1=['linear_0.w_0_fp32_master_0_moment1_0'], Moment2=['linear_0.w_0_fp32_master_0_moment2_0'], Param=['linear_0.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_1/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_1.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_1.w_0_fp32_master_0'], Moment1Out=['linear_1.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_1.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_1.w_0']} = adamw(inputs={Beta1Pow=['linear_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_1.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_1.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_1.w_0_fp32_master_0'], Moment1=['linear_1.w_0_fp32_master_0_moment1_0'], Moment2=['linear_1.w_0_fp32_master_0_moment2_0'], Param=['linear_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_2/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_2.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_2.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_2.w_0_fp32_master_0'], Moment1Out=['linear_2.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_2.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_2.w_0']} = adamw(inputs={Beta1Pow=['linear_2.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_2.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_2.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_2.w_0_fp32_master_0'], Moment1=['linear_2.w_0_fp32_master_0_moment1_0'], Moment2=['linear_2.w_0_fp32_master_0_moment2_0'], Param=['linear_2.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_3/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_3.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_3.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_3.w_0_fp32_master_0'], Moment1Out=['linear_3.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_3.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_3.w_0']} = adamw(inputs={Beta1Pow=['linear_3.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_3.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_3.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_3.w_0_fp32_master_0'], Moment1=['linear_3.w_0_fp32_master_0_moment1_0'], Moment2=['linear_3.w_0_fp32_master_0_moment2_0'], Param=['linear_3.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_4/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_1.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['create_parameter_1.w_0_fp32_master_0'], Moment1Out=['create_parameter_1.w_0_fp32_master_0_moment1_0'], Moment2Out=['create_parameter_1.w_0_fp32_master_0_moment2_0'], ParamOut=['create_parameter_1.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_1.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_1.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_1.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['create_parameter_1.w_0_fp32_master_0'], Moment1=['create_parameter_1.w_0_fp32_master_0_moment1_0'], Moment2=['create_parameter_1.w_0_fp32_master_0_moment2_0'], Param=['create_parameter_1.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_9/, op_role = 2, use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_4.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_4.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_4.w_0_fp32_master_0'], Moment1Out=['linear_4.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_4.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_4.w_0']} = adamw(inputs={Beta1Pow=['linear_4.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_4.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_4.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_4.w_0_fp32_master_0'], Moment1=['linear_4.w_0_fp32_master_0_moment1_0'], Moment2=['linear_4.w_0_fp32_master_0_moment2_0'], Param=['linear_4.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_5/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_5.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_5.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_5.w_0_fp32_master_0'], Moment1Out=['linear_5.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_5.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_5.w_0']} = adamw(inputs={Beta1Pow=['linear_5.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_5.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_5.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_5.w_0_fp32_master_0'], Moment1=['linear_5.w_0_fp32_master_0_moment1_0'], Moment2=['linear_5.w_0_fp32_master_0_moment2_0'], Param=['linear_5.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_6/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['create_parameter_2.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['create_parameter_2.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['create_parameter_2.w_0_fp32_master_0'], Moment1Out=['create_parameter_2.w_0_fp32_master_0_moment1_0'], Moment2Out=['create_parameter_2.w_0_fp32_master_0_moment2_0'], ParamOut=['create_parameter_2.w_0']} = adamw(inputs={Beta1Pow=['create_parameter_2.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['create_parameter_2.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['create_parameter_2.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['create_parameter_2.w_0_fp32_master_0'], Moment1=['create_parameter_2.w_0_fp32_master_0_moment1_0'], Moment2=['create_parameter_2.w_0_fp32_master_0_moment2_0'], Param=['create_parameter_2.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_17/, op_role = 2, use_global_beta_pow = False, with_decay = False, with_quant_attr = False)
    {Beta1PowOut=['linear_7.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_7.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_7.w_0_fp32_master_0'], Moment1Out=['linear_7.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_7.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_7.w_0']} = adamw(inputs={Beta1Pow=['linear_7.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_7.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_7.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_7.w_0_fp32_master_0'], Moment1=['linear_7.w_0_fp32_master_0_moment1_0'], Moment2=['linear_7.w_0_fp32_master_0_moment2_0'], Param=['linear_7.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_10/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_8.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_8.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_8.w_0_fp32_master_0'], Moment1Out=['linear_8.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_8.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_8.w_0']} = adamw(inputs={Beta1Pow=['linear_8.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_8.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_8.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_8.w_0_fp32_master_0'], Moment1=['linear_8.w_0_fp32_master_0_moment1_0'], Moment2=['linear_8.w_0_fp32_master_0_moment2_0'], Param=['linear_8.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_11/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_10.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_10.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_10.w_0_fp32_master_0'], Moment1Out=['linear_10.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_10.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_10.w_0']} = adamw(inputs={Beta1Pow=['linear_10.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_10.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_10.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_10.w_0_fp32_master_0'], Moment1=['linear_10.w_0_fp32_master_0_moment1_0'], Moment2=['linear_10.w_0_fp32_master_0_moment2_0'], Param=['linear_10.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_13/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Beta1PowOut=['linear_12.w_0_fp32_master_0_beta1_pow_acc_0'], Beta2PowOut=['linear_12.w_0_fp32_master_0_beta2_pow_acc_0'], MasterParamOut=['linear_12.w_0_fp32_master_0'], Moment1Out=['linear_12.w_0_fp32_master_0_moment1_0'], Moment2Out=['linear_12.w_0_fp32_master_0_moment2_0'], ParamOut=['linear_12.w_0']} = adamw(inputs={Beta1Pow=['linear_12.w_0_fp32_master_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['linear_12.w_0_fp32_master_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['linear_12.w_0@GRAD@MERGE'], LearningRate=['learning_rate_0'], MasterParam=['linear_12.w_0_fp32_master_0'], Moment1=['linear_12.w_0_fp32_master_0_moment1_0'], Moment2=['linear_12.w_0_fp32_master_0_moment2_0'], Param=['linear_12.w_0'], SkipUpdate=['memcopy__0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, coeff = 0.009999999776482582, epsilon = 9.99999993922529e-09, lazy_mode = False, lr_ratio = 1.0, min_row_size_to_use_multithread = 1000, multi_precision = True, op_device = , op_namescope = /optimizer_15/, op_role = 2, use_global_beta_pow = False, with_decay = True, with_quant_attr = False)
    {Out=['embedding_0.w_0']} = c_broadcast(inputs={X=['embedding_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_0.w_0']} = c_broadcast(inputs={X=['create_parameter_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_0.w_0']} = c_broadcast(inputs={X=['linear_0.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_1.w_0']} = c_broadcast(inputs={X=['linear_1.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_2.w_0']} = c_broadcast(inputs={X=['linear_2.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_3.w_0']} = c_broadcast(inputs={X=['linear_3.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_1.w_0']} = c_broadcast(inputs={X=['create_parameter_1.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_4.w_0']} = c_broadcast(inputs={X=['linear_4.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_5.w_0']} = c_broadcast(inputs={X=['linear_5.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_6.w_0']} = c_broadcast(inputs={X=['linear_6.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_2.w_0']} = c_broadcast(inputs={X=['create_parameter_2.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_7.w_0']} = c_broadcast(inputs={X=['linear_7.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_8.w_0']} = c_broadcast(inputs={X=['linear_8.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_9.w_0']} = c_broadcast(inputs={X=['linear_9.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_10.w_0']} = c_broadcast(inputs={X=['linear_10.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_3.w_0']} = c_broadcast(inputs={X=['create_parameter_3.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_11.w_0']} = c_broadcast(inputs={X=['linear_11.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_12.w_0']} = c_broadcast(inputs={X=['linear_12.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 1, use_calc_stream = True, with_quant_attr = False)
    {Out=['linear_13.w_0']} = c_broadcast(inputs={X=['linear_13.w_0']}, op_device = , op_namescope = /auto_parallel/data_parallel, op_role = 2, ring_id = 27, root = 0, use_calc_stream = True, with_quant_attr = False)
    {Out=['create_parameter_0.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['create_parameter_0.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_0.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_0.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_1.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_1.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_2.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_2.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_3.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_3.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['create_parameter_1.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['create_parameter_1.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_4.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_4.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_5.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_5.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['create_parameter_2.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['create_parameter_2.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_7.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_7.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_8.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_8.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_10.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_10.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
    {Out=['linear_12.w_0@GRAD@MERGE']} = set_value(inputs={EndsTensorList=[], Input=['linear_12.w_0@GRAD@MERGE'], StartsTensorList=[], StepsTensorList=[], ValueTensor=[]}, axes = [], decrease_axes = [], dtype = 4, ends = [], none_axes = [], op_device = , op_namescope = /, op_role = 2, op_role_var = [], shape = [1], starts = [], steps = [], values = [Scalar(float64(0))], with_quant_attr = False)
}

[2024-03-01 04:42:35,900] [    INFO] process_group.py:150 - group_id: 0, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:53274
[2024-03-01 04:42:36,073] [    INFO] process_group.py:150 - group_id: 27, ranks: [0, 2], nranks: 2, trainer_endpoints: 172.17.0.3:53274
[2024-03-01 04:42:36,491] [    INFO] process_group.py:150 - group_id: 29, ranks: [0, 1, 2, 3], nranks: 4, trainer_endpoints: 172.17.0.3:53274
[2024-03-01 04:42:36,811] [    INFO] process_group.py:150 - group_id: 31, ranks: [3, 2], nranks: 2, trainer_endpoints: 172.17.0.3:53274
/usr/local/lib/python3.9/dist-packages/paddle/distributed/auto_parallel/static/process_group.py:255: VisibleDeprecationWarning: [93m
Warning:
API "paddle.device.cuda.synchronize" is deprecated since 2.5.0, and will be removed in future versions. Please use "paddle.device.synchronize" instead.
    Reason: synchronize in paddle.device.cuda will be removed in future [0m
  paddle.device.cuda.synchronize()
[2024-03-01 04:42:37,206] [    INFO] process_group.py:150 - group_id: 33, ranks: [2, 3], nranks: 2, trainer_endpoints: 172.17.0.3:53274
I0301 04:42:37.428563 12188 program_interpreter.cc:220] New Executor is Running.
I0301 04:42:39.993664 12188 interpreter_util.cc:652] Standalone Executor is Used.
I0301 04:42:58.405005 13452 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
[32m[2024-03-01 04:43:00,649] [    INFO][0m - loss: 0.0, learning_rate: 9.182e-05, global_step: 1, interval_runtime: 23.1778, interval_samples_per_second: 1.380630234322489, interval_steps_per_second: 0.04314469482257778[0m
[32m[2024-03-01 04:43:20,981] [    INFO][0m - loss: 0.0, learning_rate: 8.727e-05, global_step: 2, interval_runtime: 20.33, interval_samples_per_second: 1.5740311194808587, interval_steps_per_second: 0.049188472483776835[0m
[32m[2024-03-01 04:43:41,645] [    INFO][0m - loss: 0.0, learning_rate: 8.273e-05, global_step: 3, interval_runtime: 20.6631, interval_samples_per_second: 1.5486515945462265, interval_steps_per_second: 0.04839536232956958[0m
[32m[2024-03-01 04:44:02,471] [    INFO][0m - loss: 0.0, learning_rate: 7.818e-05, global_step: 4, interval_runtime: 20.8253, interval_samples_per_second: 1.5365920718841433, interval_steps_per_second: 0.04801850224637948[0m
[32m[2024-03-01 04:44:23,800] [    INFO][0m - loss: 0.0, learning_rate: 7.364e-05, global_step: 5, interval_runtime: 21.328, interval_samples_per_second: 1.500371885461357, interval_steps_per_second: 0.04688662142066741[0m
[32m[2024-03-01 04:44:45,313] [    INFO][0m - loss: 0.0, learning_rate: 6.909e-05, global_step: 6, interval_runtime: 21.5118, interval_samples_per_second: 1.4875541303423154, interval_steps_per_second: 0.04648606657319736[0m
[32m[2024-03-01 04:45:09,344] [    INFO][0m - loss: 0.0, learning_rate: 6.455e-05, global_step: 7, interval_runtime: 24.0293, interval_samples_per_second: 1.3317057976548226, interval_steps_per_second: 0.041615806176713205[0m
[32m[2024-03-01 04:45:31,063] [    INFO][0m - loss: 0.0, learning_rate: 6e-05, global_step: 8, interval_runtime: 21.7179, interval_samples_per_second: 1.4734373889167265, interval_steps_per_second: 0.0460449184036477[0m
[32m[2024-03-01 04:45:53,247] [    INFO][0m - loss: 0.0, learning_rate: 5.545e-05, global_step: 9, interval_runtime: 22.1837, interval_samples_per_second: 1.4424990654281342, interval_steps_per_second: 0.045078095794629194[0m
[32m[2024-03-01 04:46:16,967] [    INFO][0m - loss: 0.0, learning_rate: 5.091e-05, global_step: 10, interval_runtime: 23.7188, interval_samples_per_second: 1.3491384838004044, interval_steps_per_second: 0.042160577618762636[0m
[32m[2024-03-01 04:46:42,740] [    INFO][0m - loss: 0.0, learning_rate: 4.636e-05, global_step: 11, interval_runtime: 25.7728, interval_samples_per_second: 1.2416175221747698, interval_steps_per_second: 0.03880054756796156[0m
[32m[2024-03-01 04:47:08,059] [    INFO][0m - loss: 0.0, learning_rate: 4.182e-05, global_step: 12, interval_runtime: 25.3178, interval_samples_per_second: 1.2639344302433309, interval_steps_per_second: 0.03949795094510409[0m
[32m[2024-03-01 04:47:30,314] [    INFO][0m - loss: 0.0, learning_rate: 3.727e-05, global_step: 13, interval_runtime: 22.2542, interval_samples_per_second: 1.4379329138409267, interval_steps_per_second: 0.04493540355752896[0m
[32m[2024-03-01 04:47:53,082] [    INFO][0m - loss: 0.0, learning_rate: 3.273e-05, global_step: 14, interval_runtime: 22.7671, interval_samples_per_second: 1.4055389770870859, interval_steps_per_second: 0.04392309303397143[0m
[32m[2024-03-01 04:48:16,214] [    INFO][0m - loss: 0.0, learning_rate: 2.818e-05, global_step: 15, interval_runtime: 23.1308, interval_samples_per_second: 1.3834372871809522, interval_steps_per_second: 0.043232415224404755[0m
[32m[2024-03-01 04:48:39,317] [    INFO][0m - loss: 0.0, learning_rate: 2.364e-05, global_step: 16, interval_runtime: 23.1018, interval_samples_per_second: 1.3851738663609476, interval_steps_per_second: 0.043286683323779614[0m
[32m[2024-03-01 04:49:02,988] [    INFO][0m - loss: 0.0, learning_rate: 1.909e-05, global_step: 17, interval_runtime: 23.6704, interval_samples_per_second: 1.3518970956625105, interval_steps_per_second: 0.04224678423945345[0m
[32m[2024-03-01 04:49:25,514] [    INFO][0m - loss: 0.0, learning_rate: 1.455e-05, global_step: 18, interval_runtime: 22.525, interval_samples_per_second: 1.4206442043583545, interval_steps_per_second: 0.04439513138619858[0m
[32m[2024-03-01 04:49:50,412] [    INFO][0m - loss: 0.0, learning_rate: 1e-05, global_step: 19, interval_runtime: 24.897, interval_samples_per_second: 1.2852951055723763, interval_steps_per_second: 0.04016547204913676[0m
[32m[2024-03-01 04:50:14,067] [    INFO][0m - loss: 0.0, learning_rate: 1e-05, global_step: 20, interval_runtime: 23.6542, interval_samples_per_second: 1.3528251912127607, interval_steps_per_second: 0.04227578722539877[0m
